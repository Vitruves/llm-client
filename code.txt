
// File: internal/progress/progress.go
// Base: progress

package progress

import (
	"fmt"
	"os"
	"os/signal"
	"strings"
	"sync"
	"sync/atomic"
	"syscall"
	"time"

	"github.com/Vitruves/llm-client/internal/logger"

	"github.com/Vitruves/llm-client/internal/metrics"
)

type Progress struct {
	total        int64
	current      int64
	success      int64
	failed       int64
	startTime    time.Time
	lastUpdate   time.Time
	done         chan struct{}
	metricsCalc  *metrics.Calculator
	updateCount  int64
	ticker       *time.Ticker
	displayChan  chan struct{}
	lastPercent  float64
	mu           sync.Mutex
	isVisible    bool
	lastDisplay  string
	messageQueue chan string
}

func New(total int) *Progress {
	return NewWithMetrics(total, nil)
}

func NewWithMetrics(total int, metricsCalc *metrics.Calculator) *Progress {
	now := time.Now()
	return &Progress{
		total:        int64(total),
		startTime:    now,
		lastUpdate:   now,
		done:         make(chan struct{}),
		metricsCalc:  metricsCalc,
		displayChan:  make(chan struct{}, 1),
		lastPercent:  -1,
		messageQueue: make(chan string, 100),
	}
}

func (p *Progress) Start() {
	logger.Info("Starting process with %d items", p.total)
	p.setupStickyDisplay()
	p.ticker = time.NewTicker(1 * time.Second)

	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, os.Interrupt, syscall.SIGTERM)

	go func() {
		defer p.ticker.Stop()
		defer p.restoreCursor() // Always restore cursor when goroutine exits

		for {
			select {
			case <-p.done:
				p.displayFinal()
				return
			case <-sigCh:
				p.restoreCursor()
				return
			case <-p.ticker.C:
				select {
				case p.displayChan <- struct{}{}:
				default:
				}
			case <-p.displayChan:
				p.display()
			case msg := <-p.messageQueue:
				p.showMessage(msg)
			}
		}
	}()
}

func (p *Progress) LogMessage(message string) {
	select {
	case p.messageQueue <- message:
	default:
	}
}

func (p *Progress) setupStickyDisplay() {
	fmt.Print("\033[?25l")
	p.isVisible = true
	p.display()
}

func (p *Progress) showMessage(message string) {
	p.mu.Lock()
	defer p.mu.Unlock()

	if p.isVisible {
		fmt.Print("\r\033[K")
		logger.Info(message)
		p.displayImmediate()
	} else {
		logger.Info(message)
	}
}

func (p *Progress) Update(current, success, failed int) {
	atomic.StoreInt64(&p.current, int64(current))
	atomic.StoreInt64(&p.success, int64(success))
	atomic.StoreInt64(&p.failed, int64(failed))
	atomic.AddInt64(&p.updateCount, 1)
	p.triggerDisplay()
}

func (p *Progress) Increment() {
	atomic.AddInt64(&p.current, 1)
	atomic.AddInt64(&p.updateCount, 1)
	p.triggerDisplay()
}

func (p *Progress) triggerDisplay() {
	if !p.isVisible {
		return
	}
	select {
	case p.displayChan <- struct{}{}:
	default:
	}
}

func (p *Progress) restoreCursor() {
	fmt.Print("\r\033[K\033[?25h")
}

func (p *Progress) Stop() {
	p.mu.Lock()
	p.isVisible = false
	p.mu.Unlock()

	close(p.done)

	p.restoreCursor()
}

func formatDuration(d time.Duration) string {
	if d <= 0 {
		return "00:00"
	}

	seconds := int(d.Seconds())
	if seconds < 60 {
		return fmt.Sprintf("00:%02d", seconds)
	}

	minutes := seconds / 60
	if minutes < 60 {
		return fmt.Sprintf("%02d:%02d", minutes, seconds%60)
	}

	hours := minutes / 60
	minutes = minutes % 60
	return fmt.Sprintf("%02d:%02d:%02d", hours, minutes, seconds%3600%60)
}

func formatThroughput(rate float64) string {
	if rate < 1 {
		return fmt.Sprintf("%.2f/s", rate)
	} else if rate < 100 {
		return fmt.Sprintf("%.1f/s", rate)
	} else {
		return fmt.Sprintf("%.0f/s", rate)
	}
}

func (p *Progress) getMetricText(current int64) string {
	if p.metricsCalc == nil || current == 0 {
		return ""
	}

	metric := p.metricsCalc.GetCurrentMetric()
	metricName := p.metricsCalc.GetMetricName()
	return fmt.Sprintf(" | %s%s%s: %s%.2f%s", logger.ColorPurple, metricName, logger.ColorReset, logger.ColorBold, metric, logger.ColorReset)
}

func (p *Progress) displayImmediate() {
	current := atomic.LoadInt64(&p.current)
	failed := atomic.LoadInt64(&p.failed)

	if p.total == 0 {
		return
	}

	percent := float64(current) / float64(p.total) * 100
	elapsed := time.Since(p.startTime)

	var eta time.Duration
	var speed float64
	if current > 0 && elapsed > 0 {
		speed = float64(current) / elapsed.Seconds()
		if speed > 0 && current < p.total {
			eta = time.Duration((float64(p.total-current) / speed) * float64(time.Second))
		}
	}

	timestampColored := fmt.Sprintf("%s%s%s", logger.ColorBlue, time.Now().Format("15:04"), logger.ColorReset)
	infoColored := fmt.Sprintf("%s%sINFO%s%s", logger.ColorCyan, logger.ColorBold, logger.ColorReset, logger.ColorReset)
	percentColored := fmt.Sprintf("%s%.1f%%%s", logger.ColorYellow, percent, logger.ColorReset)
	countColored := fmt.Sprintf("%s%d%s/%s%d%s", logger.ColorGreen, current, logger.ColorReset, logger.ColorGreen, p.total, logger.ColorReset)
	timeColored := fmt.Sprintf("%s[%s, %s]%s", logger.ColorGray, formatDuration(elapsed), formatThroughput(speed), logger.ColorReset)
	etaColored := fmt.Sprintf("ETA: %s%s%s", logger.ColorPurple, formatDuration(eta), logger.ColorReset)

	barWidth := 30
	filledWidth := int(float64(barWidth) * percent / 100)
	if filledWidth > barWidth {
		filledWidth = barWidth
	}

	var barColor string
	if current == p.total {
		barColor = logger.ColorGreen
	} else if failed > 0 {
		barColor = logger.ColorRed
	} else if percent > 75 {
		barColor = logger.ColorYellow
	} else {
		barColor = logger.ColorCyan
	}

	progressBar := fmt.Sprintf("%s%s%s%s",
		barColor,
		strings.Repeat("█", filledWidth),
		logger.ColorReset,
		strings.Repeat("░", barWidth-filledWidth))

	metricText := p.getMetricText(current)
	
	progressLine := fmt.Sprintf("%s - %s : %s|%s| %s %s %s%s",
		timestampColored,
		infoColored,
		percentColored,
		progressBar,
		countColored,
		timeColored,
		etaColored,
		metricText)

	fmt.Printf("\r%s\033[K", progressLine)
}

func (p *Progress) display() {
	current := atomic.LoadInt64(&p.current)
	percent := float64(current) / float64(p.total) * 100

	now := time.Now()
	percentChanged := percent != p.lastPercent
	timeElapsed := now.Sub(p.lastUpdate) > 1*time.Second
	isComplete := current == p.total

	if !percentChanged && !timeElapsed && !isComplete {
		return
	}
	p.lastPercent = percent
	p.lastUpdate = now

	p.displayImmediate()
}

func (p *Progress) displayFinal() {
	p.mu.Lock()
	defer p.mu.Unlock()

	p.restoreCursor()

	success := atomic.LoadInt64(&p.success)
	failed := atomic.LoadInt64(&p.failed)

	if failed > 0 {
		logger.Error("Completed with %d errors", failed)
	} else {
		logger.Success("Completed successfully with %d items", success)
	}
}



// File: internal/client/client.go
// Base: client

package client

import (
	"fmt"
	"github.com/Vitruves/llm-client/internal/models"
)

func NewClient(cfg *models.Config) (models.Client, error) {
	switch cfg.Provider.Name {
	case "llamacpp":
		return NewLlamaCppClient(cfg)
	case "vllm":
		return NewVLLMClient(cfg)
	case "openai":
		return NewOpenAIClient(cfg)
	default:
		return nil, fmt.Errorf("unsupported provider: %s", cfg.Provider.Name)
	}
}



// File: internal/client/llamacpp.go
// Base: llamacpp

package client

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"time"

	"github.com/Vitruves/llm-client/internal/models"
)

type LlamaCppClient struct {
	baseURL    string
	httpClient *http.Client
	timeout    time.Duration
	config     *models.Config
}

func NewLlamaCppClient(cfg *models.Config) (*LlamaCppClient, error) {
	timeout, err := time.ParseDuration(cfg.Provider.Timeout)
	if err != nil {
		return nil, fmt.Errorf("invalid timeout: %w", err)
	}

	return &LlamaCppClient{
		baseURL:    cfg.Provider.BaseURL,
		httpClient: &http.Client{Timeout: timeout},
		timeout:    timeout,
		config:     cfg,
	}, nil
}

func (c *LlamaCppClient) SendRequest(ctx context.Context, req models.Request) (*models.Response, error) {
	start := time.Now()

	if c.supportsChatCompletions() {
		return c.sendChatCompletionRequest(ctx, req, start)
	}
	return c.sendCompletionRequest(ctx, req, start)
}

func (c *LlamaCppClient) supportsChatCompletions() bool {
	return c.config.Model.Parameters.ChatFormat != nil
}

func (c *LlamaCppClient) sendChatCompletionRequest(ctx context.Context, req models.Request, start time.Time) (*models.Response, error) {
	llamaReq := map[string]interface{}{
		"messages": c.formatMessages(req.Messages),
		"stream":   false,
	}

	c.addParameters(llamaReq, req.Options)
	c.addChatCompletionFeatures(llamaReq, req)

	jsonData, err := json.Marshal(llamaReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/v1/chat/completions", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	var llamaResp struct {
		Choices []struct {
			Message struct {
				Content   *string           `json:"content"`
				ToolCalls []models.ToolCall `json:"tool_calls,omitempty"`
			} `json:"message"`
			FinishReason *string `json:"finish_reason"`
		} `json:"choices"`
		Usage *models.Usage `json:"usage,omitempty"`
		Error struct {
			Message string `json:"message"`
		} `json:"error"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&llamaResp); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if llamaResp.Error.Message != "" {
		return &models.Response{
			Success:      false,
			Error:        llamaResp.Error.Message,
			ResponseTime: time.Since(start),
		}, nil
	}

	if len(llamaResp.Choices) == 0 {
		return &models.Response{
			Success:      false,
			Error:        "no response choices returned",
			ResponseTime: time.Since(start),
		}, nil
	}

	choice := llamaResp.Choices[0]
	var content string
	if choice.Message.Content != nil {
		content = *choice.Message.Content
	}

	return &models.Response{
		Content:      content,
		Success:      true,
		ResponseTime: time.Since(start),
		ToolCalls:    choice.Message.ToolCalls,
		Usage:        llamaResp.Usage,
		FinishReason: choice.FinishReason,
	}, nil
}

func (c *LlamaCppClient) sendCompletionRequest(ctx context.Context, req models.Request, start time.Time) (*models.Response, error) {
	prompt := c.formatMessagesAsPrompt(req.Messages)

	llamaReq := map[string]interface{}{
		"prompt": prompt,
		"stream": false,
	}

	c.addParameters(llamaReq, req.Options)

	jsonData, err := json.Marshal(llamaReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/completion", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	var llamaResp struct {
		Content string `json:"content"`
		Error   struct {
			Message string `json:"message"`
		} `json:"error"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&llamaResp); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if llamaResp.Error.Message != "" {
		return &models.Response{
			Success:      false,
			Error:        llamaResp.Error.Message,
			ResponseTime: time.Since(start),
		}, nil
	}

	return &models.Response{
		Content:      llamaResp.Content,
		Success:      true,
		ResponseTime: time.Since(start),
	}, nil
}

func (c *LlamaCppClient) formatMessages(messages []models.Message) []interface{} {
	formatted := make([]interface{}, len(messages))
	for i, msg := range messages {
		msgMap := map[string]interface{}{
			"role": msg.Role,
		}

		if msg.IsTextOnly() {
			msgMap["content"] = msg.GetTextContent()
		} else {
			msgMap["content"] = msg.Content
		}

		if msg.Name != nil {
			msgMap["name"] = *msg.Name
		}
		if len(msg.ToolCalls) > 0 {
			msgMap["tool_calls"] = msg.ToolCalls
		}
		if msg.ToolCallId != nil {
			msgMap["tool_call_id"] = *msg.ToolCallId
		}
		if msg.FunctionCall != nil {
			msgMap["function_call"] = msg.FunctionCall
		}

		formatted[i] = msgMap
	}
	return formatted
}

func (c *LlamaCppClient) formatMessagesAsPrompt(messages []models.Message) string {
	var result string
	for _, msg := range messages {
		content := msg.GetTextContent()
		result += fmt.Sprintf("%s: %s\n", msg.Role, content)
	}
	return result + "assistant: "
}

func (c *LlamaCppClient) addParameters(req map[string]interface{}, params models.ModelParameters) {
	// Basic sampling parameters
	if params.Temperature != nil {
		req["temperature"] = *params.Temperature
	}
	if params.MaxTokens != nil {
		req["n_predict"] = *params.MaxTokens
	}
	if params.MinTokens != nil {
		req["min_tokens"] = *params.MinTokens
	}
	if params.TopP != nil {
		req["top_p"] = *params.TopP
	}
	if params.TopK != nil {
		req["top_k"] = *params.TopK
	}
	if params.MinP != nil {
		req["min_p"] = *params.MinP
	}
	
	// Penalty parameters
	if params.RepetitionPenalty != nil {
		req["repeat_penalty"] = *params.RepetitionPenalty
	}
	if params.PresencePenalty != nil {
		req["presence_penalty"] = *params.PresencePenalty
	}
	if params.FrequencyPenalty != nil {
		req["frequency_penalty"] = *params.FrequencyPenalty
	}
	
	// Seed and stop conditions
	if params.Seed != nil {
		req["seed"] = *params.Seed
	}
	if len(params.Stop) > 0 {
		req["stop"] = params.Stop
	}
	
	// Advanced sampling parameters supported by modern llama.cpp
	if params.Mirostat != nil {
		req["mirostat"] = *params.Mirostat
	}
	if params.MirostatTau != nil {
		req["mirostat_tau"] = *params.MirostatTau
	}
	if params.MirostatEta != nil {
		req["mirostat_eta"] = *params.MirostatEta
	}
	if params.TfsZ != nil {
		req["tfs_z"] = *params.TfsZ
	}
	if params.TypicalP != nil {
		req["typical_p"] = *params.TypicalP
	}
	
	// Request-level processing parameters
	if params.NKeep != nil {
		req["n_keep"] = *params.NKeep
	}
	if params.PenalizeNl != nil {
		req["penalize_nl"] = *params.PenalizeNl
	}
	
	// Additional parameters supported by modern llama.cpp
	if params.N != nil {
		req["n_choices"] = *params.N
	}
	if params.IgnoreEos != nil {
		req["ignore_eos"] = *params.IgnoreEos
	}
	if params.Logprobs != nil {
		req["logprobs"] = *params.Logprobs
	}
	
	if params.ChatFormat != nil {
		req["chat_format"] = *params.ChatFormat
	}
}

func (c *LlamaCppClient) addChatCompletionFeatures(req map[string]interface{}, chatReq models.Request) {
	if len(chatReq.Tools) > 0 {
		req["tools"] = chatReq.Tools
	}
	if chatReq.ToolChoice != nil {
		req["tool_choice"] = chatReq.ToolChoice
	}
	if chatReq.ResponseFormat != nil {
		req["response_format"] = chatReq.ResponseFormat
	}
	if chatReq.Stream != nil {
		req["stream"] = *chatReq.Stream
	}
	if chatReq.StreamOptions != nil {
		req["stream_options"] = chatReq.StreamOptions
	}
}

func (c *LlamaCppClient) HealthCheck(ctx context.Context) error {
	url := fmt.Sprintf("%s/health", c.baseURL)
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("health check failed with status: %d", resp.StatusCode)
	}
	return nil
}

func (c *LlamaCppClient) GetServerInfo(ctx context.Context) (*models.ServerInfo, error) {
	serverInfo := &models.ServerInfo{
		ServerURL:  c.baseURL,
		ServerType: "llamacpp",
		Timestamp:  float64(time.Now().Unix()),
		Available:  false,
		Config:     make(map[string]interface{}),
		Models:     make(map[string]interface{}),
		Features:   make(map[string]interface{}),
	}

	if err := c.HealthCheck(ctx); err != nil {
		return serverInfo, nil // Return with Available=false
	}
	serverInfo.Available = true

	propsURL := fmt.Sprintf("%s/props", c.baseURL)
	req, err := http.NewRequestWithContext(ctx, "GET", propsURL, nil)
	if err == nil {
		resp, err := c.httpClient.Do(req)
		if err == nil && resp.StatusCode == http.StatusOK {
			defer resp.Body.Close()
			var propsResp map[string]interface{}
			if json.NewDecoder(resp.Body).Decode(&propsResp) == nil {
				serverInfo.Models = propsResp
			}
		}
	}

	slotsURL := fmt.Sprintf("%s/slots", c.baseURL)
	req, err = http.NewRequestWithContext(ctx, "GET", slotsURL, nil)
	if err == nil {
		resp, err := c.httpClient.Do(req)
		if err == nil && resp.StatusCode == http.StatusOK {
			defer resp.Body.Close()
			var slotsResp []map[string]interface{}
			if json.NewDecoder(resp.Body).Decode(&slotsResp) == nil {
				serverInfo.Features["slots"] = slotsResp
			}
		}
	}

	serverInfo.Config = map[string]interface{}{
		"base_url":   c.baseURL,
		"timeout":    c.timeout.String(),
		"model_name": c.config.Model.Name,
		"provider":   c.config.Provider.Name,
		"workers":    c.config.Processing.Workers,
		"batch_size": c.config.Processing.BatchSize,
		"rate_limit": c.config.Processing.RateLimit,
	}

	// Add model parameters
	if c.config.Model.Parameters.Temperature != nil {
		serverInfo.Config["temperature"] = *c.config.Model.Parameters.Temperature
	}
	if c.config.Model.Parameters.MaxTokens != nil {
		serverInfo.Config["max_tokens"] = *c.config.Model.Parameters.MaxTokens
	}
	if c.config.Model.Parameters.TopP != nil {
		serverInfo.Config["top_p"] = *c.config.Model.Parameters.TopP
	}
	if c.config.Model.Parameters.TopK != nil {
		serverInfo.Config["top_k"] = *c.config.Model.Parameters.TopK
	}

	serverInfo.Features["supports_streaming"] = true
	serverInfo.Features["supports_chat"] = false // LlamaCpp typically uses completion format
	serverInfo.Features["supports_completions"] = true
	serverInfo.Features["supports_embeddings"] = false
	serverInfo.Features["supports_reasoning"] = false
	serverInfo.Features["supports_tool_calling"] = false
	serverInfo.Features["supports_vision"] = false
	serverInfo.Features["api_version"] = "llamacpp"

	return serverInfo, nil
}

func (c *LlamaCppClient) Close() error {
	return nil
}



// File: internal/client/openai.go
// Base: openai

package client

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"time"

	"github.com/Vitruves/llm-client/internal/models"
)

type OpenAIClient struct {
	apiKey     string
	httpClient *http.Client
	timeout    time.Duration
	config     *models.Config
}

func NewOpenAIClient(cfg *models.Config) (*OpenAIClient, error) {
	timeout, err := time.ParseDuration(cfg.Provider.Timeout)
	if err != nil {
		return nil, fmt.Errorf("invalid timeout: %w", err)
	}

	return &OpenAIClient{
		apiKey:     cfg.Provider.APIKey,
		httpClient: &http.Client{Timeout: timeout},
		timeout:    timeout,
		config:     cfg,
	}, nil
}

func (c *OpenAIClient) SendRequest(ctx context.Context, req models.Request) (*models.Response, error) {
	start := time.Now()

	openaiReq := map[string]interface{}{
		"model":    c.config.Model.Name,
		"messages": c.formatMessages(req.Messages),
	}

	c.addParameters(openaiReq, req.Options)
	c.addChatCompletionFeatures(openaiReq, req)

	jsonData, err := json.Marshal(openaiReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	httpReq, err := http.NewRequestWithContext(ctx, "POST", "https://api.openai.com/v1/chat/completions", bytes.NewBuffer(jsonData))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	resp, err := c.httpClient.Do(httpReq)
	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	var openaiResp struct {
		Choices []struct {
			Message struct {
				Content   *string           `json:"content"`
				ToolCalls []models.ToolCall `json:"tool_calls,omitempty"`
			} `json:"message"`
			FinishReason *string `json:"finish_reason"`
		} `json:"choices"`
		Usage *models.Usage `json:"usage,omitempty"`
		Error struct {
			Message string `json:"message"`
		} `json:"error"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&openaiResp); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if openaiResp.Error.Message != "" {
		return &models.Response{
			Success:      false,
			Error:        openaiResp.Error.Message,
			ResponseTime: time.Since(start),
		}, nil
	}

	if len(openaiResp.Choices) == 0 {
		return &models.Response{
			Success:      false,
			Error:        "no response choices returned",
			ResponseTime: time.Since(start),
		}, nil
	}

	choice := openaiResp.Choices[0]
	var content string
	if choice.Message.Content != nil {
		content = *choice.Message.Content
	}

	return &models.Response{
		Content:      content,
		Success:      true,
		ResponseTime: time.Since(start),
		ToolCalls:    choice.Message.ToolCalls,
		Usage:        openaiResp.Usage,
		FinishReason: choice.FinishReason,
	}, nil
}

func (c *OpenAIClient) formatMessages(messages []models.Message) []interface{} {
	formatted := make([]interface{}, len(messages))
	for i, msg := range messages {
		msgMap := map[string]interface{}{
			"role": msg.Role,
		}

		if msg.IsTextOnly() {
			msgMap["content"] = msg.GetTextContent()
		} else {
			msgMap["content"] = msg.Content
		}

		if msg.Name != nil {
			msgMap["name"] = *msg.Name
		}
		if len(msg.ToolCalls) > 0 {
			msgMap["tool_calls"] = msg.ToolCalls
		}
		if msg.ToolCallId != nil {
			msgMap["tool_call_id"] = *msg.ToolCallId
		}
		if msg.FunctionCall != nil {
			msgMap["function_call"] = msg.FunctionCall
		}

		formatted[i] = msgMap
	}
	return formatted
}

func (c *OpenAIClient) addParameters(req map[string]interface{}, params models.ModelParameters) {
	if params.Temperature != nil {
		req["temperature"] = *params.Temperature
	}
	if params.MaxTokens != nil {
		req["max_tokens"] = *params.MaxTokens
	}
	if params.TopP != nil {
		req["top_p"] = *params.TopP
	}
	if params.Seed != nil {
		req["seed"] = *params.Seed
	}
	if len(params.Stop) > 0 {
		req["stop"] = params.Stop
	}
	if params.PresencePenalty != nil {
		req["presence_penalty"] = *params.PresencePenalty
	}
	if params.FrequencyPenalty != nil {
		req["frequency_penalty"] = *params.FrequencyPenalty
	}
	if params.N != nil {
		req["n"] = *params.N
	}
	if params.Logprobs != nil {
		req["logprobs"] = *params.Logprobs
	}
}

func (c *OpenAIClient) addChatCompletionFeatures(req map[string]interface{}, chatReq models.Request) {
	if len(chatReq.Tools) > 0 {
		req["tools"] = chatReq.Tools
	}
	if chatReq.ToolChoice != nil {
		req["tool_choice"] = chatReq.ToolChoice
	}
	if chatReq.ResponseFormat != nil {
		req["response_format"] = chatReq.ResponseFormat
	}
	if chatReq.Stream != nil {
		req["stream"] = *chatReq.Stream
	}
	if chatReq.StreamOptions != nil {
		req["stream_options"] = chatReq.StreamOptions
	}
}

func (c *OpenAIClient) HealthCheck(ctx context.Context) error {
	testReq := map[string]interface{}{
		"model": "gpt-3.5-turbo",
		"messages": []map[string]string{
			{"role": "user", "content": "test"},
		},
		"max_tokens": 1,
	}

	jsonData, err := json.Marshal(testReq)
	if err != nil {
		return err
	}

	req, err := http.NewRequestWithContext(context.Background(), "POST", "https://api.openai.com/v1/chat/completions", bytes.NewBuffer(jsonData))
	if err != nil {
		return err
	}

	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", fmt.Sprintf("Bearer %s", c.apiKey))

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	return nil
}

func (c *OpenAIClient) GetServerInfo(ctx context.Context) (*models.ServerInfo, error) {
	serverInfo := &models.ServerInfo{
		ServerURL:  "https://api.openai.com",
		ServerType: "OpenAI",
		Timestamp:  float64(time.Now().Unix()),
		Available:  true,
		Features: map[string]interface{}{
			"chat_completions": true,
			"tool_calling":     true,
			"function_calling": true,
			"multimodal":       true,
			"streaming":        true,
			"json_mode":        true,
		},
	}
	return serverInfo, nil
}

func (c *OpenAIClient) Close() error {
	return nil
}



// File: internal/parser/parser.go
// Base: parser

package parser

import (
	"github.com/Vitruves/llm-client/internal/models"
	"regexp"
	"strings"
)

type Parser struct {
	config models.ParsingConfig
}

func New(config models.ParsingConfig) *Parser {
	return &Parser{config: config}
}

func (p *Parser) Parse(response string) string {
	finalAnswer, _ := p.ParseWithThinking(response)
	return finalAnswer
}

func (p *Parser) ParseWithThinking(response string) (string, string) {
	thinkingContent := p.extractThinkingContent(response)
	processedResponse := p.processResponse(response)
	finalAnswer := p.extractFinalAnswer(processedResponse)

	return finalAnswer, thinkingContent
}

func (p *Parser) extractThinkingContent(response string) string {
	if p.config.ThinkingTags == "" {
		return ""
	}

	startTag, endTag := p.parseThinkingTags(p.config.ThinkingTags)
	if startTag == "" || endTag == "" {
		return ""
	}

	startIdx := strings.Index(response, startTag)
	if startIdx == -1 {
		return ""
	}

	startIdx += len(startTag)
	endIdx := strings.Index(response[startIdx:], endTag)
	if endIdx == -1 {
		return ""
	}

	return strings.TrimSpace(response[startIdx : startIdx+endIdx])
}

func (p *Parser) processResponse(response string) string {
	if p.config.PreserveThinking != nil && *p.config.PreserveThinking {
		return response
	}

	if p.config.ThinkingTags == "" {
		return response
	}

	startTag, endTag := p.parseThinkingTags(p.config.ThinkingTags)
	if startTag == "" || endTag == "" {
		return response
	}

	pattern := regexp.QuoteMeta(startTag) + ".*?" + regexp.QuoteMeta(endTag)
	re, err := regexp.Compile(pattern)
	if err != nil {
		return response
	}

	return re.ReplaceAllString(response, "")
}

func (p *Parser) extractFinalAnswer(response string) string {
	response = strings.TrimSpace(response)

	var answer string

	if len(p.config.AnswerPatterns) > 0 {
		if result := p.parseWithPatterns(response); result != "" {
			answer = result
		}
	}

	if answer == "" && len(p.config.Find) > 0 {
		if result := p.parseWithFind(response); result != "" {
			answer = result
		}
	}

	if answer != "" {
		if mapped, exists := p.config.Map[answer]; exists {
			return mapped
		}
		return answer
	}

	return p.config.Default
}

func (p *Parser) parseWithPatterns(response string) string {
	for _, pattern := range p.config.AnswerPatterns {
		flags := ""
		if p.config.CaseSensitive == nil || !*p.config.CaseSensitive {
			flags = "(?i)"
		}

		fullPattern := flags + pattern
		re, err := regexp.Compile(fullPattern)
		if err != nil {
			continue
		}

		matches := re.FindStringSubmatch(response)
		if len(matches) > 1 {
			return strings.TrimSpace(matches[1])
		}
	}
	return ""
}

func (p *Parser) parseWithFind(response string) string {
	responseToCheck := response
	if p.config.CaseSensitive == nil || !*p.config.CaseSensitive {
		responseToCheck = strings.ToLower(response)
	}

	for _, target := range p.config.Find {
		if target == "*" {
			if strings.TrimSpace(response) != "" {
				return strings.TrimSpace(response)
			}
			continue
		}

		targetToCheck := target
		if p.config.CaseSensitive == nil || !*p.config.CaseSensitive {
			targetToCheck = strings.ToLower(target)
		}

		if p.config.ExactMatch != nil && *p.config.ExactMatch {
			if responseToCheck == targetToCheck {
				return target
			}
		} else {
			if strings.Contains(responseToCheck, targetToCheck) {
				return target
			}
		}
	}

	return ""
}

func (p *Parser) parseThinkingTags(tags string) (string, string) {
	if tags == "" {
		return "", ""
	}

	if strings.Contains(tags, "><") {
		parts := strings.SplitN(tags, "><", 2)
		if len(parts) == 2 {
			return parts[0] + ">", "<" + parts[1]
		}
	} else if strings.Contains(tags, "][") {
		parts := strings.SplitN(tags, "][", 2)
		if len(parts) == 2 {
			return parts[0] + "]", "[" + parts[1]
		}
	}

	if strings.Contains(tags, " ") {
		parts := strings.Fields(tags)
		if len(parts) == 2 {
			return parts[0], parts[1]
		}
	}

	return "", ""
}



// File: internal/cli/utils.go
// Base: utils

package cli

import (
	"os"
	"runtime"

	"github.com/Vitruves/llm-client/internal/logger"
)

const (
	ColorReset  = "\033[0m"
	ColorRed    = "\033[31m"
	ColorGreen  = "\033[32m"
	ColorYellow = "\033[33m"
	ColorBlue   = "\033[34m"
	ColorPurple = "\033[35m"
	ColorCyan   = "\033[36m"
	ColorGray   = "\033[90m"
	ColorBold   = "\033[1m"
	ColorWhite  = "\033[37m"
)


func init() {
	if runtime.GOOS == "windows" {
		if os.Getenv("FORCE_COLOR") == "" {
		}
	}

	if os.Getenv("NO_COLOR") != "" {
	}
}

// This function is no longer needed as color control is handled by the logger package.
func SetColorEnabled(enabled bool) {
}

// Helper functions for consistent CLI output
// These are deprecated and should be replaced by direct logger calls
func Error(text string) string {
	return text
}

func Success(text string) string {
	return text
}

func Warning(text string) string {
	return text
}

func Info(text string) string {
	return text
}

func Highlight(text string) string {
	return text
}

func Header(text string) string {
	return text
}

func Label(text string) string {
	return text
}

func Value(text string) string {
	return text
}

func PrintError(format string, args ...interface{}) {
	logger.Error(format, args...)
}

func PrintWarning(format string, args ...interface{}) {
	logger.Warning(format, args...)
}

func PrintSuccess(format string, args ...interface{}) {
	logger.Success(format, args...)
}

func PrintInfo(format string, args ...interface{}) {
	logger.Info(format, args...)
}



// File: internal/metrics/metrics.go
// Base: metrics

package metrics

import (
	"github.com/Vitruves/llm-client/internal/models"
	"math"
	"strings"
	"sync"
)

type Calculator struct {
	mu          sync.RWMutex
	config      *models.LiveMetrics
	classes     []string
	predictions []string
	actuals     []string
	confusionMx map[string]map[string]int
	total       int
}

func NewCalculator(config *models.LiveMetrics, classes []string) *Calculator {
	if config == nil || !config.Enabled {
		return nil
	}

	if config.GroundTruth == "" {
		return nil
	}

	calc := &Calculator{
		config:      config,
		classes:     classes,
		confusionMx: make(map[string]map[string]int),
	}

	for _, class := range classes {
		calc.confusionMx[class] = make(map[string]int)
		for _, predClass := range classes {
			calc.confusionMx[class][predClass] = 0
		}
	}

	return calc
}

func (c *Calculator) AddResult(predicted, actual string) {
	if c == nil {
		return
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	c.predictions = append(c.predictions, predicted)
	c.actuals = append(c.actuals, actual)
	c.total++

	if c.confusionMx[actual] != nil {
		c.confusionMx[actual][predicted]++
	}
}

func (c *Calculator) GetCurrentMetric() float64 {
	if c == nil || c.total == 0 {
		return 0.0
	}

	c.mu.RLock()
	defer c.mu.RUnlock()

	switch c.config.Metric {
	case "accuracy":
		return c.calculateAccuracy()
	case "f1":
		return c.calculateF1()
	case "kappa":
		return c.calculateKappa()
	default:
		return c.calculateAccuracy()
	}
}

func (c *Calculator) GetMetricName() string {
	if c == nil {
		return "None"
	}

	switch c.config.Metric {
	case "accuracy":
		return "Accuracy"
	case "f1":
		avgType := c.config.Average
		if avgType == "" {
			avgType = "macro"
		}
		return "F1-" + avgType
	case "kappa":
		return "Kappa"
	default:
		return "Accuracy"
	}
}

func (c *Calculator) calculateAccuracy() float64 {
	if c.total == 0 {
		return 0.0
	}

	correct := 0
	for i := 0; i < len(c.predictions) && i < len(c.actuals); i++ {
		if c.predictions[i] == c.actuals[i] {
			correct++
		}
	}

	return float64(correct) / float64(c.total) * 100
}

func (c *Calculator) calculateF1() float64 {
	if c.total == 0 {
		return 0.0
	}

	switch c.config.Average {
	case "macro":
		return c.calculateMacroF1()
	case "micro":
		return c.calculateMicroF1()
	case "weighted":
		return c.calculateWeightedF1()
	default:
		return c.calculateMacroF1()
	}
}

func (c *Calculator) calculateMacroF1() float64 {
	perClassF1 := c.calculatePerClassF1()

	if len(perClassF1) == 0 {
		return 0.0
	}

	sum := 0.0
	count := 0
	for _, f1 := range perClassF1 {
		if !math.IsNaN(f1) {
			sum += f1
			count++
		}
	}

	if count == 0 {
		return 0.0
	}

	return sum / float64(count) * 100
}

func (c *Calculator) calculateMicroF1() float64 {
	totalTP := 0
	totalFP := 0
	totalFN := 0

	for _, class := range c.classes {
		tp, fp, fn := c.getClassMetrics(class)
		totalTP += tp
		totalFP += fp
		totalFN += fn
	}

	if totalTP == 0 {
		return 0.0
	}

	precision := float64(totalTP) / float64(totalTP+totalFP)
	recall := float64(totalTP) / float64(totalTP+totalFN)

	if precision+recall == 0 {
		return 0.0
	}

	return 2 * precision * recall / (precision + recall) * 100
}

func (c *Calculator) calculateWeightedF1() float64 {
	perClassF1 := c.calculatePerClassF1()

	weightedSum := 0.0
	totalSupport := 0

	for _, class := range c.classes {
		support := 0
		for actual, predictions := range c.confusionMx {
			if actual == class {
				for _, count := range predictions {
					support += count
				}
			}
		}

		f1 := perClassF1[class]
		if !math.IsNaN(f1) {
			weightedSum += f1 * float64(support)
		}
		totalSupport += support
	}

	if totalSupport == 0 {
		return 0.0
	}

	return weightedSum / float64(totalSupport) * 100
}

func (c *Calculator) calculatePerClassF1() map[string]float64 {
	result := make(map[string]float64)

	for _, class := range c.classes {
		tp, fp, fn := c.getClassMetrics(class)

		if tp == 0 {
			result[class] = 0.0
			continue
		}

		precision := float64(tp) / float64(tp+fp)
		recall := float64(tp) / float64(tp+fn)

		if precision+recall == 0 {
			result[class] = 0.0
		} else {
			result[class] = 2 * precision * recall / (precision + recall)
		}
	}

	return result
}

func (c *Calculator) getClassMetrics(class string) (tp, fp, fn int) {
	for actual, predictions := range c.confusionMx {
		for predicted, count := range predictions {
			if actual == class && predicted == class {
				tp += count
			} else if actual != class && predicted == class {
				fp += count
			} else if actual == class && predicted != class {
				fn += count
			}
		}
	}
	return tp, fp, fn
}

func (c *Calculator) calculateKappa() float64 {
	if c.total == 0 {
		return 0.0
	}

	po := c.calculateAccuracy() / 100

	pe := 0.0
	total := float64(c.total)

	for _, class := range c.classes {
		actualCount := 0
		predictedCount := 0

		for actual, predictions := range c.confusionMx {
			if actual == class {
				for _, count := range predictions {
					actualCount += count
				}
			}
			if predictions[class] > 0 {
				predictedCount += predictions[class]
			}
		}

		pActual := float64(actualCount) / total
		pPredicted := float64(predictedCount) / total
		pe += pActual * pPredicted
	}

	if pe == 1.0 {
		return 0.0
	}

	return (po - pe) / (1.0 - pe) * 100
}

func NormalizeLabel(label string) string {
	return strings.TrimSpace(label)
}



// File: internal/utils/utils.go
// Base: utils

package utils

import (
	"fmt"
	"strings"
	"time"
)

func FormatDuration(d time.Duration) string {
	if d < time.Millisecond {
		return fmt.Sprintf("%.2fμs", float64(d.Nanoseconds())/1000)
	}
	if d < time.Second {
		return fmt.Sprintf("%.2fms", float64(d.Nanoseconds())/1000000)
	}
	return d.Round(time.Millisecond).String()
}

func TruncateString(s string, maxLength int) string {
	if len(s) <= maxLength {
		return s
	}
	if maxLength <= 3 {
		return "..."
	}
	return s[:maxLength-3] + "..."
}

func NormalizeLabel(label string) string {
	return strings.TrimSpace(strings.ToLower(label))
}

func CalculatePercentage(part, total int) float64 {
	if total == 0 {
		return 0
	}
	return float64(part) / float64(total) * 100
}

func ParseTemplate(template string, data map[string]interface{}) string {
	result := template
	for key, value := range data {
		placeholder := fmt.Sprintf("{%s}", key)
		result = strings.ReplaceAll(result, placeholder, fmt.Sprintf("%v", value))
	}
	return result
}

func Contains(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

func RemoveDuplicates(slice []string) []string {
	seen := make(map[string]bool)
	var result []string

	for _, item := range slice {
		if !seen[item] {
			seen[item] = true
			result = append(result, item)
		}
	}

	return result
}

func MostFrequent(items []string) (string, int) {
	if len(items) == 0 {
		return "", 0
	}

	counts := make(map[string]int)
	for _, item := range items {
		counts[item]++
	}

	var maxItem string
	var maxCount int
	for item, count := range counts {
		if count > maxCount {
			maxCount = count
			maxItem = item
		}
	}

	return maxItem, maxCount
}



// File: internal/models/chat_helpers.go
// Base: chat_helpers

package models

import (
	"encoding/json"
	"fmt"
	"strings"
)

type ConversationBuilder struct {
	messages []Message
}

func NewConversation() *ConversationBuilder {
	return &ConversationBuilder{
		messages: make([]Message, 0),
	}
}

func (cb *ConversationBuilder) AddSystemMessage(content string) *ConversationBuilder {
	cb.messages = append(cb.messages, NewTextMessage("system", content))
	return cb
}

func (cb *ConversationBuilder) AddUserMessage(content string) *ConversationBuilder {
	cb.messages = append(cb.messages, NewTextMessage("user", content))
	return cb
}

func (cb *ConversationBuilder) AddAssistantMessage(content string) *ConversationBuilder {
	cb.messages = append(cb.messages, NewTextMessage("assistant", content))
	return cb
}

func (cb *ConversationBuilder) AddUserMessageWithImages(text string, imageUrls []string) *ConversationBuilder {
	contents := make([]interface{}, 0, len(imageUrls)+1)

	if text != "" {
		contents = append(contents, map[string]interface{}{
			"type": "text",
			"text": text,
		})
	}

	for _, url := range imageUrls {
		contents = append(contents, map[string]interface{}{
			"type": "image_url",
			"image_url": map[string]interface{}{
				"url": url,
			},
		})
	}

	cb.messages = append(cb.messages, NewMultiModalMessage("user", contents))
	return cb
}

func (cb *ConversationBuilder) AddToolMessage(toolCallId, content string) *ConversationBuilder {
	cb.messages = append(cb.messages, NewToolMessage(toolCallId, content))
	return cb
}

func (cb *ConversationBuilder) AddAssistantMessageWithToolCalls(content string, toolCalls []ToolCall) *ConversationBuilder {
	msg := NewTextMessage("assistant", content)
	msg.ToolCalls = toolCalls
	cb.messages = append(cb.messages, msg)
	return cb
}

// Build returns the built conversation
func (cb *ConversationBuilder) Build() []Message {
	return cb.messages
}

func (cb *ConversationBuilder) Clear() *ConversationBuilder {
	cb.messages = cb.messages[:0]
	return cb
}

// GetLastMessage returns the last message in the conversation
func (cb *ConversationBuilder) GetLastMessage() *Message {
	if len(cb.messages) == 0 {
		return nil
	}
	return &cb.messages[len(cb.messages)-1]
}

// GetMessageCount returns the number of messages in the conversation
func (cb *ConversationBuilder) GetMessageCount() int {
	return len(cb.messages)
}

type ChatTemplate struct {
	Name            string
	SystemPrefix    string
	SystemSuffix    string
	UserPrefix      string
	UserSuffix      string
	AssistantPrefix string
	AssistantSuffix string
	BosToken        string
	EosToken        string
	StopTokens      []string
}

var (
	ChatMLTemplate = ChatTemplate{
		Name:            "chatml",
		SystemPrefix:    "<|im_start|>system\n",
		SystemSuffix:    "<|im_end|>\n",
		UserPrefix:      "<|im_start|>user\n",
		UserSuffix:      "<|im_end|>\n",
		AssistantPrefix: "<|im_start|>assistant\n",
		AssistantSuffix: "<|im_end|>\n",
		BosToken:        "",
		EosToken:        "<|im_end|>",
		StopTokens:      []string{"<|im_end|>", "<|endoftext|>"},
	}

	Llama2Template = ChatTemplate{
		Name:            "llama2",
		SystemPrefix:    "<s>[INST] <<SYS>>\n",
		SystemSuffix:    "\n<</SYS>>\n\n",
		UserPrefix:      "",
		UserSuffix:      " [/INST] ",
		AssistantPrefix: "",
		AssistantSuffix: " </s><s>[INST] ",
		BosToken:        "<s>",
		EosToken:        "</s>",
		StopTokens:      []string{"</s>"},
	}

	Llama3Template = ChatTemplate{
		Name:            "llama3",
		SystemPrefix:    "<|start_header_id|>system<|end_header_id|>\n\n",
		SystemSuffix:    "<|eot_id|>",
		UserPrefix:      "<|start_header_id|>user<|end_header_id|>\n\n",
		UserSuffix:      "<|eot_id|>",
		AssistantPrefix: "<|start_header_id|>assistant<|end_header_id|>\n\n",
		AssistantSuffix: "<|eot_id|>",
		BosToken:        "<|begin_of_text|>",
		EosToken:        "<|eot_id|>",
		StopTokens:      []string{"<|eot_id|>", "<|end_of_text|>"},
	}

	VicunaTemplate = ChatTemplate{
		Name:            "vicuna",
		SystemPrefix:    "",
		SystemSuffix:    "\n\n",
		UserPrefix:      "USER: ",
		UserSuffix:      "\n",
		AssistantPrefix: "ASSISTANT: ",
		AssistantSuffix: "\n",
		BosToken:        "",
		EosToken:        "</s>",
		StopTokens:      []string{"</s>"},
	}
)

// GetChatTemplate returns a chat template by name
func GetChatTemplate(name string) *ChatTemplate {
	switch strings.ToLower(name) {
	case "chatml":
		return &ChatMLTemplate
	case "llama2":
		return &Llama2Template
	case "llama3":
		return &Llama3Template
	case "vicuna":
		return &VicunaTemplate
	default:
		return &ChatMLTemplate // Default fallback
	}
}

func (ct *ChatTemplate) FormatConversation(messages []Message) string {
	var result strings.Builder

	if ct.BosToken != "" {
		result.WriteString(ct.BosToken)
	}

	for i, msg := range messages {
		content := msg.GetTextContent()

		switch msg.Role {
		case "system":
			result.WriteString(ct.SystemPrefix)
			result.WriteString(content)
			result.WriteString(ct.SystemSuffix)
		case "user":
			result.WriteString(ct.UserPrefix)
			result.WriteString(content)
			result.WriteString(ct.UserSuffix)
		case "assistant":
			result.WriteString(ct.AssistantPrefix)
			result.WriteString(content)
			if i < len(messages)-1 { // Don't add suffix for last assistant message
				result.WriteString(ct.AssistantSuffix)
			}
		}
	}

	return result.String()
}

func ValidateMessage(msg Message) error {
	if msg.Role == "" {
		return fmt.Errorf("message role cannot be empty")
	}

	validRoles := []string{"system", "user", "assistant", "tool", "function"}
	isValidRole := false
	for _, role := range validRoles {
		if msg.Role == role {
			isValidRole = true
			break
		}
	}
	if !isValidRole {
		return fmt.Errorf("invalid message role: %s", msg.Role)
	}

	if msg.Content == nil || (msg.IsTextOnly() && msg.GetTextContent() == "") {
		if len(msg.ToolCalls) == 0 {
			return fmt.Errorf("message content cannot be empty unless it has tool calls")
		}
	}

	if msg.Role == "tool" && msg.ToolCallId == nil {
		return fmt.Errorf("tool messages must have a tool_call_id")
	}

	return nil
}

func ValidateConversation(messages []Message) error {
	if len(messages) == 0 {
		return fmt.Errorf("conversation cannot be empty")
	}

	for i, msg := range messages {
		if err := ValidateMessage(msg); err != nil {
			return fmt.Errorf("message %d: %w", i, err)
		}
	}

	return nil
}

func EstimateTokenCount(messages []Message) int {
	totalTokens := 0

	for _, msg := range messages {
		content := msg.GetTextContent()
		contentTokens := len(content) / 4
		totalTokens += contentTokens + 10

		if len(msg.ToolCalls) > 0 {
			for _, toolCall := range msg.ToolCalls {
				totalTokens += len(toolCall.Function.Name)/4 + len(toolCall.Function.Arguments)/4 + 20
			}
		}
	}

	return totalTokens
}

func ConversationToJSON(messages []Message) (string, error) {
	data, err := json.MarshalIndent(messages, "", "  ")
	if err != nil {
		return "", fmt.Errorf("failed to marshal conversation: %w", err)
	}
	return string(data), nil
}

func ConversationFromJSON(jsonStr string) ([]Message, error) {
	var messages []Message
	if err := json.Unmarshal([]byte(jsonStr), &messages); err != nil {
		return nil, fmt.Errorf("failed to unmarshal conversation: %w", err)
	}
	return messages, nil
}

func TruncateConversation(messages []Message, maxTokens int) []Message {
	if len(messages) == 0 {
		return messages
	}

	var systemMsg *Message
	startIdx := 0
	if messages[0].Role == "system" {
		systemMsg = &messages[0]
		startIdx = 1
	}

	currentTokens := 0
	if systemMsg != nil {
		currentTokens = EstimateTokenCount([]Message{*systemMsg})
	}

	var result []Message
	if systemMsg != nil {
		result = append(result, *systemMsg)
	}

	for i := len(messages) - 1; i >= startIdx; i-- {
		msgTokens := EstimateTokenCount([]Message{messages[i]})
		if currentTokens+msgTokens > maxTokens {
			break
		}
		currentTokens += msgTokens
		result = append([]Message{messages[i]}, result...)
	}

	return result
}



// File: internal/writer/stream_writer.go
// Base: stream_writer

package writer

import (
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"sort"
	"strconv"
	"sync"

	"github.com/Vitruves/llm-client/internal/models"

	"github.com/parquet-go/parquet-go"
)

type StreamWriter interface {
	WriteResult(result models.Result) error
	Close() error
	GetFilename() string
	Flush() error
}

type JSONStreamWriter struct {
	filename   string
	file       *os.File
	encoder    *json.Encoder
	mutex      sync.Mutex
	firstWrite bool
}

type CSVStreamWriter struct {
	filename        string
	file            *os.File
	writer          *csv.Writer
	mutex           sync.Mutex
	headerWritten   bool
	config          *models.OutputConfig
	knownHeaders    []string
	originalHeaders map[string]struct{}
}

type ParquetStreamWriter struct {
	filename   string
	file       *os.File
	mutex      sync.Mutex
	config     *models.OutputConfig
	buffer     []StreamParquetResult
	bufferSize int
}

func NewStreamWriter(format, directory, timestamp string, config *models.OutputConfig) (StreamWriter, error) {
	if err := os.MkdirAll(directory, 0755); err != nil {
		return nil, err
	}

	switch format {
	case "json":
		return NewJSONStreamWriter(directory, timestamp)
	case "csv":
		return NewCSVStreamWriter(directory, timestamp, config)
	case "parquet":
		return NewParquetStreamWriter(directory, timestamp, config)
	default:
		return nil, fmt.Errorf("unsupported streaming format: %s", format)
	}
}

func NewJSONStreamWriter(directory, timestamp string) (*JSONStreamWriter, error) {
	filename := fmt.Sprintf("%s/results_%s.json", directory, timestamp)
	file, err := os.Create(filename)
	if err != nil {
		return nil, err
	}

	if _, err := file.WriteString("[\n"); err != nil {
		file.Close()
		return nil, err
	}

	encoder := json.NewEncoder(file)
	encoder.SetIndent("  ", "  ")

	return &JSONStreamWriter{
		filename:   filename,
		file:       file,
		encoder:    encoder,
		firstWrite: true,
	}, nil
}

func (w *JSONStreamWriter) WriteResult(result models.Result) error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if !w.firstWrite {
		if _, err := w.file.WriteString(",\n"); err != nil {
			return err
		}
	} else {
		w.firstWrite = false
	}

	if _, err := w.file.WriteString("  "); err != nil {
		return err
	}

	return w.encoder.Encode(result)
}

func (w *JSONStreamWriter) Close() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if w.file != nil {
		if _, err := w.file.WriteString("\n]"); err != nil {
			w.file.Close()
			return err
		}
		return w.file.Close()
	}
	return nil
}

func (w *JSONStreamWriter) GetFilename() string {
	return w.filename
}

func (w *JSONStreamWriter) Flush() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if w.file != nil {
		return w.file.Sync()
	}
	return nil
}

func NewCSVStreamWriter(directory, timestamp string, config *models.OutputConfig) (*CSVStreamWriter, error) {
	filename := fmt.Sprintf("%s/results_%s.csv", directory, timestamp)
	file, err := os.Create(filename)
	if err != nil {
		return nil, err
	}

	writer := csv.NewWriter(file)

	return &CSVStreamWriter{
		filename:        filename,
		file:            file,
		writer:          writer,
		config:          config,
		originalHeaders: make(map[string]struct{}),
	}, nil
}

func (w *CSVStreamWriter) WriteResult(result models.Result) error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	for key := range result.OriginalData {
		w.originalHeaders[key] = struct{}{}
	}

	if !w.headerWritten {
		w.writeHeader()
		w.headerWritten = true
	}

	row := []string{
		strconv.Itoa(result.Index),
		result.InputText,
		result.GroundTruth,
		result.FinalAnswer,
		strconv.FormatBool(result.Success),
		strconv.FormatInt(result.ResponseTime.Nanoseconds()/1000000, 10),
	}

	for _, h := range w.knownHeaders {
		val := result.OriginalData[h]
		row = append(row, fmt.Sprintf("%v", val))
	}

	if w.config.IncludeThinking {
		row = append(row, result.ThinkingContent)
	}
	if w.config.IncludeRawResponse {
		row = append(row, result.RawResponse)
	}

	if err := w.writer.Write(row); err != nil {
		return err
	}

	w.writer.Flush()
	return w.writer.Error()
}

func (w *CSVStreamWriter) writeHeader() {
	var sortedHeaders []string
	for key := range w.originalHeaders {
		sortedHeaders = append(sortedHeaders, key)
	}
	sort.Strings(sortedHeaders)
	w.knownHeaders = sortedHeaders

	header := []string{"index", "input_text", "ground_truth", "final_answer", "success", "response_time_ms"}
	header = append(header, w.knownHeaders...)

	if w.config.IncludeThinking {
		header = append(header, "thinking_content")
	}
	if w.config.IncludeRawResponse {
		header = append(header, "raw_response")
	}

	w.writer.Write(header)
	w.writer.Flush()
}

func (w *CSVStreamWriter) Close() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if w.writer != nil {
		w.writer.Flush()
	}
	if w.file != nil {
		return w.file.Close()
	}
	return nil
}

func (w *CSVStreamWriter) GetFilename() string {
	return w.filename
}

func (w *CSVStreamWriter) Flush() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	if w.writer != nil {
		w.writer.Flush()
		return w.writer.Error()
	}
	return nil
}

// StreamParquetResult represents the structure for parquet streaming output
// We serialize OriginalData as JSON to avoid interface{} schema issues
type StreamParquetResult struct {
	Index            int32  `parquet:"index"`
	InputText        string `parquet:"input_text"`
	GroundTruth      string `parquet:"ground_truth"`
	FinalAnswer      string `parquet:"final_answer"`
	Success          bool   `parquet:"success"`
	ResponseTimeMs   int64  `parquet:"response_time_ms"`
	ThinkingContent  string `parquet:"thinking_content,optional"`
	RawResponse      string `parquet:"raw_response,optional"`
	OriginalDataJSON string `parquet:"original_data_json,optional"`
}

func NewParquetStreamWriter(directory, timestamp string, config *models.OutputConfig) (*ParquetStreamWriter, error) {
	filename := fmt.Sprintf("%s/results_%s.parquet", directory, timestamp)
	file, err := os.Create(filename)
	if err != nil {
		return nil, err
	}

	bufferSize := config.StreamSaveEvery
	if bufferSize <= 0 {
		bufferSize = 100
	}

	return &ParquetStreamWriter{
		filename:   filename,
		file:       file,
		config:     config,
		buffer:     make([]StreamParquetResult, 0, bufferSize),
		bufferSize: bufferSize,
	}, nil
}

func (w *ParquetStreamWriter) WriteResult(result models.Result) error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	// Serialize OriginalData to JSON to avoid interface{} schema issues
	var originalDataJSON string
	if result.OriginalData != nil {
		if jsonBytes, err := json.Marshal(result.OriginalData); err == nil {
			originalDataJSON = string(jsonBytes)
		}
	}

	parquetResult := StreamParquetResult{
		Index:            int32(result.Index),
		InputText:        result.InputText,
		GroundTruth:      result.GroundTruth,
		FinalAnswer:      result.FinalAnswer,
		Success:          result.Success,
		ResponseTimeMs:   result.ResponseTime.Nanoseconds() / 1000000,
		ThinkingContent:  result.ThinkingContent,
		RawResponse:      result.RawResponse,
		OriginalDataJSON: originalDataJSON,
	}

	w.buffer = append(w.buffer, parquetResult)

	if len(w.buffer) >= w.bufferSize {
		return w.flushBuffer()
	}

	return nil
}

func (w *ParquetStreamWriter) flushBuffer() error {
	if len(w.buffer) == 0 {
		return nil
	}

	err := parquet.Write(w.file, w.buffer)
	if err != nil {
		return err
	}

	w.buffer = w.buffer[:0]
	return nil
}

func (w *ParquetStreamWriter) Close() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	err := w.flushBuffer()
	if err != nil {
		if w.file != nil {
			w.file.Close()
		}
		return err
	}

	if w.file != nil {
		return w.file.Close()
	}
	return nil
}

func (w *ParquetStreamWriter) GetFilename() string {
	return w.filename
}

func (w *ParquetStreamWriter) Flush() error {
	w.mutex.Lock()
	defer w.mutex.Unlock()

	err := w.flushBuffer()
	if err != nil {
		return err
	}

	if w.file != nil {
		return w.file.Sync()
	}
	return nil
}



// File: internal/config/config.go
// Base: config

package config

import (
	"fmt"
	"os"

	"github.com/Vitruves/llm-client/internal/models"

	"gopkg.in/yaml.v3"
)

func Load(filename string) (*models.Config, error) {
	data, err := os.ReadFile(filename)
	if err != nil {
		return nil, fmt.Errorf("failed to read config file: %w", err)
	}

	content := os.ExpandEnv(string(data))

	var cfg models.Config
	if err := yaml.Unmarshal([]byte(content), &cfg); err != nil {
		return nil, fmt.Errorf("failed to parse config: %w", err)
	}

	setDefaults(&cfg)
	if err := validate(&cfg); err != nil {
		return nil, fmt.Errorf("invalid config: %w", err)
	}

	return &cfg, nil
}

func setDefaults(cfg *models.Config) {
	if cfg.Provider.Timeout == "" {
		cfg.Provider.Timeout = "60s"
	}
	if cfg.Processing.Workers == 0 {
		cfg.Processing.Workers = 4
	}
	if cfg.Processing.BatchSize == 0 {
		cfg.Processing.BatchSize = 1
	}
	if cfg.Processing.Repeat == 0 {
		cfg.Processing.Repeat = 1
	}
	if cfg.Output.Directory == "" {
		cfg.Output.Directory = "./output"
	}
	if cfg.Output.Format == "" {
		cfg.Output.Format = "json"
	}
}

func validate(cfg *models.Config) error {
	if cfg.Provider.Name == "" {
		return fmt.Errorf("provider name is required")
	}

	validProviders := map[string]bool{
		"llamacpp": true,
		"vllm":     true,
		"openai":   true,
	}

	if !validProviders[cfg.Provider.Name] {
		return fmt.Errorf("unsupported provider: %s", cfg.Provider.Name)
	}

	if cfg.Provider.BaseURL == "" && cfg.Provider.Name != "openai" {
		return fmt.Errorf("base_url is required for provider: %s", cfg.Provider.Name)
	}

	if cfg.Provider.Name == "openai" && cfg.Provider.APIKey == "" {
		return fmt.Errorf("api_key is required for OpenAI provider")
	}

	validFormats := map[string]bool{"json": true, "csv": true, "parquet": true, "xlsx": true}
	if !validFormats[cfg.Output.Format] {
		return fmt.Errorf("unsupported output format: %s", cfg.Output.Format)
	}

	if cfg.Processing.Repeat < 1 || cfg.Processing.Repeat > 10 {
		return fmt.Errorf("repeat count must be between 1 and 10")
	}

	return nil
}



// File: main.go
// Base: main

package main

import (
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"os"
	"os/signal"
	"strings"
	"syscall"
	"time"

	"github.com/Vitruves/llm-client/internal/cli"
	"github.com/Vitruves/llm-client/internal/client"
	"github.com/Vitruves/llm-client/internal/config"
	"github.com/Vitruves/llm-client/internal/loader"
	"github.com/Vitruves/llm-client/internal/logger"
	"github.com/Vitruves/llm-client/internal/models"
	"github.com/Vitruves/llm-client/internal/parser"
	"github.com/Vitruves/llm-client/internal/processor"
	"github.com/Vitruves/llm-client/internal/reporter"

	"github.com/fatih/color"
	"github.com/spf13/cobra"
)

var version = "2.0.0"

func main() {
	if err := newRootCmd().Execute(); err != nil {
		cli.PrintError("%v", err)
		os.Exit(1)
	}
}

func newRootCmd() *cobra.Command {
	var rootCmd = &cobra.Command{
		Use:     "llm-client",
		Short:   "A versatile client for LLM classification tasks",
		Long:    "LLM Client - A versatile client designed for LLM classification tasks, offering a range of robust capabilities for data processing and analysis.",
		Version: version,
		PersistentPreRun: func(cmd *cobra.Command, args []string) {
			if os.Getenv("NO_COLOR") != "" {
				color.NoColor = true
			}
		},
	}

	rootCmd.PersistentFlags().Bool("no-color", false, "Disable colored output")
	rootCmd.PersistentFlags().BoolP("help", "h", false, "Show help message")

	rootCmd.AddCommand(newRunCmd())
	rootCmd.AddCommand(newReportCmd())
	rootCmd.AddCommand(newHealthCmd())
	rootCmd.AddCommand(newConfigCmd())

	return rootCmd
}

func newRunCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "run",
		Short: "Run LLM processing on input data",
		Long: `Process input data through Large Language Models for classification, extraction, and analysis tasks.
Supports multiple providers (vLLM, llama.cpp, OpenAI), output formats, and advanced processing features.

Examples:
  llm-client run -c config.yaml -i data.csv
  llm-client run -c config.yaml -i data.csv --temperature 0.7 --max-tokens 100 -w 50
  llm-client run -c config.yaml -i data.csv --stream-output --guided-regex "^(positive|negative|neutral)$"
  llm-client run --resume results/state_20240101_123456.json`,
		RunE: runClassify,
	}

	cmd.Flags().StringP("config", "c", "config.yaml", "Configuration file path")
	cmd.Flags().StringP("input", "i", "", "Input data file (CSV/JSON/Excel/Parquet)")
	cmd.Flags().StringP("output", "o", "", "Output directory (overrides config)")
	cmd.Flags().IntP("workers", "w", 0, "Number of workers (overrides config)")
	cmd.Flags().IntP("repeat", "r", 0, "Repeat count for consensus (overrides config)")
	cmd.Flags().IntP("limit", "l", 0, "Limit processing to first N rows")
	cmd.Flags().Bool("progress", true, "Show progress bar")
	cmd.Flags().BoolP("verbose", "v", false, "Enable verbose output")
	cmd.Flags().Bool("dry-run", false, "Validate config without processing")
	cmd.Flags().String("resume", "", "Resume from saved state file")

	cmd.Flags().Bool("stream-output", false, "Enable streaming output (overrides config)")
	cmd.Flags().Int("stream-save-every", 0, "Force save every N results when streaming (overrides config)")
	cmd.Flags().Bool("minimal-mode", false, "Enable minimal processing mode (overrides config)")
	cmd.Flags().String("format", "", "Output format: json, csv, parquet, xlsx (overrides config)")

	// Model parameter overrides
	cmd.Flags().Float64("temperature", -1, "Temperature for sampling (0.0-2.0, overrides config)")
	cmd.Flags().Int("max-tokens", -1, "Maximum tokens to generate (overrides config)")
	cmd.Flags().Int("min-tokens", -1, "Minimum tokens to generate (overrides config)")
	cmd.Flags().Float64("top-p", -1, "Top-p nucleus sampling (0.0-1.0, overrides config)")
	cmd.Flags().Int("top-k", -1, "Top-k sampling (overrides config)")
	cmd.Flags().Float64("min-p", -1, "Min-p sampling (overrides config)")
	cmd.Flags().Float64("repetition-penalty", -1, "Repetition penalty (overrides config)")
	cmd.Flags().Float64("presence-penalty", -1, "Presence penalty (-2.0 to 2.0, overrides config)")
	cmd.Flags().Float64("frequency-penalty", -1, "Frequency penalty (-2.0 to 2.0, overrides config)")
	cmd.Flags().Int64("seed", -1, "Random seed for reproducibility (overrides config)")

	cmd.Flags().Int("mirostat", -1, "Mirostat sampling mode: 0=disabled, 1=Mirostat, 2=Mirostat 2.0")
	cmd.Flags().Float64("mirostat-tau", -1, "Mirostat target entropy (default: 5.0)")
	cmd.Flags().Float64("mirostat-eta", -1, "Mirostat learning rate (default: 0.1)")
	cmd.Flags().Float64("tfs-z", -1, "Tail-free sampling parameter (default: 1.0)")
	cmd.Flags().Float64("typical-p", -1, "Locally typical sampling probability (default: 1.0)")

	cmd.Flags().Bool("disable-thinking", false, "Disable thinking mode for Qwen3 (vLLM only, overrides config)")
	cmd.Flags().StringSlice("guided-choice", nil, "Constrain output to specific choices (vLLM)")
	cmd.Flags().String("guided-regex", "", "Enforce regex pattern on output (vLLM)")
	cmd.Flags().String("guided-grammar", "", "Apply grammar constraints (vLLM)")
	cmd.Flags().Bool("use-beam-search", false, "Use beam search instead of sampling (vLLM)")
	cmd.Flags().Int("best-of", -1, "Generate best_of completions and return best (vLLM)")

	cmd.Flags().StringSlice("stop", nil, "Stop generation at these strings (overrides config)")
	cmd.Flags().IntSlice("stop-token-ids", nil, "Stop generation at these token IDs (overrides config)")

	// Custom validation function
	cmd.PreRunE = func(cmd *cobra.Command, args []string) error {
		resume, _ := cmd.Flags().GetString("resume")
		input, _ := cmd.Flags().GetString("input")

		if resume == "" && input == "" {
			return fmt.Errorf("either --input or --resume must be specified")
		}
		return nil
	}

	return cmd
}

func newReportCmd() *cobra.Command {
	var reportCmd = &cobra.Command{
		Use:   "report",
		Short: "Generate reports from classification results",
		Long:  "Analyze and generate detailed reports from LLM classification result files",
	}

	reportCmd.AddCommand(newAnalyzeCmd())
	reportCmd.AddCommand(newCompareCmd())

	return reportCmd
}

func newAnalyzeCmd() *cobra.Command {
	var analyzeCmd = &cobra.Command{
		Use:   "analyze [result-file]",
		Short: "Analyze classification results",
		Args:  cobra.ExactArgs(1),
		RunE:  runAnalyze,
	}

	analyzeCmd.Flags().StringP("output", "o", "", "Output file path")
	analyzeCmd.Flags().String("format", "text", "Output format (text/json)")

	return analyzeCmd
}

func newCompareCmd() *cobra.Command {
	var compareCmd = &cobra.Command{
		Use:   "compare [file1] [file2]",
		Short: "Compare two result files",
		Args:  cobra.ExactArgs(2),
		RunE:  runCompare,
	}

	compareCmd.Flags().StringP("output", "o", "", "Output file path")
	compareCmd.Flags().String("format", "text", "Output format (text/json)")

	return compareCmd
}

func newHealthCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "health",
		Short: "Check LLM server health and configuration",
		Long: `Perform health checks on LLM servers and retrieve configuration information.

Examples:
  llm-client health --vllm
  llm-client health --llamacpp --get-server-config
  llm-client health --curl-test http://localhost:8000`,
		RunE: runHealth,
	}

	cmd.Flags().Bool("vllm", false, "Check vLLM server health")
	cmd.Flags().Bool("llamacpp", false, "Check llama.cpp server health")
	cmd.Flags().Bool("get-server-config", false, "Retrieve and display server configuration")
	cmd.Flags().String("curl-test", "", "Test server endpoint with curl-like request")
	cmd.Flags().StringP("config", "c", "config.yaml", "Configuration file for server connection")
	cmd.Flags().BoolP("verbose", "v", false, "Enable verbose output")

	return cmd
}

func newConfigCmd() *cobra.Command {
	var configCmd = &cobra.Command{
		Use:   "config",
		Short: "Configuration utilities",
		Long:  "Validate configurations and test request/response processing",
	}

	configCmd.AddCommand(newConfigValidateCmd())

	return configCmd
}

func newConfigValidateCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "validate [config-file]",
		Short: "Validate configuration file",
		Long: `Validate configuration file and optionally test with sample data.

Examples:
  llm-client config validate config.yaml
  llm-client config validate config.yaml --test-file data.csv
  llm-client config validate config.yaml --test-file data.csv --show-request`,
		Args: cobra.ExactArgs(1),
		RunE: runConfigValidate,
	}

	cmd.Flags().String("test-file", "", "Test configuration with sample data file")
	cmd.Flags().Bool("show-request", false, "Display how data will be sent to server")
	cmd.Flags().Bool("show-response", false, "Display expected server response and parsing")
	cmd.Flags().Int("test-rows", 3, "Number of rows to test (default 3)")

	return cmd
}



func runClassify(cmd *cobra.Command, args []string) error {
	configFile, _ := cmd.Flags().GetString("config")
	inputFile, _ := cmd.Flags().GetString("input")
	outputDir, _ := cmd.Flags().GetString("output")
	limit, _ := cmd.Flags().GetInt("limit")
	workers, _ := cmd.Flags().GetInt("workers")
	repeat, _ := cmd.Flags().GetInt("repeat")
	verbose, _ := cmd.Flags().GetBool("verbose")
	showProgress, _ := cmd.Flags().GetBool("progress")
	dryRun, _ := cmd.Flags().GetBool("dry-run")
	resume, _ := cmd.Flags().GetString("resume")
	streamOutput, _ := cmd.Flags().GetBool("stream-output")
	streamSaveEvery, _ := cmd.Flags().GetInt("stream-save-every")
	minimalMode, _ := cmd.Flags().GetBool("minimal-mode")
	disableThinking, _ := cmd.Flags().GetBool("disable-thinking")

	if resume == "" {
		if _, err := os.Stat(inputFile); os.IsNotExist(err) {
			return fmt.Errorf("input file does not exist: %s", inputFile)
		}
	}

	cfg, err := config.Load(configFile)
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	if outputDir != "" {
		cfg.Output.Directory = outputDir
	}
	if workers > 0 {
		cfg.Processing.Workers = workers
	}
	if repeat > 0 {
		cfg.Processing.Repeat = repeat
	}

	if cmd.Flags().Changed("stream-output") {
		cfg.Output.StreamOutput = streamOutput
	}
	if cmd.Flags().Changed("stream-save-every") {
		cfg.Output.StreamSaveEvery = streamSaveEvery
	}
	if cmd.Flags().Changed("minimal-mode") {
		cfg.Processing.MinimalMode = minimalMode
	}
	if cmd.Flags().Changed("disable-thinking") {
		if cfg.Provider.Name == "vllm" {
			if cfg.Model.Parameters.EnableThinking == nil {
				cfg.Model.Parameters.EnableThinking = new(bool)
			}
			*cfg.Model.Parameters.EnableThinking = !disableThinking // Set to false when disabling
		} else {
			cli.PrintWarning("disable-thinking flag only supported for vLLM provider, ignoring")
		}
	}

	if format, _ := cmd.Flags().GetString("format"); format != "" {
		cfg.Output.Format = format
	}

	// Apply model parameter overrides
	applyModelParameterOverrides(cmd, &cfg.Model.Parameters, cfg.Provider.Name)

	if dryRun {
		cli.PrintSuccess("Configuration validation passed")
		printConfigSummary(cfg, configFile)
		return nil
	}

	proc := processor.New(cfg)
	proc.SetConfigFile(configFile)

	ctx, cancel := signal.NotifyContext(context.Background(), os.Interrupt, syscall.SIGTERM)
	defer cancel()

	opts := processor.Options{
		Limit:        limit,
		ShowProgress: showProgress,
		Verbose:      verbose,
		ResumeFile:   resume,
	}

	if resume != "" {
		cli.PrintInfo("Resuming from: %s", resume)
	} else {
		cli.PrintInfo("Starting classification: %s", inputFile)
	}

	if verbose {
		logger.SetVerbose(true)
		printConfigSummary(cfg, configFile)
	} else {
		logger.Info("Workers: %d, Repeat: %d, Provider: %s",
			cfg.Processing.Workers, cfg.Processing.Repeat, cfg.Provider.Name)
	}

	if err := proc.ProcessFile(ctx, inputFile, opts); err != nil {
		if strings.Contains(err.Error(), "processing cancelled") {
			cmd.SilenceUsage = true
		}
		return fmt.Errorf("processing failed: %w", err)
	}

	cli.PrintSuccess("Processing completed successfully")
	return nil
}

func runAnalyze(cmd *cobra.Command, args []string) error {
	inputFile := args[0]
	output, _ := cmd.Flags().GetString("output")
	format, _ := cmd.Flags().GetString("format")

	results, err := loadResults(inputFile)
	if err != nil {
		return fmt.Errorf("failed to load results: %w", err)
	}

	rep := reporter.New(results)

	var content string
	switch format {
	case "json":
		content, err = rep.GenerateJSON()
	case "text":
		content = rep.GenerateText()
	default:
		return fmt.Errorf("unsupported format: %s", format)
	}

	if err != nil {
		return err
	}

	if output != "" {
		return os.WriteFile(output, []byte(content), 0644)
	}

	fmt.Print(content)
	return nil
}

func runCompare(cmd *cobra.Command, args []string) error {
	file1, file2 := args[0], args[1]
	output, _ := cmd.Flags().GetString("output")
	format, _ := cmd.Flags().GetString("format")

	results1, err := loadResults(file1)
	if err != nil {
		return fmt.Errorf("failed to load first file: %w", err)
	}

	results2, err := loadResults(file2)
	if err != nil {
		return fmt.Errorf("failed to load second file: %w", err)
	}

	comparison := generateComparison(results1, results2)

	var content string
	switch format {
	case "json":
		data, err := json.MarshalIndent(comparison, "", "  ")
		if err != nil {
			return err
		}
		content = string(data)
	case "text":
		content = formatComparison(comparison)
	default:
		return fmt.Errorf("unsupported format: %s", format)
	}

	if output != "" {
		return os.WriteFile(output, []byte(content), 0644)
	}

	fmt.Print(content)
	return nil
}

func loadResults(filename string) ([]models.Result, error) {
	data, err := os.ReadFile(filename)
	if err != nil {
		return nil, err
	}

	var output struct {
		Results []models.Result `json:"results"`
	}

	if err := json.Unmarshal(data, &output); err != nil {
		return nil, err
	}

	return output.Results, nil
}

type Comparison struct {
	File1 ComparisonStats `json:"file1"`
	File2 ComparisonStats `json:"file2"`
	Diff  DifferenceStats `json:"difference"`
}

type ComparisonStats struct {
	Total       int           `json:"total"`
	Success     int           `json:"success"`
	Failed      int           `json:"failed"`
	SuccessRate float64       `json:"success_rate"`
	AvgTime     time.Duration `json:"avg_time"`
}

type DifferenceStats struct {
	TotalDiff       int     `json:"total_diff"`
	SuccessRateDiff float64 `json:"success_rate_diff"`
	AvgTimeDiff     string  `json:"avg_time_diff"`
}

func generateComparison(results1, results2 []models.Result) *Comparison {
	stats1 := calculateComparisonStats(results1)
	stats2 := calculateComparisonStats(results2)

	diff := DifferenceStats{
		TotalDiff:       stats2.Total - stats1.Total,
		SuccessRateDiff: stats2.SuccessRate - stats1.SuccessRate,
		AvgTimeDiff:     (stats2.AvgTime - stats1.AvgTime).String(),
	}

	return &Comparison{
		File1: stats1,
		File2: stats2,
		Diff:  diff,
	}
}

func calculateComparisonStats(results []models.Result) ComparisonStats {
	if len(results) == 0 {
		return ComparisonStats{}
	}

	var successCount int
	var totalTime time.Duration

	for _, result := range results {
		if result.Success {
			successCount++
		}
		totalTime += result.ResponseTime
	}

	return ComparisonStats{
		Total:       len(results),
		Success:     successCount,
		Failed:      len(results) - successCount,
		SuccessRate: float64(successCount) / float64(len(results)) * 100,
		AvgTime:     totalTime / time.Duration(len(results)),
	}
}

func formatComparison(comp *Comparison) string {
	var report strings.Builder

	report.WriteString("COMPARISON REPORT\n")
	report.WriteString("=================\n\n")

	report.WriteString("FILE 1 STATS:\n")
	report.WriteString(fmt.Sprintf("  Total: %d\n", comp.File1.Total))
	report.WriteString(fmt.Sprintf("  Success: %d (%.2f%%)\n", comp.File1.Success, comp.File1.SuccessRate))
	report.WriteString(fmt.Sprintf("  Failed: %d\n", comp.File1.Failed))
	report.WriteString(fmt.Sprintf("  Avg Time: %v\n", comp.File1.AvgTime))

	report.WriteString("\nFILE 2 STATS:\n")
	report.WriteString(fmt.Sprintf("  Total: %d\n", comp.File2.Total))
	report.WriteString(fmt.Sprintf("  Success: %d (%.2f%%)\n", comp.File2.Success, comp.File2.SuccessRate))
	report.WriteString(fmt.Sprintf("  Failed: %d\n", comp.File2.Failed))
	report.WriteString(fmt.Sprintf("  Avg Time: %v\n", comp.File2.AvgTime))

	report.WriteString("\nDIFFERENCE:\n")
	report.WriteString(fmt.Sprintf("  Total Diff: %+d\n", comp.Diff.TotalDiff))
	report.WriteString(fmt.Sprintf("  Success Rate Diff: %+.2f%%\n", comp.Diff.SuccessRateDiff))
	report.WriteString(fmt.Sprintf("  Avg Time Diff: %s\n", comp.Diff.AvgTimeDiff))

	return report.String()
}

func applyModelParameterOverrides(cmd *cobra.Command, params *models.ModelParameters, providerName string) {
	// Basic sampling parameters
	if temp, _ := cmd.Flags().GetFloat64("temperature"); temp >= 0 {
		params.Temperature = &temp
	}
	if maxTokens, _ := cmd.Flags().GetInt("max-tokens"); maxTokens >= 0 {
		params.MaxTokens = &maxTokens
	}
	if minTokens, _ := cmd.Flags().GetInt("min-tokens"); minTokens >= 0 {
		params.MinTokens = &minTokens
	}
	if topP, _ := cmd.Flags().GetFloat64("top-p"); topP >= 0 {
		params.TopP = &topP
	}
	if topK, _ := cmd.Flags().GetInt("top-k"); topK >= 0 {
		params.TopK = &topK
	}
	if minP, _ := cmd.Flags().GetFloat64("min-p"); minP >= 0 {
		params.MinP = &minP
	}
	if repPenalty, _ := cmd.Flags().GetFloat64("repetition-penalty"); repPenalty >= 0 {
		params.RepetitionPenalty = &repPenalty
	}
	if presPenalty, _ := cmd.Flags().GetFloat64("presence-penalty"); presPenalty >= -2.0 {
		params.PresencePenalty = &presPenalty
	}
	if freqPenalty, _ := cmd.Flags().GetFloat64("frequency-penalty"); freqPenalty >= -2.0 {
		params.FrequencyPenalty = &freqPenalty
	}
	if seed, _ := cmd.Flags().GetInt64("seed"); seed >= 0 {
		params.Seed = &seed
	}

	if stop, _ := cmd.Flags().GetStringSlice("stop"); len(stop) > 0 {
		params.Stop = stop
	}
	if stopTokenIds, _ := cmd.Flags().GetIntSlice("stop-token-ids"); len(stopTokenIds) > 0 {
		params.StopTokenIds = stopTokenIds
	}

	// llama.cpp specific parameters
	if providerName == "llamacpp" {
		if mirostat, _ := cmd.Flags().GetInt("mirostat"); mirostat >= 0 {
			params.Mirostat = &mirostat
		}
		if mirostatTau, _ := cmd.Flags().GetFloat64("mirostat-tau"); mirostatTau >= 0 {
			params.MirostatTau = &mirostatTau
		}
		if mirostatEta, _ := cmd.Flags().GetFloat64("mirostat-eta"); mirostatEta >= 0 {
			params.MirostatEta = &mirostatEta
		}
		if tfsZ, _ := cmd.Flags().GetFloat64("tfs-z"); tfsZ >= 0 {
			params.TfsZ = &tfsZ
		}
		if typicalP, _ := cmd.Flags().GetFloat64("typical-p"); typicalP >= 0 {
			params.TypicalP = &typicalP
		}
	}

	// vLLM specific parameters
	if providerName == "vllm" {
		if guidedChoice, _ := cmd.Flags().GetStringSlice("guided-choice"); len(guidedChoice) > 0 {
			params.GuidedChoice = guidedChoice
		}
		if guidedRegex, _ := cmd.Flags().GetString("guided-regex"); guidedRegex != "" {
			params.GuidedRegex = &guidedRegex
		}
		if guidedGrammar, _ := cmd.Flags().GetString("guided-grammar"); guidedGrammar != "" {
			params.GuidedGrammar = &guidedGrammar
		}
		if cmd.Flags().Changed("use-beam-search") {
			useBeamSearch, _ := cmd.Flags().GetBool("use-beam-search")
			params.UseBeamSearch = &useBeamSearch
		}
		if bestOf, _ := cmd.Flags().GetInt("best-of"); bestOf >= 0 {
			params.BestOf = &bestOf
		}
	}
}

func runHealth(cmd *cobra.Command, args []string) error {
	vllm, _ := cmd.Flags().GetBool("vllm")
	llamacpp, _ := cmd.Flags().GetBool("llamacpp")
	getServerConfig, _ := cmd.Flags().GetBool("get-server-config")
	curlTest, _ := cmd.Flags().GetString("curl-test")
	configFile, _ := cmd.Flags().GetString("config")
	verbose, _ := cmd.Flags().GetBool("verbose")

	if verbose {
		logger.SetVerbose(true)
	}

	if curlTest != "" {
		return runCurlTest(curlTest)
	}

	if vllm {
		return runVLLMHealthCheck(configFile, getServerConfig, true)
	}

	if llamacpp {
		return runLlamaCppHealthCheck(configFile, getServerConfig, true)
	}

	if !vllm && !llamacpp && curlTest == "" {
		cfg, err := config.Load(configFile)
		if err != nil {
			cmd.SilenceUsage = true
			return fmt.Errorf("failed to load config: %w", err)
		}

		switch cfg.Provider.Name {
		case "vllm":
			return runVLLMHealthCheck(configFile, getServerConfig, false)
		case "llamacpp":
			return runLlamaCppHealthCheck(configFile, getServerConfig, false)
		default:
			cmd.SilenceUsage = true
			return fmt.Errorf("unsupported provider: %s", cfg.Provider.Name)
		}
	}

	cmd.SilenceUsage = true
	return fmt.Errorf("specify --vllm, --llamacpp, or --curl-test")
}

func runConfigValidate(cmd *cobra.Command, args []string) error {
	configFile := args[0]
	testFile, _ := cmd.Flags().GetString("test-file")
	showRequest, _ := cmd.Flags().GetBool("show-request")
	showResponse, _ := cmd.Flags().GetBool("show-response")
	testRows, _ := cmd.Flags().GetInt("test-rows")

	logger.Header("Configuration Validation")
	logger.Info("Validating: %s", configFile)

	cfg, err := config.Load(configFile)
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	logger.Info("✓ Configuration loaded successfully")
	printConfigSummary(cfg, configFile)

	if testFile != "" {
		return runConfigTest(cfg, testFile, showRequest, showResponse, testRows)
	}

	logger.Success("Configuration validation completed")
	return nil
}

func runVLLMHealthCheck(configFile string, getServerConfig bool, useDefault bool) error {
	var cfg *models.Config
	var err error

	if useDefault && configFile == "config.yaml" {
		cfg = &models.Config{
			Provider: models.ProviderConfig{
				Name:    "vllm",
				BaseURL: "http://localhost:8000",
				Timeout: "10s",
			},
			Model: models.ModelConfig{
				Name: "default",
			},
		}
		logger.Header("vLLM Health Check")
		logger.Info("Server: %s (using default)", cfg.Provider.BaseURL)
	} else {
		cfg, err = config.Load(configFile)
		if err != nil {
			return fmt.Errorf("failed to load config: %w", err)
		}

		if cfg.Provider.Name != "vllm" {
			return fmt.Errorf("config provider is %s, not vllm", cfg.Provider.Name)
		}

		logger.Header("vLLM Health Check")
		logger.Info("Server: %s", cfg.Provider.BaseURL)
	}

	client, err := client.NewClient(cfg)
	if err != nil {
		return fmt.Errorf("failed to create client: %w", err)
	}
	defer client.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := client.HealthCheck(ctx); err != nil {
		logger.Error("Health check failed: %v", err)
		return err
	}

	logger.Success("✓ vLLM server is healthy")

	if getServerConfig {
		logger.Info("Retrieving server configuration...")
		serverInfo, err := client.GetServerInfo(ctx)
		if err != nil {
			logger.Warning("Failed to get server info: %v", err)
		} else {
			printServerInfo(serverInfo)
		}
	}

	return nil
}

func runLlamaCppHealthCheck(configFile string, getServerConfig bool, useDefault bool) error {
	var cfg *models.Config
	var err error

	if useDefault && configFile == "config.yaml" {
		cfg = &models.Config{
			Provider: models.ProviderConfig{
				Name:    "llamacpp",
				BaseURL: "http://localhost:8080",
				Timeout: "10s",
			},
			Model: models.ModelConfig{
				Name: "default",
			},
		}
		logger.Header("llama.cpp Health Check")
		logger.Info("Server: %s (using default)", cfg.Provider.BaseURL)
	} else {
		cfg, err = config.Load(configFile)
		if err != nil {
			return fmt.Errorf("failed to load config: %w", err)
		}

		if cfg.Provider.Name != "llamacpp" {
			return fmt.Errorf("config provider is %s, not llamacpp", cfg.Provider.Name)
		}

		logger.Header("llama.cpp Health Check")
		logger.Info("Server: %s", cfg.Provider.BaseURL)
	}

	client, err := client.NewClient(cfg)
	if err != nil {
		return fmt.Errorf("failed to create client: %w", err)
	}
	defer client.Close()

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	if err := client.HealthCheck(ctx); err != nil {
		logger.Error("Health check failed: %v", err)
		return err
	}

	logger.Success("✓ llama.cpp server is healthy")

	if getServerConfig {
		logger.Info("Retrieving server configuration...")
		serverInfo, err := client.GetServerInfo(ctx)
		if err != nil {
			logger.Warning("Failed to get server info: %v", err)
		} else {
			printServerInfo(serverInfo)
		}
	}

	return nil
}

func runCurlTest(url string) error {
	logger.Header("cURL-like Test")
	logger.Info("Testing endpoint: %s", url)

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return fmt.Errorf("failed to create request: %w", err)
	}

	client := &http.Client{Timeout: 10 * time.Second}
	resp, err := client.Do(req)
	if err != nil {
		logger.Error("Request failed: %v", err)
		return err
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return fmt.Errorf("failed to read response: %w", err)
	}

	logger.Info("Status: %d %s", resp.StatusCode, resp.Status)
	logger.Info("Content-Type: %s", resp.Header.Get("Content-Type"))
	logger.Info("Content-Length: %d bytes", len(body))

	if resp.StatusCode >= 200 && resp.StatusCode < 300 {
		logger.Success("✓ Endpoint is accessible")
	} else {
		logger.Warning("⚠ Endpoint returned status %d", resp.StatusCode)
	}

	if strings.Contains(resp.Header.Get("Content-Type"), "json") && len(body) < 2048 {
		logger.Debug("Response body: %s", string(body))
	}

	return nil
}

func runConfigTest(cfg *models.Config, testFile string, showRequest, showResponse bool, testRows int) error {
	logger.Header("Configuration Test")
	logger.Info("Testing with file: %s", testFile)

	if showRequest || showResponse {
		logger.SetVerbose(true)
	}

	data, err := loader.LoadData(testFile)
	if err != nil {
		return fmt.Errorf("failed to load test data: %w", err)
	}

	if len(data) == 0 {
		return fmt.Errorf("test file contains no data")
	}

	if testRows > 0 && testRows < len(data) {
		data = data[:testRows]
	}

	logger.Info("Testing with %d rows", len(data))

	parser := parser.New(cfg.Classification.Parsing)

	for i, row := range data {
		logger.Info("--- Test Row %d ---", i+1)
		logger.Info("Input: %s", row.Text)

		if showRequest {
			systemPrompt := cfg.Classification.Template.System
			userPrompt := cfg.Classification.Template.User

			// Apply placeholders (this is a simplified version of what happens internally)
			renderedUser := strings.ReplaceAll(userPrompt, "{text}", row.Text)
			renderedUser = strings.ReplaceAll(renderedUser, "{index}", fmt.Sprintf("%d", row.Index))

			for key, value := range row.Data {
				placeholder := fmt.Sprintf("{%s}", strings.ToLower(key))
				renderedUser = strings.ReplaceAll(renderedUser, placeholder, fmt.Sprintf("%v", value))
			}

			logger.Debug("System prompt: %s", systemPrompt)
			logger.Debug("User prompt: %s", renderedUser)
		}

		if showResponse {
			sampleResponses := []string{
				"positive",
				"The sentiment is negative",
				"I think this is neutral in tone",
			}

			for _, response := range sampleResponses {
				parsed := parser.Parse(response)
				logger.Debug("Sample response: '%s' -> Parsed: '%s'", response, parsed)
			}
		}
	}

	logger.Success("Configuration test completed")
	return nil
}

func printServerInfo(info *models.ServerInfo) {
	logger.Info("Server URL: %s", info.ServerURL)
	logger.Info("Server Type: %s", info.ServerType)
	logger.Info("Available: %v", info.Available)

	if len(info.Models) > 0 {
		logger.Header("Server Models")
		if modelName, ok := info.Models["model_name"]; ok {
			logger.Info("Model Name: %v", modelName)
		}
		if maxModelLen, ok := info.Models["max_model_len"]; ok {
			logger.Info("Max Model Length: %v", maxModelLen)
		}
		if created, ok := info.Models["created"]; ok {
			logger.Info("Created: %v", created)
		}
		if ownedBy, ok := info.Models["owned_by"]; ok {
			logger.Info("Owned By: %v", ownedBy)
		}
	}

	// Show version information
	if len(info.Features) > 0 {
		if versionInfo, ok := info.Features["version_info"]; ok {
			logger.Header("Server Version")
			if versionMap, ok := versionInfo.(map[string]interface{}); ok {
				for key, value := range versionMap {
					logger.Info("%s: %v", key, value)
				}
			}
		}

		logger.Header("Server Features")
		for key, value := range info.Features {
			if key != "version_info" {
				logger.Info("%s: %v", key, value)
			}
		}
	}

	if len(info.Config) > 0 {
		logger.Header("Client Configuration")

		// Group important parameters
		if temperature, ok := info.Config["temperature"]; ok {
			logger.Info("Temperature: %v", temperature)
		}
		if maxTokens, ok := info.Config["max_tokens"]; ok {
			logger.Info("Max Tokens: %v", maxTokens)
		}
		if topP, ok := info.Config["top_p"]; ok {
			logger.Info("Top-P: %v", topP)
		}
		if topK, ok := info.Config["top_k"]; ok {
			logger.Info("Top-K: %v", topK)
		}

		// Show all other parameters
		logger.Debug("All Client Parameters:")
		for key, value := range info.Config {
			if key != "temperature" && key != "max_tokens" && key != "top_p" && key != "top_k" {
				logger.Debug("  %s: %v", key, value)
			}
		}
	}
}

func printConfigSummary(cfg *models.Config, configFile string) {
	logger.Header("Configuration Summary")
	logger.Info("Config File: %s", configFile)
	logger.Info("Provider: %s (%s)", cfg.Provider.Name, cfg.Provider.BaseURL)
	logger.Info("Model: %s", cfg.Model.Name)
	logger.Info("Workers: %d, Repeat: %d", cfg.Processing.Workers, cfg.Processing.Repeat)
	logger.Info("Output: %s format to %s", cfg.Output.Format, cfg.Output.Directory)

	params := ""
	if cfg.Model.Parameters.Temperature != nil {
		params += fmt.Sprintf("Temperature: %.2f", *cfg.Model.Parameters.Temperature)
	}
	if cfg.Model.Parameters.MaxTokens != nil {
		if params != "" {
			params += ", "
		}
		params += fmt.Sprintf("Max Tokens: %d", *cfg.Model.Parameters.MaxTokens)
	}
	if cfg.Model.Parameters.TopP != nil {
		if params != "" {
			params += ", "
		}
		params += fmt.Sprintf("Top-P: %.2f", *cfg.Model.Parameters.TopP)
	}
	if params != "" {
		logger.Info("Parameters: %s", params)
	}
}



// File: internal/models/types.go
// Base: types

package models

import (
	"context"
	"time"
)

type Config struct {
	Provider       ProviderConfig       `yaml:"provider"`
	Model          ModelConfig          `yaml:"model"`
	Classification ClassificationConfig `yaml:"classification"`
	Processing     ProcessingConfig     `yaml:"processing"`
	Output         OutputConfig         `yaml:"output"`
	Reference      ReferenceConfig      `yaml:"reference,omitempty"`
}

type ProviderConfig struct {
	Name    string `yaml:"name"`
	BaseURL string `yaml:"base_url"`
	Timeout string `yaml:"timeout"`
	APIKey  string `yaml:"api_key"`
}

type ModelConfig struct {
	Name       string          `yaml:"name"`
	Parameters ModelParameters `yaml:"parameters"`
}

type ModelParameters struct {
	// Basic sampling parameters
	Temperature                *float64 `yaml:"temperature,omitempty"`
	MaxTokens                  *int     `yaml:"max_tokens,omitempty"`
	MinTokens                  *int     `yaml:"min_tokens,omitempty"`
	TopP                       *float64 `yaml:"top_p,omitempty"`
	TopK                       *int     `yaml:"top_k,omitempty"`
	MinP                       *float64 `yaml:"min_p,omitempty"`
	RepetitionPenalty          *float64 `yaml:"repetition_penalty,omitempty"`
	PresencePenalty            *float64 `yaml:"presence_penalty,omitempty"`
	FrequencyPenalty           *float64 `yaml:"frequency_penalty,omitempty"`
	Seed                       *int64   `yaml:"seed,omitempty"`
	N                          *int     `yaml:"n,omitempty"`
	Stop                       []string `yaml:"stop,omitempty"`
	StopTokenIds               []int    `yaml:"stop_token_ids,omitempty"`
	BadWords                   []string `yaml:"bad_words,omitempty"`
	IncludeStopStrInOutput     *bool    `yaml:"include_stop_str_in_output,omitempty"`
	IgnoreEos                  *bool    `yaml:"ignore_eos,omitempty"`
	Logprobs                   *int     `yaml:"logprobs,omitempty"`
	PromptLogprobs             *int     `yaml:"prompt_logprobs,omitempty"`
	TruncatePromptTokens       *int     `yaml:"truncate_prompt_tokens,omitempty"`
	ChatFormat                 *string  `yaml:"chat_format,omitempty"`
	SkipSpecialTokens          *bool    `yaml:"skip_special_tokens,omitempty"`
	SpacesBetweenSpecialTokens *bool    `yaml:"spaces_between_special_tokens,omitempty"`
	EnableThinking             *bool    `yaml:"enable_thinking,omitempty"`

	// vLLM Guided Generation Parameters
	GuidedChoice              []string               `yaml:"guided_choice,omitempty"`
	GuidedRegex               *string                `yaml:"guided_regex,omitempty"`
	GuidedJSON                map[string]interface{} `yaml:"guided_json,omitempty"`
	GuidedGrammar             *string                `yaml:"guided_grammar,omitempty"`
	GuidedWhitespacePattern   *string                `yaml:"guided_whitespace_pattern,omitempty"`
	GuidedDecodingBackend     *string                `yaml:"guided_decoding_backend,omitempty"`

	// Additional vLLM Parameters
	MaxLogprobs               *int     `yaml:"max_logprobs,omitempty"`
	Echo                      *bool    `yaml:"echo,omitempty"`
	BestOf                    *int     `yaml:"best_of,omitempty"`
	UseBeamSearch             *bool    `yaml:"use_beam_search,omitempty"`
	LengthPenalty             *float64 `yaml:"length_penalty,omitempty"`
	EarlyStopping             *bool    `yaml:"early_stopping,omitempty"`

	// llama.cpp Sampling Parameters
	Mirostat                  *int     `yaml:"mirostat,omitempty"`
	MirostatTau               *float64 `yaml:"mirostat_tau,omitempty"`
	MirostatEta               *float64 `yaml:"mirostat_eta,omitempty"`
	TfsZ                      *float64 `yaml:"tfs_z,omitempty"`
	TypicalP                  *float64 `yaml:"typical_p,omitempty"`

	// llama.cpp Request-level Parameters
	NKeep                     *int  `yaml:"n_keep,omitempty"`
	PenalizeNl                *bool `yaml:"penalize_nl,omitempty"`
}

type ClassificationConfig struct {
	Template     TemplateConfig      `yaml:"template"`
	Parsing      ParsingConfig       `yaml:"parsing"`
	FieldMapping *FieldMappingConfig `yaml:"field_mapping,omitempty"`
}

type FieldMappingConfig struct {
	InputTextField string            `yaml:"input_text_field,omitempty"`
	PlaceholderMap map[string]string `yaml:"placeholder_map,omitempty"`
}

type TemplateConfig struct {
	System string `yaml:"system"`
	User   string `yaml:"user"`
}

type ParsingConfig struct {
	Find             []string          `yaml:"find"`
	Default          string            `yaml:"default"`
	Fallback         string            `yaml:"fallback"`
	Map              map[string]string `yaml:"map"`
	ThinkingTags     string            `yaml:"thinking_tags"`
	PreserveThinking *bool             `yaml:"preserve_thinking,omitempty"`
	AnswerPatterns   []string          `yaml:"answer_patterns"`
	CaseSensitive    *bool             `yaml:"case_sensitive,omitempty"`
	ExactMatch       *bool             `yaml:"exact_match,omitempty"`
}

type ProcessingConfig struct {
	Workers        int          `yaml:"workers"`
	BatchSize      int          `yaml:"batch_size"`
	Repeat         int          `yaml:"repeat"`
	RateLimit      bool         `yaml:"rate_limit"`
	FlashInferSafe *bool        `yaml:"flashinfer_safe,omitempty"`
	LiveMetrics    *LiveMetrics `yaml:"live_metrics,omitempty"`
	MinimalMode    bool         `yaml:"minimal_mode,omitempty"`
}

type LiveMetrics struct {
	Enabled     bool     `yaml:"enabled"`
	Metric      string   `yaml:"metric"`
	GroundTruth string   `yaml:"ground_truth"`
	Average     string   `yaml:"average,omitempty"`
	Classes     []string `yaml:"classes,omitempty"`
}

type OutputConfig struct {
	Directory          string `yaml:"directory"`
	Format             string `yaml:"format"`
	InputTextField     string `yaml:"input_text_field,omitempty"`
	IncludeRawResponse bool   `yaml:"include_raw_response,omitempty"`
	IncludeThinking    bool   `yaml:"include_thinking,omitempty"`
	StreamOutput       bool   `yaml:"stream_output,omitempty"`
	StreamSaveEvery    int    `yaml:"stream_save_every,omitempty"`
}

type ReferenceConfig struct {
	File        string `yaml:"file,omitempty"`        // Path to reference/ground truth file
	Column      string `yaml:"column,omitempty"`      // Column name containing reference values
	Format      string `yaml:"format,omitempty"`      // File format (csv, json, xlsx, parquet)
	IndexColumn string `yaml:"index_column,omitempty"` // Column to match with input data index
}

// Enhanced Message struct with support for different content types and tool calls
type Message struct {
	Role         string        `json:"role"`
	Content      interface{}   `json:"content"`
	Name         *string       `json:"name,omitempty"`
	ToolCalls    []ToolCall    `json:"tool_calls,omitempty"`
	ToolCallId   *string       `json:"tool_call_id,omitempty"`
	FunctionCall *FunctionCall `json:"function_call,omitempty"`
}

type TextContent struct {
	Type string `json:"type"`
	Text string `json:"text"`
}

type ImageContent struct {
	Type     string    `json:"type"`
	ImageUrl *ImageUrl `json:"image_url,omitempty"`
}

type ImageUrl struct {
	Url    string  `json:"url"`
	Detail *string `json:"detail,omitempty"`
}

type ToolCall struct {
	Id       string   `json:"id"`
	Type     string   `json:"type"`
	Function Function `json:"function"`
}

type Function struct {
	Name      string `json:"name"`
	Arguments string `json:"arguments"`
}

type FunctionCall struct {
	Name      string `json:"name"`
	Arguments string `json:"arguments"`
}

// Tool definition for function calling
type Tool struct {
	Type     string             `json:"type"`
	Function FunctionDefinition `json:"function"`
}

type FunctionDefinition struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	Parameters  map[string]interface{} `json:"parameters"`
}

// Helper methods for Message
func NewTextMessage(role, content string) Message {
	return Message{
		Role:    role,
		Content: content,
	}
}

func NewMultiModalMessage(role string, contents []interface{}) Message {
	return Message{
		Role:    role,
		Content: contents,
	}
}

func NewToolMessage(toolCallId, content string) Message {
	return Message{
		Role:       "tool",
		Content:    content,
		ToolCallId: &toolCallId,
	}
}

func (m *Message) GetTextContent() string {
	switch content := m.Content.(type) {
	case string:
		return content
	case []interface{}:
		for _, item := range content {
			if textContent, ok := item.(map[string]interface{}); ok {
				if textContent["type"] == "text" {
					if text, exists := textContent["text"]; exists {
						if textStr, ok := text.(string); ok {
							return textStr
						}
					}
				}
			}
		}
	}
	return ""
}

func (m *Message) IsTextOnly() bool {
	_, ok := m.Content.(string)
	return ok
}

func (m *Message) HasToolCalls() bool {
	return len(m.ToolCalls) > 0
}

type Request struct {
	Messages       []Message       `json:"messages"`
	Options        ModelParameters `json:",inline"`
	Tools          []Tool          `json:"tools,omitempty"`
	ToolChoice     interface{}     `json:"tool_choice,omitempty"`
	ResponseFormat *ResponseFormat `json:"response_format,omitempty"`
	Stream         *bool           `json:"stream,omitempty"`
	StreamOptions  *StreamOptions  `json:"stream_options,omitempty"`
}

type ResponseFormat struct {
	Type   string                 `json:"type"`
	Schema map[string]interface{} `json:"schema,omitempty"`
}

type StreamOptions struct {
	IncludeUsage *bool `json:"include_usage,omitempty"`
}

type Response struct {
	Content      string        `json:"content"`
	Success      bool          `json:"success"`
	Error        string        `json:"error,omitempty"`
	ResponseTime time.Duration `json:"response_time"`
	ToolCalls    []ToolCall    `json:"tool_calls,omitempty"`
	Usage        *Usage        `json:"usage,omitempty"`
	FinishReason *string       `json:"finish_reason,omitempty"`
}

type Usage struct {
	PromptTokens     int `json:"prompt_tokens"`
	CompletionTokens int `json:"completion_tokens"`
	TotalTokens      int `json:"total_tokens"`
}

type DataRow struct {
	Index int                    `json:"index"`
	Text  string                 `json:"text"`
	Data  map[string]interface{} `json:"data"`
}

type Result struct {
	Index           int                    `json:"index"`
	InputText       string                 `json:"input_text"`
	OriginalData    map[string]interface{} `json:"original_data,omitempty"`
	GroundTruth     string                 `json:"ground_truth"`
	FinalAnswer     string                 `json:"final_answer"`
	RawResponse     string                 `json:"raw_response"`
	ThinkingContent string                 `json:"thinking_content,omitempty"`
	Success         bool                   `json:"success"`
	Error           string                 `json:"error,omitempty"`
	ResponseTime    time.Duration          `json:"response_time"`
	Attempts        []Attempt              `json:"attempts,omitempty"`
	Consensus       *Consensus             `json:"consensus,omitempty"`
	ToolCalls       []ToolCall             `json:"tool_calls,omitempty"`
	Usage           *Usage                 `json:"usage,omitempty"`
}

type Attempt struct {
	Response     string        `json:"response"`
	Answer       string        `json:"answer"`
	ResponseTime time.Duration `json:"response_time"`
	Success      bool          `json:"success"`
	Error        string        `json:"error,omitempty"`
	ToolCalls    []ToolCall    `json:"tool_calls,omitempty"`
}

type Consensus struct {
	FinalAnswer  string         `json:"final_answer"`
	Count        int            `json:"count"`
	Total        int            `json:"total"`
	Ratio        float64        `json:"ratio"`
	Distribution map[string]int `json:"distribution"`
}

type Client interface {
	SendRequest(ctx context.Context, req Request) (*Response, error)
	HealthCheck(ctx context.Context) error
	GetServerInfo(ctx context.Context) (*ServerInfo, error)
	Close() error
}

type ServerInfo struct {
	ServerURL  string                 `json:"server_url"`
	ServerType string                 `json:"server_type"`
	Timestamp  float64                `json:"timestamp"`
	Config     map[string]interface{} `json:"config,omitempty"`
	Models     map[string]interface{} `json:"models,omitempty"`
	Features   map[string]interface{} `json:"features,omitempty"`
	Available  bool                   `json:"available"`
}

// Resume state structures for restart functionality
type ResumeState struct {
	ConfigFile      string        `json:"config_file"`
	InputFile       string        `json:"input_file"`
	OutputDirectory string        `json:"output_directory"`
	ProcessedItems  []int         `json:"processed_items"`
	CompletedCount  int           `json:"completed_count"`
	TotalCount      int           `json:"total_count"`
	Results         []Result      `json:"results"`
	Timestamp       time.Time     `json:"timestamp"`
	Options         ResumeOptions `json:"options"`
}

type ResumeOptions struct {
	Workers      int  `json:"workers"`
	Repeat       int  `json:"repeat"`
	Limit        int  `json:"limit"`
	ShowProgress bool `json:"show_progress"`
	Verbose      bool `json:"verbose"`
}



// File: internal/logger/logger.go
// Base: logger

package logger

import (
	"fmt"
	"os"
	"runtime"
	"time"

	"github.com/fatih/color"
)

const (
	ColorReset  = "\033[0m"
	ColorRed    = "\033[31m"
	ColorGreen  = "\033[32m"
	ColorYellow = "\033[33m"
	ColorBlue   = "\033[34m"
	ColorPurple = "\033[35m"
	ColorCyan   = "\033[36m"
	ColorGray   = "\033[90m"
	ColorBold   = "\033[1m"
	ColorWhite  = "\033[37m"
)

type LogLevel int

const (
	DEBUG LogLevel = iota
	INFO
	WARNING
	ERROR
	PROGRESS
)

var (
	currentLevel = INFO
	verbose      = false
	// Enhanced color functions using fatih/color
	colorError   = color.New(color.FgRed, color.Bold)
	colorSuccess = color.New(color.FgGreen, color.Bold)
	colorWarning = color.New(color.FgYellow, color.Bold)
	colorInfo    = color.New(color.FgCyan, color.Bold)
	colorDebug   = color.New(color.FgHiBlack)
	colorHeader  = color.New(color.FgMagenta, color.Bold, color.Underline)
	colorValue   = color.New(color.FgHiGreen)
	colorKey     = color.New(color.FgBlue)
)

func SetLevel(level LogLevel) {
	currentLevel = level
}

func SetVerbose(enabled bool) {
	verbose = enabled
	if enabled {
		currentLevel = DEBUG
	}
}

// IsVerbose returns whether verbose logging is enabled
func IsVerbose() bool {
	return verbose
}

func (l LogLevel) String() string {
	switch l {
	case DEBUG:
		return "DEBUG"
	case INFO:
		return "INFO"
	case WARNING:
		return "WARNING"
	case ERROR:
		return "ERROR"
	case PROGRESS:
		return "PROGRESS"
	default:
		return "UNKNOWN"
	}
}

func (l LogLevel) Color() string {
	switch l {
	case DEBUG:
		return ColorGray
	case INFO:
		return ColorBlue
	case WARNING:
		return ColorYellow
	case ERROR:
		return ColorRed
	case PROGRESS:
		return ColorCyan // Cyan for progress bar
	default:
		return ColorReset
	}
}

// GetColorFunc returns the enhanced color function for the log level
func (l LogLevel) GetColorFunc() *color.Color {
	switch l {
	case DEBUG:
		return colorDebug
	case INFO:
		return colorInfo
	case WARNING:
		return colorWarning
	case ERROR:
		return colorError
	case PROGRESS:
		return colorInfo
	default:
		return color.New(color.Reset)
	}
}

func formatMessage(level LogLevel, message string, args ...interface{}) string {
	now := time.Now()
	timestamp := colorKey.Sprintf("%s", now.Format("15:04")) // HH:MM format with color

	levelColorFunc := level.GetColorFunc()
	levelStr := levelColorFunc.Sprintf("%s", level.String())

	var formattedMsg string
	if level == PROGRESS {
		formattedMsg = message // Message is already pre-formatted for progress bar
	} else {
		formattedMsg = fmt.Sprintf(message, args...)
	}

	return fmt.Sprintf("%s - %s : %s", timestamp, levelStr, formattedMsg)
}

func shouldLog(level LogLevel) bool {
	if level == PROGRESS {
		return true
	}
	return level >= currentLevel
}

func Debug(message string, args ...interface{}) {
	if shouldLog(DEBUG) {
		fmt.Println(formatMessage(DEBUG, message, args...))
	}
}

func Info(message string, args ...interface{}) {
	if shouldLog(INFO) {
		fmt.Println(formatMessage(INFO, message, args...))
	}
}

func Warning(message string, args ...interface{}) {
	if shouldLog(WARNING) {
		fmt.Println(formatMessage(WARNING, message, args...))
	}
}

func Error(message string, args ...interface{}) {
	if shouldLog(ERROR) {
		fmt.Fprintf(os.Stderr, formatMessage(ERROR, message, args...)+"\n")
	}
}

func DebugRequest(provider, url string, params map[string]interface{}) {
	if !shouldLog(DEBUG) {
		return
	}

	Debug("Sending request to %s provider", provider)
	Debug("Request URL: %s", url)

	if len(params) > 0 {
		Debug("Request parameters:")
		for key, value := range params {
			Debug("  %s: %v", key, value)
		}
	}
}

func DebugResponse(statusCode int, response string, duration time.Duration) {
	if !shouldLog(DEBUG) {
		return
	}

	Debug("Response received (HTTP %d) in %v", statusCode, duration)

	if len(response) > 500 {
		Debug("Response body (truncated): %s...", response[:500])
	} else {
		Debug("Response body: %s", response)
	}
}

func DebugSystem() {
	if !shouldLog(DEBUG) {
		return
	}

	Debug("System information:")
	Debug("  OS: %s", runtime.GOOS)
	Debug("  Architecture: %s", runtime.GOARCH)
	Debug("  Go version: %s", runtime.Version())
	Debug("  CPU count: %d", runtime.NumCPU())
}

func DebugConfig(config interface{}) {
	if !shouldLog(DEBUG) {
		return
	}

	Debug("Configuration loaded:")
	Debug("  %+v", config)
}

func Progress(message string, args ...interface{}) {
	fmt.Print("\r\033[K" + formatMessage(PROGRESS, message, args...))
}

func Fatal(message string, args ...interface{}) {
	Error(message, args...)
	os.Exit(1)
}

func Success(message string, args ...interface{}) {
	if shouldLog(INFO) {
		timestamp := colorKey.Sprintf("%s", time.Now().Format("15:04"))
		successMsg := colorSuccess.Sprintf("SUCCESS")
		formattedMsg := fmt.Sprintf(message, args...)
		fmt.Printf("%s - %s : %s\n", timestamp, successMsg, formattedMsg)
	}
}

func Header(message string, args ...interface{}) {
	if shouldLog(INFO) {
		timestamp := colorKey.Sprintf("%s", time.Now().Format("15:04"))
		headerMsg := colorHeader.Sprintf("HEADER")
		formattedMsg := colorHeader.Sprintf(message, args...)
		fmt.Printf("%s - %s : %s\n", timestamp, headerMsg, formattedMsg)
	}
}



// File: internal/processor/processor.go
// Base: processor

package processor

import (
	"context"
	"encoding/csv"
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/Vitruves/llm-client/internal/client"
	"github.com/Vitruves/llm-client/internal/loader"
	"github.com/Vitruves/llm-client/internal/logger"
	"github.com/Vitruves/llm-client/internal/metrics"
	"github.com/Vitruves/llm-client/internal/models"
	"github.com/Vitruves/llm-client/internal/parser"
	"github.com/Vitruves/llm-client/internal/progress"
	"github.com/Vitruves/llm-client/internal/writer"

	"github.com/parquet-go/parquet-go"
	"github.com/xuri/excelize/v2"
)

type Processor struct {
	config         *models.Config
	client         models.Client
	parser         *parser.Parser
	metricsCalc    *metrics.Calculator
	cancelRequests int32
	configFile     string
	inputFile      string
}

type Options struct {
	Limit        int
	ShowProgress bool
	Verbose      bool
	ResumeFile   string
}

func New(config *models.Config) *Processor {
	proc := &Processor{
		config: config,
		parser: parser.New(config.Classification.Parsing),
	}

	if config.Processing.LiveMetrics != nil && config.Processing.LiveMetrics.Enabled {
		// Use classes from config, fallback to parsing Find field, then default
		classes := config.Processing.LiveMetrics.Classes
		if len(classes) == 0 && len(config.Classification.Parsing.Find) > 0 {
			classes = config.Classification.Parsing.Find
		}
		if len(classes) == 0 {
			classes = []string{"0", "1", "2"} // Final fallback for backward compatibility
		}
		proc.metricsCalc = metrics.NewCalculator(config.Processing.LiveMetrics, classes)
	}

	return proc
}

func (p *Processor) SetConfigFile(configFile string) {
	p.configFile = configFile
}

func (p *Processor) ProcessFile(ctx context.Context, inputFile string, opts Options) error {
	// Store input file for resume functionality
	p.inputFile = inputFile
	var data []models.DataRow
	var err error

	if opts.ResumeFile != "" {
		return p.resumeProcessing(ctx, opts)
	}

	p.client, err = client.NewClient(p.config)
	if err != nil {
		return fmt.Errorf("failed to create client: %w", err)
	}
	defer p.client.Close()

	if err := p.client.HealthCheck(ctx); err != nil {
		return fmt.Errorf("health check failed: %w", err)
	}

	data, err = loader.LoadData(inputFile)
	if err != nil {
		return fmt.Errorf("failed to load data: %w", err)
	}

	if opts.Limit > 0 && opts.Limit < len(data) {
		data = data[:opts.Limit]
	}

	p.printProcessingInfo(data)

	// Choose processing method based on streaming configuration
	var results []models.Result
	if p.config.Output.StreamOutput {
		results = p.processWithWorkersStreaming(ctx, data, opts)
	} else {
		results = p.processWithWorkersSimple(ctx, data, opts)
	}

	if atomic.LoadInt32(&p.cancelRequests) > 0 {
		resumeFile := p.savePartialResults(data, results, opts)
		logger.Warning("Processing cancelled. Processed %d items before cancellation.", len(results))
		if resumeFile != "" {
			if p.configFile != "" {
				logger.Info("Resume with: ./llm-client run -c %s --resume %s", p.configFile, resumeFile)
			} else {
				logger.Info("Resume with: ./llm-client run --resume %s", resumeFile)
			}
		}
		return fmt.Errorf("processing cancelled")
	}

	if p.config.Output.StreamOutput {
		logger.Success("Processing completed successfully. Results written via streaming.")
		if err := p.saveConfigAndServerInfo(); err != nil {
			logger.Warning("Failed to save config and server info: %v", err)
		}
		return nil
	}

	if err := p.saveResults(results); err != nil {
		return err
	}

	if err := p.saveConfigAndServerInfo(); err != nil {
		logger.Warning("Failed to save config and server info: %v", err)
	}

	return nil
}

func (p *Processor) savePartialResults(allData []models.DataRow, results []models.Result, opts Options) string {
	if len(results) == 0 {
		return ""
	}

	processedItems := make(map[int]bool)
	for _, result := range results {
		processedItems[result.Index] = true
	}

	resumeState := models.ResumeState{
		ConfigFile:      p.configFile,
		InputFile:       p.inputFile,
		OutputDirectory: p.config.Output.Directory,
		ProcessedItems:  make([]int, 0, len(processedItems)),
		CompletedCount:  len(results),
		TotalCount:      len(allData),
		Results:         results,
		Timestamp:       time.Now(),
		Options: models.ResumeOptions{
			Workers:      p.config.Processing.Workers,
			Repeat:       p.config.Processing.Repeat,
			Limit:        opts.Limit,
			ShowProgress: opts.ShowProgress,
			Verbose:      opts.Verbose,
		},
	}

	for idx := range processedItems {
		resumeState.ProcessedItems = append(resumeState.ProcessedItems, idx)
	}

	timestamp := time.Now().Format("20060102_150405")
	resumeFile := filepath.Join(p.config.Output.Directory, fmt.Sprintf("resume_cancelled_%s.json", timestamp))

	if err := os.MkdirAll(p.config.Output.Directory, 0755); err != nil {
		logger.Warning("Failed to create output directory: %v", err)
		resumeFile = fmt.Sprintf("resume_cancelled_%s.json", timestamp)
	}

	file, err := os.Create(resumeFile)
	if err != nil {
		logger.Warning("Failed to create resume file: %v", err)
		return ""
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")
	if err := encoder.Encode(resumeState); err != nil {
		logger.Warning("Failed to save resume state: %v", err)
		return ""
	}

	return resumeFile
}

func (p *Processor) resumeProcessing(ctx context.Context, opts Options) error {
	resumeState, err := p.loadResumeState(opts.ResumeFile)
	if err != nil {
		return fmt.Errorf("failed to load resume state: %w", err)
	}

	p.client, err = client.NewClient(p.config)
	if err != nil {
		return fmt.Errorf("failed to create client: %w", err)
	}
	defer p.client.Close()

	if err := p.client.HealthCheck(ctx); err != nil {
		return fmt.Errorf("health check failed: %w", err)
	}

	logger.Info("Resuming processing from: %s", resumeState.InputFile)
	logger.Info("Previously completed: %d/%d items", resumeState.CompletedCount, resumeState.TotalCount)

	data, err := loader.LoadData(resumeState.InputFile)
	if err != nil {
		return fmt.Errorf("failed to load original data: %w", err)
	}

	processedItems := make(map[int]bool)
	for _, idx := range resumeState.ProcessedItems {
		processedItems[idx] = true
	}

	var remainingData []models.DataRow
	for _, row := range data {
		if !processedItems[row.Index] {
			remainingData = append(remainingData, row)
		}
	}

	if len(remainingData) == 0 {
		logger.Info("All items already processed. Saving final results...")
		return p.saveResults(resumeState.Results)
	}

	logger.Info("Resuming with %d remaining items", len(remainingData))

	newResults := p.processWithWorkersWithResume(ctx, remainingData, opts, processedItems)

	allResults := append(resumeState.Results, newResults...)

	if atomic.LoadInt32(&p.cancelRequests) > 0 {
		logger.Warning("Processing cancelled again. Processed %d additional items.", len(newResults))
		logger.Info("Updating resume state...")

		for _, result := range newResults {
			processedItems[result.Index] = true
		}

		resumeFile := p.saveResumeState(resumeState.InputFile, data, allResults, processedItems, opts)
		logger.Info("To resume processing, run:")
		logger.Info("./llm-client run --resume %s", resumeFile)

		return fmt.Errorf("processing cancelled")
	}

	os.Remove(opts.ResumeFile)
	logger.Success("Processing completed successfully. Resume file cleaned up.")

	return p.saveResults(allResults)
}

func (p *Processor) saveResumeState(inputFile string, allData []models.DataRow, results []models.Result, processedItems map[int]bool, opts Options) string {
	processedList := make([]int, 0, len(processedItems))
	for idx := range processedItems {
		processedList = append(processedList, idx)
	}

	resumeState := models.ResumeState{
		ConfigFile:      "", // Will be set by caller
		InputFile:       inputFile,
		OutputDirectory: p.config.Output.Directory,
		ProcessedItems:  processedList,
		CompletedCount:  len(results),
		TotalCount:      len(allData),
		Results:         results,
		Timestamp:       time.Now(),
		Options: models.ResumeOptions{
			Workers:      p.config.Processing.Workers,
			Repeat:       p.config.Processing.Repeat,
			Limit:        opts.Limit,
			ShowProgress: opts.ShowProgress,
			Verbose:      opts.Verbose,
		},
	}

	timestamp := time.Now().Format("20060102_150405")
	resumeFile := filepath.Join(p.config.Output.Directory, fmt.Sprintf("resume_%s.json", timestamp))

	if err := os.MkdirAll(p.config.Output.Directory, 0755); err != nil {
		logger.Warning("Failed to create output directory: %v", err)
		resumeFile = fmt.Sprintf("resume_%s.json", timestamp)
	}

	file, err := os.Create(resumeFile)
	if err != nil {
		logger.Warning("Failed to create resume file: %v", err)
		return resumeFile
	}
	defer file.Close()

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")
	if err := encoder.Encode(resumeState); err != nil {
		logger.Warning("Failed to save resume state: %v", err)
	}

	return resumeFile
}

func (p *Processor) loadResumeState(resumeFile string) (*models.ResumeState, error) {
	data, err := os.ReadFile(resumeFile)
	if err != nil {
		return nil, err
	}

	var resumeState models.ResumeState
	if err := json.Unmarshal(data, &resumeState); err != nil {
		return nil, err
	}

	return &resumeState, nil
}

func (p *Processor) printProcessingInfo(data []models.DataRow) {
	logger.Info("Starting processing: %d items, %d workers", len(data), p.config.Processing.Workers)
	if p.config.Processing.Repeat > 1 {
		logger.Info("Consensus mode: %d requests per item", p.config.Processing.Repeat)
	}
	if p.metricsCalc != nil {
		logger.Info("Live metrics: %s", p.metricsCalc.GetMetricName())
	}
}

func (p *Processor) calculateOptimalBufferSizes(dataLen int) (resultBuffer, dataBuffer int) {
	workers := p.config.Processing.Workers

	resultBuffer = workers * 4
	if resultBuffer > dataLen {
		resultBuffer = dataLen
	}
	if resultBuffer < 10 {
		resultBuffer = 10
	}

	dataBuffer = workers * 2
	if dataBuffer > 1000 {
		dataBuffer = 1000
	}
	if dataBuffer < 10 {
		dataBuffer = 10
	}

	return resultBuffer, dataBuffer
}

func (p *Processor) processWithWorkersSimple(ctx context.Context, data []models.DataRow, opts Options) []models.Result {
	resultBuffer, dataBuffer := p.calculateOptimalBufferSizes(len(data))
	resultsCh := make(chan models.Result, resultBuffer)
	dataCh := make(chan models.DataRow, dataBuffer)

	var processed, succeeded, failed int64
	var prog *progress.Progress

	if opts.ShowProgress {
		prog = progress.NewWithMetrics(len(data), p.metricsCalc)
		prog.Start()
	}

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	go p.dataFeeder(ctx, dataCh, data)

	var wg sync.WaitGroup

	for i := 0; i < p.config.Processing.Workers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			for {
				select {
				case <-ctx.Done():
					atomic.StoreInt32(&p.cancelRequests, 1)
					return
				case row, ok := <-dataCh:
					if !ok {
						return
					}

					if atomic.LoadInt32(&p.cancelRequests) > 0 {
						return
					}

					result := p.processRow(ctx, row, opts.Verbose)

					select {
					case resultsCh <- result:
						p.updateCounters(result, &processed, &succeeded, &failed)
						p.logVerboseResult(workerID, row, result, opts.Verbose, atomic.LoadInt64(&processed))
						p.updateProgress(prog, &processed, &succeeded, &failed)
					case <-ctx.Done():
						atomic.StoreInt32(&p.cancelRequests, 1)
						return
					}
				}
			}
		}(i)
	}

	go func() {
		wg.Wait()
		close(resultsCh)
		if prog != nil {
			prog.Stop()
		}
	}()

	return p.collectResults(resultsCh)
}

func (p *Processor) processWithWorkersStreaming(ctx context.Context, data []models.DataRow, opts Options) []models.Result {
	var streamWriter writer.StreamWriter
	var err error

	if p.config.Output.StreamOutput {
		timestamp := time.Now().Format("20060102_150405")
		streamWriter, err = writer.NewStreamWriter(p.config.Output.Format, p.config.Output.Directory, timestamp, &p.config.Output)
		if err != nil {
			logger.Warning("Failed to create stream writer: %v, falling back to batch mode", err)
		} else {
			defer func() {
				if err := streamWriter.Close(); err != nil {
					logger.Warning("Failed to close stream writer: %v", err)
				} else if streamWriter != nil {
					logger.Info("Results streamed to: %s", streamWriter.GetFilename())
				}
			}()
		}
	}

	resultBuffer, dataBuffer := p.calculateOptimalBufferSizes(len(data))
	resultsCh := make(chan models.Result, resultBuffer)
	dataCh := make(chan models.DataRow, dataBuffer)

	var processed, succeeded, failed int64
	var prog *progress.Progress

	if opts.ShowProgress {
		prog = progress.NewWithMetrics(len(data), p.metricsCalc)
		prog.Start()
	}

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	go p.dataFeeder(ctx, dataCh, data)

	var wg sync.WaitGroup

	for i := 0; i < p.config.Processing.Workers; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()

			for {
				select {
				case <-ctx.Done():
					atomic.StoreInt32(&p.cancelRequests, 1)
					return
				case row, ok := <-dataCh:
					if !ok {
						return
					}

					if atomic.LoadInt32(&p.cancelRequests) > 0 {
						return
					}

					result := p.processRow(ctx, row, opts.Verbose)

					select {
					case resultsCh <- result:
						p.updateCounters(result, &processed, &succeeded, &failed)
						p.logVerboseResult(workerID, row, result, opts.Verbose, atomic.LoadInt64(&processed))
						p.updateProgress(prog, &processed, &succeeded, &failed)
					case <-ctx.Done():
						atomic.StoreInt32(&p.cancelRequests, 1)
						return
					}
				}
			}
		}(i)
	}

	go func() {
		wg.Wait()
		close(resultsCh)
		if prog != nil {
			prog.Stop()
		}
	}()

	return p.collectResultsWithStreaming(resultsCh, streamWriter)
}

func (p *Processor) processWithWorkersWithResume(ctx context.Context, data []models.DataRow, opts Options, processedItems map[int]bool) []models.Result {
	resultBuffer, dataBuffer := p.calculateOptimalBufferSizes(len(data))
	resultsCh := make(chan models.Result, resultBuffer)
	dataCh := make(chan models.DataRow, dataBuffer)

	var processed, succeeded, failed int64
	var prog *progress.Progress

	if opts.ShowProgress {
		prog = progress.NewWithMetrics(len(data), p.metricsCalc)
		prog.Start()
	}

	ctx, cancel := context.WithCancel(ctx)
	defer cancel()

	go p.dataFeeder(ctx, dataCh, data)

	var wg sync.WaitGroup

	for i := 0; i < p.config.Processing.Workers; i++ {
		wg.Add(1)
		go p.workerWithResume(ctx, i, dataCh, resultsCh, &wg, &processed, &succeeded, &failed, prog, opts.Verbose, processedItems)
	}

	go func() {
		wg.Wait()
		close(resultsCh)
		if prog != nil {
			prog.Stop()
		}
	}()

	return p.collectResults(resultsCh)
}

func (p *Processor) dataFeeder(ctx context.Context, dataCh chan<- models.DataRow, data []models.DataRow) {
	defer close(dataCh)
	for _, row := range data {
		select {
		case dataCh <- row:
		case <-ctx.Done():
			return
		}
	}
}

func (p *Processor) workerWithResume(ctx context.Context, workerID int, dataCh <-chan models.DataRow,
	resultsCh chan<- models.Result, wg *sync.WaitGroup,
	processed, succeeded, failed *int64, prog *progress.Progress, verbose bool,
	processedItems map[int]bool) {

	defer wg.Done()

	for {
		select {
		case <-ctx.Done():
			atomic.StoreInt32(&p.cancelRequests, 1)
			return
		case row, ok := <-dataCh:
			if !ok {
				return
			}

			if atomic.LoadInt32(&p.cancelRequests) > 0 {
				return
			}

			result := p.processRow(ctx, row, verbose)

			select {
			case resultsCh <- result:
				processedItems[result.Index] = true
				p.updateCounters(result, processed, succeeded, failed)
				p.logVerboseResult(workerID, row, result, verbose, atomic.LoadInt64(processed))
				p.updateProgress(prog, processed, succeeded, failed)

			case <-ctx.Done():
				atomic.StoreInt32(&p.cancelRequests, 1)
				return
			}
		}
	}
}

func (p *Processor) updateCounters(result models.Result, processed, succeeded, failed *int64) {
	atomic.AddInt64(processed, 1)
	if result.Success {
		atomic.AddInt64(succeeded, 1)
	} else {
		atomic.AddInt64(failed, 1)
	}
}

func (p *Processor) logVerboseResult(workerID int, row models.DataRow, result models.Result, verbose bool, current int64) {
	if !verbose || current > 10 {
		return
	}

	consensusInfo := ""
	if result.Consensus != nil {
		consensusInfo = fmt.Sprintf(" [consensus: %s %.1f%%]",
			result.Consensus.FinalAnswer, result.Consensus.Ratio*100)
	}

	logger.Debug("Worker %d completed row %d: %s (%.1fms)%s",
		workerID, row.Index, result.FinalAnswer,
		float64(result.ResponseTime.Nanoseconds())/1e6, consensusInfo)
}

func (p *Processor) updateProgress(prog *progress.Progress, processed, succeeded, failed *int64) {
	if prog != nil {
		prog.Update(int(atomic.LoadInt64(processed)),
			int(atomic.LoadInt64(succeeded)),
			int(atomic.LoadInt64(failed)))
	}
}

func (p *Processor) collectResults(resultsCh <-chan models.Result) []models.Result {
	var allResults []models.Result
	for result := range resultsCh {
		allResults = append(allResults, result)
		if atomic.LoadInt32(&p.cancelRequests) > 0 {
			break
		}
	}
	return allResults
}

func (p *Processor) collectResultsWithStreaming(resultsCh <-chan models.Result, streamWriter writer.StreamWriter) []models.Result {
	var allResults []models.Result
	saveEvery := p.config.Output.StreamSaveEvery
	if saveEvery <= 0 {
		saveEvery = 1 // Default to flush every result
	}

	for result := range resultsCh {
		allResults = append(allResults, result)

		if streamWriter != nil {
			if err := streamWriter.WriteResult(result); err != nil {
				logger.Warning("Failed to stream write result %d: %v", result.Index, err)
			}

			if len(allResults)%saveEvery == 0 {
				if err := streamWriter.Flush(); err != nil {
					logger.Warning("Failed to flush stream writer: %v", err)
				}
			}
		}

		if atomic.LoadInt32(&p.cancelRequests) > 0 {
			break
		}
	}
	return allResults
}

func (p *Processor) processRow(ctx context.Context, row models.DataRow, verbose bool) models.Result {
	if atomic.LoadInt32(&p.cancelRequests) > 0 {
		return models.Result{
			Index:     row.Index,
			InputText: row.Text,
			Success:   false,
			Error:     "cancelled",
		}
	}

	if p.config.Processing.Repeat <= 1 {
		return p.processSingle(ctx, row, verbose)
	}
	return p.processMultiple(ctx, row, verbose)
}

func (p *Processor) processSingle(ctx context.Context, row models.DataRow, verbose bool) models.Result {
	start := time.Now()
	groundTruth := p.extractGroundTruth(row)

	if verbose {
		logger.Debug("Processing row %d: %s", row.Index, p.truncate(p.extractInputText(row), 80))
	}

	req := p.buildRequest(row)
	resp, err := p.client.SendRequest(ctx, req)
	responseTime := time.Since(start)

	if err != nil {
		if verbose {
			logger.Debug("Row %d failed: %v", row.Index, err)
		}
		return p.createErrorResult(row, groundTruth, err.Error(), "", responseTime)
	}

	if !resp.Success {
		if verbose {
			logger.Debug("Row %d error: %s", row.Index, resp.Error)
		}
		return p.createErrorResult(row, groundTruth, resp.Error, resp.Content, responseTime)
	}

	return p.createSuccessResult(row, groundTruth, resp.Content, responseTime, verbose)
}

func (p *Processor) processMultiple(ctx context.Context, row models.DataRow, verbose bool) models.Result {
	start := time.Now()
	groundTruth := p.extractGroundTruth(row)

	if verbose {
		logger.Debug("Processing row %d with %d attempts: %s",
			row.Index, p.config.Processing.Repeat, p.truncate(p.extractInputText(row), 80))
	}

	req := p.buildRequest(row)
	attempts, answers := p.executeAttempts(ctx, req, verbose)

	result := models.Result{
		Index:        row.Index,
		InputText:    p.extractInputText(row),
		OriginalData: row.Data,
		GroundTruth:  groundTruth,
		ResponseTime: time.Since(start),
		Attempts:     attempts,
	}

	if len(answers) == 0 {
		result.Success = false
		result.Error = "all attempts failed"
		return result
	}

	p.finalizeMultipleResult(&result, attempts, answers, verbose)
	return result
}

func (p *Processor) executeAttempts(ctx context.Context, req models.Request, verbose bool) ([]models.Attempt, []string) {
	attempts := make([]models.Attempt, 0, p.config.Processing.Repeat)
	answers := make([]string, 0, p.config.Processing.Repeat)

	for i := 0; i < p.config.Processing.Repeat; i++ {
		select {
		case <-ctx.Done():
			return attempts, answers
		default:
		}

		if atomic.LoadInt32(&p.cancelRequests) > 0 {
			return attempts, answers
		}

		if i > 0 && p.config.Processing.RateLimit {
			time.Sleep(time.Second)
		}

		attempt := p.executeAttempt(ctx, req)
		attempts = append(attempts, attempt)

		if attempt.Success {
			answers = append(answers, attempt.Answer)
		}

		if verbose {
			status := attempt.Answer
			if !attempt.Success {
				status = "failed"
			}
			logger.Debug("Attempt %d/%d: %s", i+1, p.config.Processing.Repeat, status)
		}
	}

	return attempts, answers
}

func (p *Processor) executeAttempt(ctx context.Context, req models.Request) models.Attempt {
	attemptStart := time.Now()
	resp, err := p.client.SendRequest(ctx, req)

	attempt := models.Attempt{
		ResponseTime: time.Since(attemptStart),
		Success:      false,
	}

	if err != nil {
		attempt.Error = err.Error()
		return attempt
	}

	if !resp.Success {
		attempt.Error = resp.Error
		attempt.Response = resp.Content
		return attempt
	}

	attempt.Success = true
	attempt.Response = resp.Content

	if p.config.Processing.MinimalMode {
		attempt.Answer = resp.Content
	} else {
		finalAnswer, _ := p.parser.ParseWithThinking(resp.Content)
		attempt.Answer = metrics.NormalizeLabel(finalAnswer)
	}

	return attempt
}

func (p *Processor) finalizeMultipleResult(result *models.Result, attempts []models.Attempt, answers []string, verbose bool) {
	var consensus *models.Consensus

	if p.config.Processing.MinimalMode {
		result.Success = true
		result.FinalAnswer = attempts[0].Response
		result.RawResponse = attempts[0].Response
		result.ThinkingContent = ""

		consensus = &models.Consensus{
			FinalAnswer:  attempts[0].Response,
			Count:        1,
			Total:        len(attempts),
			Ratio:        1.0,
			Distribution: map[string]int{attempts[0].Response: 1},
		}
	} else {
		consensus = p.calculateConsensus(answers)
		result.Success = true
		result.FinalAnswer = consensus.FinalAnswer
		result.RawResponse = attempts[0].Response

		if len(attempts) > 0 && attempts[0].Success {
			_, thinkingContent := p.parser.ParseWithThinking(attempts[0].Response)
			result.ThinkingContent = thinkingContent
		}
	}

	result.Consensus = consensus

	if verbose {
		if p.config.Processing.MinimalMode {
			logger.Debug("[MINIMAL] Row %d completed with %d attempts",
				result.Index, len(attempts))
		} else {
			logger.Debug("Row %d consensus: %s (%.1f%% agreement, %d/%d successful)",
				result.Index, consensus.FinalAnswer, consensus.Ratio*100, len(answers), p.config.Processing.Repeat)

			if len(consensus.Distribution) > 1 {
				logger.Debug("   Distribution: %v", consensus.Distribution)
			}
		}
	}

	if !p.config.Processing.MinimalMode {
		p.updateLiveMetrics(*result)
	}
}

func (p *Processor) buildRequest(row models.DataRow) models.Request {
	return models.Request{
		Messages: []models.Message{
			models.NewTextMessage("system", p.config.Classification.Template.System),
			models.NewTextMessage("user", p.applyTemplate(row)),
		},
		Options: p.config.Model.Parameters,
	}
}

func (p *Processor) createErrorResult(row models.DataRow, groundTruth, errorMsg, rawResponse string, responseTime time.Duration) models.Result {
	return models.Result{
		Index:        row.Index,
		InputText:    p.extractInputText(row),
		OriginalData: row.Data,
		GroundTruth:  groundTruth,
		Success:      false,
		Error:        errorMsg,
		RawResponse:  rawResponse,
		ResponseTime: responseTime,
	}
}

func (p *Processor) createSuccessResult(row models.DataRow, groundTruth, rawResponse string, responseTime time.Duration, verbose bool) models.Result {
	var finalAnswer, thinkingContent string
	var normalizedAnswer string

	if p.config.Processing.MinimalMode {
		finalAnswer = rawResponse
		normalizedAnswer = rawResponse
		thinkingContent = ""
	} else {
		finalAnswer, thinkingContent = p.parser.ParseWithThinking(rawResponse)
		normalizedAnswer = metrics.NormalizeLabel(finalAnswer)
	}

	if verbose {
		status := "OK"
		if !p.config.Processing.MinimalMode && normalizedAnswer != groundTruth {
			status = "WRONG"
		}
		if p.config.Processing.MinimalMode {
			logger.Debug("[MINIMAL] Row %d completed (%.2fms)",
				row.Index, float64(responseTime.Nanoseconds())/1e6)
		} else {
			logger.Debug("[%s] Row %d completed: %s (GT: %s) (%.2fms)",
				status, row.Index, normalizedAnswer, groundTruth, float64(responseTime.Nanoseconds())/1e6)
			if thinkingContent != "" {
				logger.Debug("Row %d thinking: %s", row.Index, p.truncate(thinkingContent, 100))
			}
		}
	}

	result := models.Result{
		Index:           row.Index,
		InputText:       p.extractInputText(row),
		OriginalData:    row.Data,
		GroundTruth:     groundTruth,
		FinalAnswer:     normalizedAnswer,
		RawResponse:     rawResponse,
		ThinkingContent: thinkingContent,
		Success:         true,
		ResponseTime:    responseTime,
	}

	if !p.config.Processing.MinimalMode {
		p.updateLiveMetrics(result)
	}
	return result
}

func (p *Processor) calculateConsensus(answers []string) *models.Consensus {
	distribution := make(map[string]int)
	for _, answer := range answers {
		distribution[answer]++
	}

	var maxCount int
	var finalAnswer string
	for answer, count := range distribution {
		if count > maxCount {
			maxCount = count
			finalAnswer = answer
		}
	}

	return &models.Consensus{
		FinalAnswer:  finalAnswer,
		Count:        maxCount,
		Total:        len(answers),
		Ratio:        float64(maxCount) / float64(len(answers)),
		Distribution: distribution,
	}
}

func (p *Processor) extractGroundTruth(row models.DataRow) string {
	if p.config.Processing.LiveMetrics == nil {
		return ""
	}

	gtField := p.config.Processing.LiveMetrics.GroundTruth
	if gtValue, ok := row.Data[gtField]; ok {
		return metrics.NormalizeLabel(fmt.Sprintf("%v", gtValue))
	}
	return ""
}

func (p *Processor) extractInputText(row models.DataRow) string {
	// Priority 1: Use field mapping from classification config
	if p.config.Classification.FieldMapping != nil && p.config.Classification.FieldMapping.InputTextField != "" {
		fieldName := p.config.Classification.FieldMapping.InputTextField
		if value, ok := row.Data[fieldName]; ok {
			return fmt.Sprintf("%v", value)
		}
	}

	if p.config.Output.InputTextField != "" {
		if value, ok := row.Data[p.config.Output.InputTextField]; ok {
			return fmt.Sprintf("%v", value)
		}
	}

	template := p.config.Classification.Template.User

	placeholders := []string{"REVIEW", "review", "TEXT", "text", "CONTENT", "content", "MESSAGE", "message"}

	for _, placeholder := range placeholders {
		if strings.Contains(template, "{"+placeholder+"}") {
			for _, key := range []string{placeholder, strings.ToLower(placeholder), strings.ToUpper(placeholder)} {
				if value, ok := row.Data[key]; ok {
					return fmt.Sprintf("%v", value)
				}
			}
		}
	}

	return row.Text
}

func (p *Processor) updateLiveMetrics(result models.Result) {
	if p.metricsCalc == nil || !result.Success || result.GroundTruth == "" {
		return
	}
	p.metricsCalc.AddResult(result.FinalAnswer, result.GroundTruth)
}

func (p *Processor) applyTemplate(row models.DataRow) string {
	template := p.config.Classification.Template.User
	template = strings.ReplaceAll(template, "{text}", row.Text)
	template = strings.ReplaceAll(template, "{index}", fmt.Sprintf("%d", row.Index))
	return p.processPlaceholders(template, row.Data, "")
}

func (p *Processor) processPlaceholders(template string, data map[string]interface{}, prefix string) string {
	for key, value := range data {
		fullKey := key
		if prefix != "" {
			fullKey = prefix + "." + key
		}

		placeholders := []string{
			fmt.Sprintf("{%s}", key),
			fmt.Sprintf("{%s}", strings.ToUpper(key)),
			fmt.Sprintf("{%s}", strings.ToLower(key)),
			fmt.Sprintf("{%s}", fullKey),
			fmt.Sprintf("{%s}", strings.ToUpper(fullKey)),
			fmt.Sprintf("{%s}", strings.ToLower(fullKey)),
		}

		var formattedValue string
		switch v := value.(type) {
		case string:
			formattedValue = v
		case int, int64, int32:
			formattedValue = fmt.Sprintf("%d", v)
		case float64, float32:
			formattedValue = fmt.Sprintf("%.2f", v)
		case bool:
			formattedValue = fmt.Sprintf("%t", v)
		case []interface{}:
			items := make([]string, len(v))
			for i, item := range v {
				items[i] = fmt.Sprintf("%v", item)
			}
			formattedValue = strings.Join(items, ", ")
		case map[string]interface{}:
			template = p.processPlaceholders(template, v, fullKey)
			formattedValue = p.formatNestedObject(v)
		default:
			formattedValue = fmt.Sprintf("%v", v)
		}

		for _, placeholder := range placeholders {
			template = strings.ReplaceAll(template, placeholder, formattedValue)
		}
	}

	return template
}

func (p *Processor) formatNestedObject(obj map[string]interface{}) string {
	parts := make([]string, 0, len(obj))
	for k, v := range obj {
		parts = append(parts, fmt.Sprintf("%s: %v", k, v))
	}
	return strings.Join(parts, ", ")
}

func (p *Processor) truncate(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen-3] + "..."
}

func (p *Processor) saveResults(results []models.Result) error {
	if err := os.MkdirAll(p.config.Output.Directory, 0755); err != nil {
		return err
	}

	timestamp := time.Now().Format("20060102_150405")

	switch p.config.Output.Format {
	case "json":
		return p.saveAsJSON(results, timestamp)
	case "csv":
		return p.saveAsCSV(results, timestamp)
	case "parquet":
		return p.saveAsParquet(results, timestamp)
	case "xlsx":
		return p.saveAsXLSX(results, timestamp)
	default:
		return fmt.Errorf("unsupported output format: %s", p.config.Output.Format)
	}
}

func (p *Processor) saveAsJSON(results []models.Result, timestamp string) error {
	filename := filepath.Join(p.config.Output.Directory, fmt.Sprintf("results_%s.json", timestamp))

	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	output := p.buildOutputData(results)

	encoder := json.NewEncoder(file)
	encoder.SetIndent("", "  ")
	return encoder.Encode(output)
}

func (p *Processor) saveAsCSV(results []models.Result, timestamp string) error {
	filename := filepath.Join(p.config.Output.Directory, fmt.Sprintf("results_%s.csv", timestamp))

	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	defer writer.Flush()

	originalHeaders := make(map[string]struct{})
	for _, result := range results {
		for key := range result.OriginalData {
			originalHeaders[key] = struct{}{}
		}
	}

	var sortedOriginalHeaders []string
	for key := range originalHeaders {
		sortedOriginalHeaders = append(sortedOriginalHeaders, key)
	}
	sort.Strings(sortedOriginalHeaders)

	header := []string{"index", "input_text", "ground_truth", "final_answer", "success", "response_time_ms"}
	header = append(header, sortedOriginalHeaders...)

	if p.config.Output.IncludeThinking {
		header = append(header, "thinking_content")
	}
	if p.config.Output.IncludeRawResponse {
		header = append(header, "raw_response")
	}

	if err := writer.Write(header); err != nil {
		return err
	}

	for _, result := range results {
		row := []string{
			strconv.Itoa(result.Index),
			result.InputText,
			result.GroundTruth,
			result.FinalAnswer,
			strconv.FormatBool(result.Success),
			strconv.FormatInt(result.ResponseTime.Nanoseconds()/1000000, 10),
		}

		for _, h := range sortedOriginalHeaders {
			val := result.OriginalData[h]
			row = append(row, fmt.Sprintf("%v", val))
		}

		if p.config.Output.IncludeThinking {
			row = append(row, result.ThinkingContent)
		}
		if p.config.Output.IncludeRawResponse {
			row = append(row, result.RawResponse)
		}

		if err := writer.Write(row); err != nil {
			return err
		}
	}

	return nil
}

// ParquetResult represents the structure for parquet output
type ParquetResult struct {
	Index           int32  `parquet:"index"`
	InputText       string `parquet:"input_text"`
	GroundTruth     string `parquet:"ground_truth"`
	FinalAnswer     string `parquet:"final_answer"`
	Success         bool   `parquet:"success"`
	ResponseTimeMs  int64  `parquet:"response_time_ms"`
	ThinkingContent string `parquet:"thinking_content,optional"`
	RawResponse     string `parquet:"raw_response,optional"`
	OriginalData    string `parquet:"original_data,optional"`
}

func (p *Processor) saveAsParquet(results []models.Result, timestamp string) error {
	filename := filepath.Join(p.config.Output.Directory, fmt.Sprintf("results_%s.parquet", timestamp))

	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	parquetResults := make([]ParquetResult, len(results))
	for i, result := range results {
		var originalDataStr string
		if len(result.OriginalData) > 0 {
			if jsonBytes, err := json.Marshal(result.OriginalData); err == nil {
				originalDataStr = string(jsonBytes)
			}
		}

		parquetResults[i] = ParquetResult{
			Index:           int32(result.Index),
			InputText:       result.InputText,
			GroundTruth:     result.GroundTruth,
			FinalAnswer:     result.FinalAnswer,
			Success:         result.Success,
			ResponseTimeMs:  result.ResponseTime.Nanoseconds() / 1000000,
			ThinkingContent: result.ThinkingContent,
			RawResponse:     result.RawResponse,
			OriginalData:    originalDataStr,
		}
	}

	return parquet.Write(file, parquetResults)
}

func (p *Processor) saveAsXLSX(results []models.Result, timestamp string) error {
	filename := filepath.Join(p.config.Output.Directory, fmt.Sprintf("results_%s.xlsx", timestamp))

	f := excelize.NewFile()
	defer f.Close()

	sheetName := "Results"
	f.SetSheetName("Sheet1", sheetName)

	originalHeaders := make(map[string]struct{})
	for _, result := range results {
		for key := range result.OriginalData {
			originalHeaders[key] = struct{}{}
		}
	}

	var sortedOriginalHeaders []string
	for key := range originalHeaders {
		sortedOriginalHeaders = append(sortedOriginalHeaders, key)
	}
	sort.Strings(sortedOriginalHeaders)

	headers := []string{"Index", "Input Text", "Ground Truth", "Final Answer", "Success", "Response Time (ms)"}
	headers = append(headers, sortedOriginalHeaders...)

	if p.config.Output.IncludeThinking {
		headers = append(headers, "Thinking Content")
	}
	if p.config.Output.IncludeRawResponse {
		headers = append(headers, "Raw Response")
	}

	for i, header := range headers {
		cell := fmt.Sprintf("%c1", 'A'+i)
		f.SetCellValue(sheetName, cell, header)
	}

	for i, result := range results {
		rowNum := i + 2 // Start from row 2 (after header)

		f.SetCellValue(sheetName, fmt.Sprintf("A%d", rowNum), result.Index)
		f.SetCellValue(sheetName, fmt.Sprintf("B%d", rowNum), result.InputText)
		f.SetCellValue(sheetName, fmt.Sprintf("C%d", rowNum), result.GroundTruth)
		f.SetCellValue(sheetName, fmt.Sprintf("D%d", rowNum), result.FinalAnswer)
		f.SetCellValue(sheetName, fmt.Sprintf("E%d", rowNum), result.Success)
		f.SetCellValue(sheetName, fmt.Sprintf("F%d", rowNum), result.ResponseTime.Nanoseconds()/1000000)

		col := 'G'
		for _, h := range sortedOriginalHeaders {
			val := result.OriginalData[h]
			f.SetCellValue(sheetName, fmt.Sprintf("%c%d", col, rowNum), fmt.Sprintf("%v", val))
			col++
		}

		if p.config.Output.IncludeThinking {
			f.SetCellValue(sheetName, fmt.Sprintf("%c%d", col, rowNum), result.ThinkingContent)
			col++
		}
		if p.config.Output.IncludeRawResponse {
			f.SetCellValue(sheetName, fmt.Sprintf("%c%d", col, rowNum), result.RawResponse)
			col++
		}
	}

	return f.SaveAs(filename)
}

func (p *Processor) buildOutputData(results []models.Result) map[string]interface{} {
	summary := p.buildSummary(results)

	outputResults := make([]map[string]interface{}, len(results))
	for i, result := range results {
		outputResult := map[string]interface{}{
			"index":            result.Index,
			"input_text":       result.InputText,
			"ground_truth":     result.GroundTruth,
			"final_answer":     result.FinalAnswer,
			"raw_response":     result.RawResponse,
			"thinking_content": result.ThinkingContent,
			"success":          result.Success,
			"error":            result.Error,
			"response_time_ms": result.ResponseTime.Nanoseconds() / 1000000,
			"attempts":         result.Attempts,
			"consensus":        result.Consensus,
			"tool_calls":       result.ToolCalls,
			"usage":            result.Usage,
		}
		for k, v := range result.OriginalData {
			outputResult[k] = v
		}
		outputResults[i] = outputResult
	}

	output := map[string]interface{}{
		"results": outputResults,
		"summary": summary,
		"config":  p.config,
	}

	if serverInfo := p.getServerInfo(); serverInfo != nil {
		output["server_info"] = serverInfo
	}

	return output
}

func (p *Processor) buildSummary(results []models.Result) map[string]interface{} {
	successCount := p.countSuccessful(results)
	summary := map[string]interface{}{
		"total":   len(results),
		"success": successCount,
		"failed":  len(results) - successCount,
	}

	if p.metricsCalc != nil {
		summary["live_metrics"] = map[string]interface{}{
			"metric_name":  p.metricsCalc.GetMetricName(),
			"metric_value": p.metricsCalc.GetCurrentMetric(),
		}
	}

	if consensusStats := p.calculateConsensusStats(results); consensusStats != nil {
		summary["consensus_stats"] = consensusStats
	}

	return summary
}

func (p *Processor) getServerInfo() *models.ServerInfo {
	if p.client == nil {
		return nil
	}

	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	info, err := p.client.GetServerInfo(ctx)
	if err != nil {
		return nil
	}
	return info
}

func (p *Processor) countSuccessful(results []models.Result) int {
	count := 0
	for _, result := range results {
		if result.Success {
			count++
		}
	}
	return count
}

func (p *Processor) calculateConsensusStats(results []models.Result) map[string]interface{} {
	if p.config.Processing.Repeat <= 1 {
		return nil
	}

	var totalConsensusRatio float64
	var consensusCount int
	distributionCounts := make(map[int]int)

	for _, result := range results {
		if result.Consensus != nil {
			totalConsensusRatio += result.Consensus.Ratio
			consensusCount++
			distributionCounts[len(result.Consensus.Distribution)]++
		}
	}

	if consensusCount == 0 {
		return nil
	}

	return map[string]interface{}{
		"repeat_count":         p.config.Processing.Repeat,
		"items_with_consensus": consensusCount,
		"avg_consensus_ratio":  totalConsensusRatio / float64(consensusCount),
		"distribution_variety": distributionCounts,
	}
}

func (p *Processor) saveConfigAndServerInfo() error {
	timestamp := time.Now().Format("20060102_150405")
	filename := filepath.Join(p.config.Output.Directory, fmt.Sprintf("config_and_server_info_%s.txt", timestamp))

	file, err := os.Create(filename)
	if err != nil {
		return fmt.Errorf("failed to create config file: %w", err)
	}
	defer file.Close()

	fmt.Fprintf(file, "LLM Client Configuration and Server Information\n")
	fmt.Fprintf(file, "Generated: %s\n", time.Now().Format("2006-01-02 15:04:05"))

	if p.configFile != "" {
		fmt.Fprintf(file, "Configuration File: %s\n", p.configFile)
	}
	if p.inputFile != "" {
		fmt.Fprintf(file, "Input File: %s\n", p.inputFile)
	}
	fmt.Fprintf(file, "\n")

	fmt.Fprintf(file, "PROVIDER CONFIGURATION:\n")
	fmt.Fprintf(file, "  Provider: %s\n", p.config.Provider.Name)
	fmt.Fprintf(file, "  Base URL: %s\n", p.config.Provider.BaseURL)
	fmt.Fprintf(file, "  Timeout: %s\n", p.config.Provider.Timeout)
	if p.config.Provider.APIKey != "" {
		fmt.Fprintf(file, "  API Key: [REDACTED]\n")
	}
	fmt.Fprintf(file, "\n")

	fmt.Fprintf(file, "MODEL CONFIGURATION:\n")
	fmt.Fprintf(file, "  Model Name: %s\n", p.config.Model.Name)

	// Write model parameters
	params := p.config.Model.Parameters
	fmt.Fprintf(file, "  Parameters:\n")

	if params.Temperature != nil {
		fmt.Fprintf(file, "    Temperature: %v\n", *params.Temperature)
	}
	if params.MaxTokens != nil {
		fmt.Fprintf(file, "    Max Tokens: %v\n", *params.MaxTokens)
	}
	if params.MinTokens != nil {
		fmt.Fprintf(file, "    Min Tokens: %v\n", *params.MinTokens)
	}
	if params.TopP != nil {
		fmt.Fprintf(file, "    Top P: %v\n", *params.TopP)
	}
	if params.TopK != nil {
		fmt.Fprintf(file, "    Top K: %v\n", *params.TopK)
	}
	if params.MinP != nil {
		fmt.Fprintf(file, "    Min P: %v\n", *params.MinP)
	}
	if params.RepetitionPenalty != nil {
		fmt.Fprintf(file, "    Repetition Penalty: %v\n", *params.RepetitionPenalty)
	}
	if params.PresencePenalty != nil {
		fmt.Fprintf(file, "    Presence Penalty: %v\n", *params.PresencePenalty)
	}
	if params.FrequencyPenalty != nil {
		fmt.Fprintf(file, "    Frequency Penalty: %v\n", *params.FrequencyPenalty)
	}
	if params.Seed != nil {
		fmt.Fprintf(file, "    Seed: %v\n", *params.Seed)
	}
	if params.N != nil {
		fmt.Fprintf(file, "    N: %v\n", *params.N)
	}
	if len(params.Stop) > 0 {
		fmt.Fprintf(file, "    Stop: %v\n", params.Stop)
	}
	if len(params.StopTokenIds) > 0 {
		fmt.Fprintf(file, "    Stop Token IDs: %v\n", params.StopTokenIds)
	}
	if len(params.BadWords) > 0 {
		fmt.Fprintf(file, "    Bad Words: %v\n", params.BadWords)
	}
	if params.EnableThinking != nil {
		fmt.Fprintf(file, "    Enable Thinking: %v\n", *params.EnableThinking)
	}

	// vLLM specific parameters
	if len(params.GuidedChoice) > 0 {
		fmt.Fprintf(file, "    Guided Choice: %v\n", params.GuidedChoice)
	}
	if params.GuidedRegex != nil {
		fmt.Fprintf(file, "    Guided Regex: %v\n", *params.GuidedRegex)
	}
	if params.GuidedJSON != nil {
		fmt.Fprintf(file, "    Guided JSON: %v\n", params.GuidedJSON)
	}
	if params.GuidedGrammar != nil {
		fmt.Fprintf(file, "    Guided Grammar: %v\n", *params.GuidedGrammar)
	}

	// llama.cpp specific parameters
	if params.Mirostat != nil {
		fmt.Fprintf(file, "    Mirostat: %v\n", *params.Mirostat)
	}
	if params.MirostatTau != nil {
		fmt.Fprintf(file, "    Mirostat Tau: %v\n", *params.MirostatTau)
	}
	if params.MirostatEta != nil {
		fmt.Fprintf(file, "    Mirostat Eta: %v\n", *params.MirostatEta)
	}
	if params.TfsZ != nil {
		fmt.Fprintf(file, "    TFS Z: %v\n", *params.TfsZ)
	}
	if params.TypicalP != nil {
		fmt.Fprintf(file, "    Typical P: %v\n", *params.TypicalP)
	}

	fmt.Fprintf(file, "\n")

	fmt.Fprintf(file, "PROCESSING CONFIGURATION:\n")
	fmt.Fprintf(file, "  Workers: %d\n", p.config.Processing.Workers)
	fmt.Fprintf(file, "  Batch Size: %d\n", p.config.Processing.BatchSize)
	fmt.Fprintf(file, "  Repeat: %d\n", p.config.Processing.Repeat)
	fmt.Fprintf(file, "  Rate Limit: %v\n", p.config.Processing.RateLimit)
	if p.config.Processing.FlashInferSafe != nil {
		fmt.Fprintf(file, "  FlashInfer Safe: %v\n", *p.config.Processing.FlashInferSafe)
	}
	fmt.Fprintf(file, "  Minimal Mode: %v\n", p.config.Processing.MinimalMode)
	fmt.Fprintf(file, "\n")

	fmt.Fprintf(file, "OUTPUT CONFIGURATION:\n")
	fmt.Fprintf(file, "  Directory: %s\n", p.config.Output.Directory)
	fmt.Fprintf(file, "  Format: %s\n", p.config.Output.Format)
	fmt.Fprintf(file, "  Include Raw Response: %v\n", p.config.Output.IncludeRawResponse)
	fmt.Fprintf(file, "  Include Thinking: %v\n", p.config.Output.IncludeThinking)
	fmt.Fprintf(file, "  Stream Output: %v\n", p.config.Output.StreamOutput)
	if p.config.Output.StreamSaveEvery > 0 {
		fmt.Fprintf(file, "  Stream Save Every: %d\n", p.config.Output.StreamSaveEvery)
	}
	fmt.Fprintf(file, "\n")

	if serverInfo := p.getServerInfo(); serverInfo != nil {
		fmt.Fprintf(file, "SERVER INFORMATION:\n")
		fmt.Fprintf(file, "  Server URL: %s\n", serverInfo.ServerURL)
		fmt.Fprintf(file, "  Server Type: %s\n", serverInfo.ServerType)
		fmt.Fprintf(file, "  Available: %v\n", serverInfo.Available)
		fmt.Fprintf(file, "  Timestamp: %v\n", serverInfo.Timestamp)

		if len(serverInfo.Config) > 0 {
			fmt.Fprintf(file, "  Configuration:\n")
			for key, value := range serverInfo.Config {
				fmt.Fprintf(file, "    %s: %v\n", key, value)
			}
		}

		if len(serverInfo.Models) > 0 {
			fmt.Fprintf(file, "  Models:\n")
			for key, value := range serverInfo.Models {
				fmt.Fprintf(file, "    %s: %v\n", key, value)
			}
		}

		if len(serverInfo.Features) > 0 {
			fmt.Fprintf(file, "  Features:\n")
			for key, value := range serverInfo.Features {
				fmt.Fprintf(file, "    %s: %v\n", key, value)
			}
		}
	} else {
		fmt.Fprintf(file, "SERVER INFORMATION:\n")
		fmt.Fprintf(file, "  Status: Unavailable or client not initialized\n")
	}

	fmt.Fprintf(file, "End of configuration and server information\n")

	logger.Info("Configuration and server info saved to: %s", filename)
	return nil
}



// File: internal/reporter/reporter.go
// Base: reporter

package reporter

import (
	"encoding/csv"
	"encoding/json"
	"fmt"
	"math"
	"os"
	"sort"
	"strings"
	"time"

	"github.com/Vitruves/llm-client/internal/models"

	"github.com/parquet-go/parquet-go"
	"github.com/xuri/excelize/v2"
)

type Reporter struct {
	results []models.Result
	stats   *Stats
}

type Stats struct {
	Total       int
	Success     int
	Failed      int
	SuccessRate float64
	AvgTime     time.Duration
	MinTime     time.Duration
	MaxTime     time.Duration
	MedianTime  time.Duration
	P95Time     time.Duration
	P99Time     time.Duration
	StdDevTime  time.Duration
	TotalTime   time.Duration

	Accuracy          float64
	BalancedAccuracy  float64
	Precision         map[string]float64
	Recall            map[string]float64
	F1Score           map[string]float64
	Specificity       map[string]float64
	NPV               map[string]float64
	FPR               map[string]float64
	FNR               map[string]float64
	MacroF1           float64
	MicroF1           float64
	WeightedF1        float64
	MacroPrecision    float64
	MacroRecall       float64
	MicroPrecision    float64
	MicroRecall       float64
	WeightedPrecision float64
	WeightedRecall    float64
	Kappa             float64
	WeightedKappa     float64
	MCC               float64

	HammingLoss     float64
	JaccardScore    map[string]float64
	MacroJaccard    float64
	MicroJaccard    float64
	WeightedJaccard float64

	ConfusionMatrix      map[string]map[string]int
	ClassificationReport string

	ClassDistribution map[string]int
	ErrorDistribution map[string]int

	ThroughputPerSecond float64
	ThroughputPerMinute float64
	ThroughputPerHour   float64

	TimePercentiles map[string]time.Duration
	TimeBuckets     map[string]int

	ConsensusStats    *ConsensusStats
	HypothesisTests   *HypothesisTests
	DistributionTests *DistributionTests
}

type ConsensusStats struct {
	ItemsWithConsensus  int
	AvgConsensusRatio   float64
	MinConsensusRatio   float64
	MaxConsensusRatio   float64
	RepeatCount         int
	DistributionVariety map[int]int
}

type HypothesisTests struct {
	ChiSquareTest       *ChiSquareResult
	McNemerTest         *McNemerResult
	BinomialTest        *BinomialResult
	KolmogorovSmirnovTest *KSResult
}

type ChiSquareResult struct {
	Statistic float64 `json:"statistic"`
	PValue    float64 `json:"p_value"`
	DegreesOfFreedom int `json:"degrees_of_freedom"`
	CriticalValue float64 `json:"critical_value"`
	Significant bool `json:"significant"`
}

type McNemerResult struct {
	Statistic float64 `json:"statistic"`
	PValue    float64 `json:"p_value"`
	Significant bool `json:"significant"`
	B         int `json:"b"` // Disagreement count (predicted=0, actual=1)
	C         int `json:"c"` // Disagreement count (predicted=1, actual=0)
}

type BinomialResult struct {
	PValue      float64 `json:"p_value"`
	Successes   int     `json:"successes"`
	Trials      int     `json:"trials"`
	Probability float64 `json:"probability"`
	Significant bool    `json:"significant"`
}

type KSResult struct {
	Statistic   float64 `json:"statistic"`
	PValue      float64 `json:"p_value"`
	Significant bool    `json:"significant"`
}

type DistributionTests struct {
	ResponseTimeNormality *NormalityTest
	ClassBalance         *BalanceTest
	ErrorPattern         *PatternTest
}

type NormalityTest struct {
	ShapiroWilkW    float64 `json:"shapiro_wilk_w"`
	ShapiroWilkP    float64 `json:"shapiro_wilk_p"`
	JarqueBeraJB    float64 `json:"jarque_bera_jb"`
	JarqueBeraP     float64 `json:"jarque_bera_p"`
	IsNormal        bool    `json:"is_normal"`
}

type BalanceTest struct {
	ChiSquareStat   float64     `json:"chi_square_stat"`
	ChiSquareP      float64     `json:"chi_square_p"`
	ImbalanceRatio  float64     `json:"imbalance_ratio"`
	IsBalanced      bool        `json:"is_balanced"`
	ClassProportions map[string]float64 `json:"class_proportions"`
}

type PatternTest struct {
	EntropyScore     float64 `json:"entropy_score"`
	UniformityTest   float64 `json:"uniformity_test"`
	HasPattern       bool    `json:"has_pattern"`
	DominantPattern  string  `json:"dominant_pattern"`
}

func New(results []models.Result) *Reporter {
	return &Reporter{
		results: results,
		stats:   calculateStats(results),
	}
}

func (r *Reporter) GenerateText() string {
	var report strings.Builder

	r.writeHeader(&report)
	r.writeOverview(&report)
	r.writePerformanceMetrics(&report)
	r.writeTimeDistribution(&report)
	r.writeClassificationMetrics(&report)
	r.writeClassDistribution(&report)
	r.writeErrorAnalysis(&report)
	r.writeConsensusAnalysis(&report)
	r.writeSummary(&report)

	return report.String()
}

func (r *Reporter) writeHeader(report *strings.Builder) {
	report.WriteString("LLM CLASSIFICATION ANALYSIS REPORT\n")
	report.WriteString(strings.Repeat("-", 50) + "\n\n")
}

func (r *Reporter) writeOverview(report *strings.Builder) {
	report.WriteString("OVERVIEW\n")
	report.WriteString(fmt.Sprintf("  Total Items Processed: %d\n", r.stats.Total))
	report.WriteString(fmt.Sprintf("  Successful: %d\n", r.stats.Success))
	report.WriteString(fmt.Sprintf("  Failed: %d\n", r.stats.Failed))
	report.WriteString(fmt.Sprintf("  Success Rate: %.2f%%\n", r.stats.SuccessRate))
	report.WriteString(fmt.Sprintf("  Total Processing Time: %v\n", r.stats.TotalTime))
	report.WriteString("\n")
}

func (r *Reporter) writePerformanceMetrics(report *strings.Builder) {
	report.WriteString("PERFORMANCE METRICS\n")
	report.WriteString(fmt.Sprintf("  Throughput: %.2f items/sec, %.1f items/min, %.0f items/hour\n",
		r.stats.ThroughputPerSecond, r.stats.ThroughputPerMinute, r.stats.ThroughputPerHour))
	report.WriteString(fmt.Sprintf("  Average Response Time: %v\n", r.stats.AvgTime))
	report.WriteString(fmt.Sprintf("  Median Response Time: %v\n", r.stats.MedianTime))
	report.WriteString(fmt.Sprintf("  95th Percentile: %v\n", r.stats.P95Time))
	report.WriteString(fmt.Sprintf("  99th Percentile: %v\n", r.stats.P99Time))
	report.WriteString(fmt.Sprintf("  Min Response Time: %v\n", r.stats.MinTime))
	report.WriteString(fmt.Sprintf("  Max Response Time: %v\n", r.stats.MaxTime))
	report.WriteString(fmt.Sprintf("  Standard Deviation: %v\n", r.stats.StdDevTime))
	report.WriteString("\n")
}

func (r *Reporter) writeTimeDistribution(report *strings.Builder) {
	if len(r.stats.TimeBuckets) == 0 {
		return
	}

	report.WriteString("RESPONSE TIME DISTRIBUTION\n")
	for bucket, count := range r.stats.TimeBuckets {
		percentage := float64(count) / float64(r.stats.Total) * 100
		report.WriteString(fmt.Sprintf("  %s: %d items (%.1f%%)\n", bucket, count, percentage))
	}
	report.WriteString("\n")
}

func (r *Reporter) writeClassificationMetrics(report *strings.Builder) {
	if r.stats.Accuracy == 0 && len(r.stats.ClassDistribution) == 0 {
		return
	}

	report.WriteString("CLASSIFICATION METRICS\n")

	if r.stats.Accuracy > 0 {
		r.writeAccuracyMetrics(report)
		r.writeF1Analysis(report)
		r.writePrecisionRecallAnalysis(report)
		r.writeJaccardAnalysis(report)
	}

	if len(r.stats.Precision) > 0 {
		r.writePerClassAnalysis(report)
	}

	if len(r.stats.ConfusionMatrix) > 0 {
		r.writeConfusionMatrix(report)
	}

	if r.stats.ClassificationReport != "" {
		report.WriteString(r.stats.ClassificationReport)
		report.WriteString("\n")
	}
}

func (r *Reporter) writeAccuracyMetrics(report *strings.Builder) {
	report.WriteString(fmt.Sprintf("  Accuracy: %.2f%%\n", r.stats.Accuracy))
	report.WriteString(fmt.Sprintf("  Balanced Accuracy: %.2f%%\n", r.stats.BalancedAccuracy))
	report.WriteString(fmt.Sprintf("  Cohen's Kappa: %.4f\n", r.stats.Kappa))
	report.WriteString(fmt.Sprintf("  Weighted Kappa: %.4f\n", r.stats.WeightedKappa))
	report.WriteString(fmt.Sprintf("  Matthews Correlation Coefficient: %.4f\n", r.stats.MCC))
	report.WriteString(fmt.Sprintf("  Hamming Loss: %.4f\n", r.stats.HammingLoss))
	report.WriteString("\n")
}

func (r *Reporter) writeF1Analysis(report *strings.Builder) {
	report.WriteString("F1-SCORE ANALYSIS\n")
	report.WriteString(fmt.Sprintf("  Macro F1-Score: %.2f%%\n", r.stats.MacroF1))
	report.WriteString(fmt.Sprintf("  Micro F1-Score: %.2f%%\n", r.stats.MicroF1))
	report.WriteString(fmt.Sprintf("  Weighted F1-Score: %.2f%%\n", r.stats.WeightedF1))
	report.WriteString("\n")
}

func (r *Reporter) writePrecisionRecallAnalysis(report *strings.Builder) {
	report.WriteString("PRECISION & RECALL ANALYSIS\n")
	report.WriteString(fmt.Sprintf("  Macro Precision: %.2f%%\n", r.stats.MacroPrecision))
	report.WriteString(fmt.Sprintf("  Macro Recall: %.2f%%\n", r.stats.MacroRecall))
	report.WriteString(fmt.Sprintf("  Micro Precision: %.2f%%\n", r.stats.MicroPrecision))
	report.WriteString(fmt.Sprintf("  Micro Recall: %.2f%%\n", r.stats.MicroRecall))
	report.WriteString(fmt.Sprintf("  Weighted Precision: %.2f%%\n", r.stats.WeightedPrecision))
	report.WriteString(fmt.Sprintf("  Weighted Recall: %.2f%%\n", r.stats.WeightedRecall))
	report.WriteString("\n")
}

func (r *Reporter) writeJaccardAnalysis(report *strings.Builder) {
	report.WriteString("JACCARD INDEX ANALYSIS\n")
	report.WriteString(fmt.Sprintf("  Macro Jaccard: %.2f%%\n", r.stats.MacroJaccard))
	report.WriteString(fmt.Sprintf("  Micro Jaccard: %.2f%%\n", r.stats.MicroJaccard))
	report.WriteString(fmt.Sprintf("  Weighted Jaccard: %.2f%%\n", r.stats.WeightedJaccard))
	report.WriteString("\n")
}

func (r *Reporter) writePerClassAnalysis(report *strings.Builder) {
	report.WriteString("COMPREHENSIVE PER-CLASS ANALYSIS\n")

	for class := range r.stats.ClassDistribution {
		precision := r.stats.Precision[class]
		recall := r.stats.Recall[class]
		f1 := r.stats.F1Score[class]
		specificity := r.stats.Specificity[class]
		npv := r.stats.NPV[class]
		fpr := r.stats.FPR[class]
		fnr := r.stats.FNR[class]
		jaccard := r.stats.JaccardScore[class]
		support := r.stats.ClassDistribution[class]

		report.WriteString(fmt.Sprintf("  Class '%s' (Support: %d):\n", class, support))
		report.WriteString(fmt.Sprintf("    Precision: %.2f%%, Recall: %.2f%%, F1-Score: %.2f%%\n",
			precision*100, recall*100, f1*100))
		report.WriteString(fmt.Sprintf("    Specificity: %.2f%%, NPV: %.2f%%, Jaccard: %.2f%%\n",
			specificity*100, npv*100, jaccard*100))
		report.WriteString(fmt.Sprintf("    False Positive Rate: %.2f%%, False Negative Rate: %.2f%%\n",
			fpr*100, fnr*100))
	}
	report.WriteString("\n")
}

func (r *Reporter) writeConfusionMatrix(report *strings.Builder) {
	report.WriteString("CONFUSION MATRIX\n")

	var classes []string
	for class := range r.stats.ConfusionMatrix {
		classes = append(classes, class)
	}
	sort.Strings(classes)

	report.WriteString("Actual\\Predicted")
	for _, class := range classes {
		report.WriteString(fmt.Sprintf("%8s", class))
	}
	report.WriteString("\n")

	for _, actualClass := range classes {
		report.WriteString(fmt.Sprintf("%-15s", actualClass))
		for _, predictedClass := range classes {
			count := r.stats.ConfusionMatrix[actualClass][predictedClass]
			report.WriteString(fmt.Sprintf("%8d", count))
		}
		report.WriteString("\n")
	}
	report.WriteString("\n")
}

func (r *Reporter) writeClassDistribution(report *strings.Builder) {
	if len(r.stats.ClassDistribution) == 0 {
		return
	}

	report.WriteString("CLASS DISTRIBUTION\n")
	for class, count := range r.stats.ClassDistribution {
		percentage := float64(count) / float64(r.stats.Total) * 100
		report.WriteString(fmt.Sprintf("  Class '%s': %d items (%.1f%%)\n", class, count, percentage))
	}
	report.WriteString("\n")
}

func (r *Reporter) writeErrorAnalysis(report *strings.Builder) {
	if len(r.stats.ErrorDistribution) == 0 {
		return
	}

	report.WriteString("ERROR ANALYSIS\n")
	for errorType, count := range r.stats.ErrorDistribution {
		percentage := float64(count) / float64(r.stats.Failed) * 100
		report.WriteString(fmt.Sprintf("  %s: %d errors (%.1f%% of failures)\n", errorType, count, percentage))
	}
	report.WriteString("\n")
}

func (r *Reporter) writeConsensusAnalysis(report *strings.Builder) {
	if r.stats.ConsensusStats == nil {
		return
	}

	cs := r.stats.ConsensusStats
	report.WriteString("CONSENSUS ANALYSIS\n")
	report.WriteString(fmt.Sprintf("  Repeat Count: %d\n", cs.RepeatCount))
	report.WriteString(fmt.Sprintf("  Items with Consensus: %d\n", cs.ItemsWithConsensus))
	report.WriteString(fmt.Sprintf("  Average Consensus Ratio: %.2f%%\n", cs.AvgConsensusRatio*100))
	report.WriteString(fmt.Sprintf("  Min Consensus Ratio: %.2f%%\n", cs.MinConsensusRatio*100))
	report.WriteString(fmt.Sprintf("  Max Consensus Ratio: %.2f%%\n", cs.MaxConsensusRatio*100))

	if len(cs.DistributionVariety) > 0 {
		report.WriteString("  Distribution Variety:\n")
		for uniqueAnswers, count := range cs.DistributionVariety {
			report.WriteString(fmt.Sprintf("    %d unique answers: %d items\n", uniqueAnswers, count))
		}
	}
	report.WriteString("\n")
}

func (r *Reporter) writeSummary(report *strings.Builder) {
	report.WriteString("SUMMARY\n")

	switch {
	case r.stats.SuccessRate >= 95:
		report.WriteString("  Status: EXCELLENT - Very high success rate\n")
	case r.stats.SuccessRate >= 90:
		report.WriteString("  Status: GOOD - High success rate\n")
	case r.stats.SuccessRate >= 80:
		report.WriteString("  Status: ACCEPTABLE - Moderate success rate\n")
	default:
		report.WriteString("  Status: NEEDS IMPROVEMENT - Low success rate\n")
	}

	avgTimeMs := float64(r.stats.AvgTime.Nanoseconds()) / 1e6
	switch {
	case avgTimeMs < 1000:
		report.WriteString("  Performance: FAST - Sub-second average response\n")
	case avgTimeMs < 5000:
		report.WriteString("  Performance: GOOD - Fast response times\n")
	case avgTimeMs < 15000:
		report.WriteString("  Performance: MODERATE - Acceptable response times\n")
	default:
		report.WriteString("  Performance: SLOW - Consider optimization\n")
	}
}

func (r *Reporter) GenerateJSON() (string, error) {
	output := map[string]interface{}{
		"stats":   r.stats,
		"results": r.results,
	}

	data, err := json.MarshalIndent(output, "", "  ")
	if err != nil {
		return "", err
	}

	return string(data), nil
}

func (r *Reporter) SaveToFile(filename, format string) error {
	switch format {
	case "json":
		return r.saveJSON(filename)
	case "text":
		return r.saveText(filename)
	case "csv":
		return r.saveCSV(filename)
	case "xlsx":
		return r.saveExcel(filename)
	case "parquet":
		return r.saveParquet(filename)
	default:
		return fmt.Errorf("unsupported format: %s", format)
	}
}

func (r *Reporter) saveJSON(filename string) error {
	content, err := r.GenerateJSON()
	if err != nil {
		return err
	}
	return os.WriteFile(filename, []byte(content), 0644)
}

func (r *Reporter) saveText(filename string) error {
	content := r.GenerateText()
	return os.WriteFile(filename, []byte(content), 0644)
}

func (r *Reporter) saveCSV(filename string) error {
	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := csv.NewWriter(file)
	defer writer.Flush()

	if err := r.writeCSVSummary(writer); err != nil {
		return err
	}

	// Write per-class metrics
	if err := r.writeCSVPerClassMetrics(writer); err != nil {
		return err
	}

	if err := r.writeCSVHypothesisTests(writer); err != nil {
		return err
	}

	return nil
}

func (r *Reporter) saveExcel(filename string) error {
	f := excelize.NewFile()
	defer f.Close()

	summarySheet := "Summary"
	perClassSheet := "PerClass"
	hypothesisSheet := "HypothesisTests"
	confusionSheet := "ConfusionMatrix"

	f.NewSheet(summarySheet)
	f.NewSheet(perClassSheet)
	f.NewSheet(hypothesisSheet)
	f.NewSheet(confusionSheet)

	if err := r.writeExcelSummary(f, summarySheet); err != nil {
		return err
	}
	if err := r.writeExcelPerClass(f, perClassSheet); err != nil {
		return err
	}
	if err := r.writeExcelHypothesis(f, hypothesisSheet); err != nil {
		return err
	}
	if err := r.writeExcelConfusion(f, confusionSheet); err != nil {
		return err
	}

	f.DeleteSheet("Sheet1")

	return f.SaveAs(filename)
}

func (r *Reporter) saveParquet(filename string) error {
	// Create a flattened structure for Parquet
	type ReportRow struct {
		Metric string  `parquet:"metric"`
		Value  float64 `parquet:"value"`
		Class  string  `parquet:"class,optional"`
		Type   string  `parquet:"type"`
	}

	var rows []ReportRow

	rows = append(rows, []ReportRow{
		{Metric: "accuracy", Value: r.stats.Accuracy, Type: "summary"},
		{Metric: "balanced_accuracy", Value: r.stats.BalancedAccuracy, Type: "summary"},
		{Metric: "macro_f1", Value: r.stats.MacroF1, Type: "summary"},
		{Metric: "micro_f1", Value: r.stats.MicroF1, Type: "summary"},
		{Metric: "weighted_f1", Value: r.stats.WeightedF1, Type: "summary"},
		{Metric: "kappa", Value: r.stats.Kappa, Type: "summary"},
		{Metric: "mcc", Value: r.stats.MCC, Type: "summary"},
		{Metric: "success_rate", Value: r.stats.SuccessRate, Type: "summary"},
		{Metric: "throughput_per_second", Value: r.stats.ThroughputPerSecond, Type: "performance"},
	}...)

	// Add per-class metrics
	for class, precision := range r.stats.Precision {
		rows = append(rows, []ReportRow{
			{Metric: "precision", Value: precision * 100, Class: class, Type: "per_class"},
			{Metric: "recall", Value: r.stats.Recall[class] * 100, Class: class, Type: "per_class"},
			{Metric: "f1_score", Value: r.stats.F1Score[class] * 100, Class: class, Type: "per_class"},
			{Metric: "specificity", Value: r.stats.Specificity[class] * 100, Class: class, Type: "per_class"},
		}...)
	}

	file, err := os.Create(filename)
	if err != nil {
		return err
	}
	defer file.Close()

	writer := parquet.NewGenericWriter[ReportRow](file)
	defer writer.Close()

	_, err = writer.Write(rows)
	return err
}

func calculateStats(results []models.Result) *Stats {
	if len(results) == 0 {
		return createEmptyStats()
	}

	stats := &Stats{
		Total:             len(results),
		ClassDistribution: make(map[string]int),
		ErrorDistribution: make(map[string]int),
		Precision:         make(map[string]float64),
		Recall:            make(map[string]float64),
		F1Score:           make(map[string]float64),
		Specificity:       make(map[string]float64),
		NPV:               make(map[string]float64),
		FPR:               make(map[string]float64),
		FNR:               make(map[string]float64),
		JaccardScore:      make(map[string]float64),
		ConfusionMatrix:   make(map[string]map[string]int),
		TimePercentiles:   make(map[string]time.Duration),
		TimeBuckets:       make(map[string]int),
	}

	predictions, actuals, times, hasGroundTruth := processResults(results, stats)

	stats.Success = len(predictions)
	stats.Failed = len(results) - stats.Success
	stats.SuccessRate = float64(stats.Success) / float64(len(results)) * 100

	calculateTimeStatistics(stats, times)

	if hasGroundTruth && len(predictions) == len(actuals) {
		calculateClassificationMetrics(stats, predictions, actuals)
	}

	stats.ConsensusStats = calculateConsensusStats(results)

	return stats
}

func createEmptyStats() *Stats {
	return &Stats{
		ClassDistribution: make(map[string]int),
		ErrorDistribution: make(map[string]int),
		Precision:         make(map[string]float64),
		Recall:            make(map[string]float64),
		F1Score:           make(map[string]float64),
		Specificity:       make(map[string]float64),
		NPV:               make(map[string]float64),
		FPR:               make(map[string]float64),
		FNR:               make(map[string]float64),
		JaccardScore:      make(map[string]float64),
		ConfusionMatrix:   make(map[string]map[string]int),
		TimePercentiles:   make(map[string]time.Duration),
		TimeBuckets:       make(map[string]int),
	}
}

func processResults(results []models.Result, stats *Stats) ([]string, []string, []time.Duration, bool) {
	var predictions, actuals []string
	var times []time.Duration
	var hasGroundTruth bool

	for _, result := range results {
		times = append(times, result.ResponseTime)
		stats.TotalTime += result.ResponseTime

		if result.Success {
			stats.ClassDistribution[result.FinalAnswer]++
			predictions = append(predictions, result.FinalAnswer)

			if result.GroundTruth != "" {
				actuals = append(actuals, result.GroundTruth)
				hasGroundTruth = true
			}
		} else {
			errorType := categorizeError(result.Error)
			stats.ErrorDistribution[errorType]++
		}
	}

	return predictions, actuals, times, hasGroundTruth
}

func categorizeError(errorMsg string) string {
	errorMsg = strings.ToLower(errorMsg)

	switch {
	case strings.Contains(errorMsg, "timeout"):
		return "timeout"
	case strings.Contains(errorMsg, "connection"):
		return "connection"
	case strings.Contains(errorMsg, "parse"):
		return "parsing"
	default:
		return "other"
	}
}

func calculateTimeStatistics(stats *Stats, times []time.Duration) {
	if len(times) == 0 {
		return
	}

	sort.Slice(times, func(i, j int) bool {
		return times[i] < times[j]
	})

	stats.AvgTime = stats.TotalTime / time.Duration(len(times))
	stats.MinTime = times[0]
	stats.MaxTime = times[len(times)-1]
	stats.MedianTime = times[len(times)/2]
	stats.P95Time = times[int(float64(len(times))*0.95)]
	stats.P99Time = times[int(float64(len(times))*0.99)]

	calculateStandardDeviation(stats, times)
	calculatePercentiles(stats, times)
	calculateTimeBuckets(stats, times)
	calculateThroughput(stats)
}

func calculateStandardDeviation(stats *Stats, times []time.Duration) {
	var sumSquaredDiff float64
	avgNanos := float64(stats.AvgTime.Nanoseconds())

	for _, t := range times {
		diff := float64(t.Nanoseconds()) - avgNanos
		sumSquaredDiff += diff * diff
	}

	variance := sumSquaredDiff / float64(len(times))
	stats.StdDevTime = time.Duration(math.Sqrt(variance))
}

func calculatePercentiles(stats *Stats, times []time.Duration) {
	stats.TimePercentiles["P50"] = times[len(times)/2]
	stats.TimePercentiles["P75"] = times[int(float64(len(times))*0.75)]
	stats.TimePercentiles["P90"] = times[int(float64(len(times))*0.90)]
	stats.TimePercentiles["P95"] = stats.P95Time
	stats.TimePercentiles["P99"] = stats.P99Time
}

func calculateTimeBuckets(stats *Stats, times []time.Duration) {
	for _, t := range times {
		ms := float64(t.Nanoseconds()) / 1e6

		switch {
		case ms < 100:
			stats.TimeBuckets["< 100ms"]++
		case ms < 500:
			stats.TimeBuckets["100-500ms"]++
		case ms < 1000:
			stats.TimeBuckets["500ms-1s"]++
		case ms < 5000:
			stats.TimeBuckets["1-5s"]++
		case ms < 15000:
			stats.TimeBuckets["5-15s"]++
		default:
			stats.TimeBuckets["> 15s"]++
		}
	}
}

func calculateThroughput(stats *Stats) {
	if stats.TotalTime.Seconds() > 0 {
		stats.ThroughputPerSecond = float64(stats.Total) / stats.TotalTime.Seconds()
		stats.ThroughputPerMinute = stats.ThroughputPerSecond * 60
		stats.ThroughputPerHour = stats.ThroughputPerSecond * 3600
	}
}

func calculateClassificationMetrics(stats *Stats, predictions, actuals []string) {
	stats.Accuracy = calculateAccuracy(predictions, actuals)
	stats.BalancedAccuracy = calculateBalancedAccuracy(predictions, actuals)
	stats.Kappa = calculateKappa(predictions, actuals)
	stats.WeightedKappa = calculateKappa(predictions, actuals)
	stats.MCC = calculateMCC(predictions, actuals)
	stats.HammingLoss = calculateHammingLoss(predictions, actuals)

	stats.Precision, stats.Recall, stats.F1Score = calculatePerClassMetrics(predictions, actuals)
	stats.Specificity, stats.NPV, stats.FPR, stats.FNR = calculateAdvancedPerClassMetrics(predictions, actuals)
	stats.JaccardScore = calculateJaccardPerClass(predictions, actuals)
	stats.ConfusionMatrix = calculateConfusionMatrix(predictions, actuals)

	calculateAverageMetrics(stats, predictions, actuals)
	stats.ClassificationReport = generateClassificationReport(stats, predictions, actuals)
}

func calculateAverageMetrics(stats *Stats, predictions, actuals []string) {
	stats.MacroF1 = calculateMacroAverage(stats.F1Score)
	stats.MicroF1 = calculateMicroF1(predictions, actuals)
	stats.WeightedF1 = calculateWeightedAverage(stats.F1Score, predictions, actuals)
	stats.MacroPrecision = calculateMacroAverage(stats.Precision)
	stats.MacroRecall = calculateMacroAverage(stats.Recall)
	stats.MicroPrecision, stats.MicroRecall = calculateMicroPrecisionRecall(predictions, actuals)
	stats.WeightedPrecision = calculateWeightedAverage(stats.Precision, predictions, actuals)
	stats.WeightedRecall = calculateWeightedAverage(stats.Recall, predictions, actuals)
	stats.MacroJaccard = calculateMacroAverage(stats.JaccardScore)
	stats.MicroJaccard = calculateMicroJaccard(predictions, actuals)
	stats.WeightedJaccard = calculateWeightedAverage(stats.JaccardScore, predictions, actuals)
}

func calculateAccuracy(predictions, actuals []string) float64 {
	if len(predictions) != len(actuals) || len(predictions) == 0 {
		return 0.0
	}

	correct := 0
	for i := 0; i < len(predictions); i++ {
		if predictions[i] == actuals[i] {
			correct++
		}
	}

	return float64(correct) / float64(len(predictions)) * 100
}

func calculateBalancedAccuracy(predictions, actuals []string) float64 {
	classSet := getUniqueClasses(actuals)
	var recallSum float64
	classCount := 0

	for class := range classSet {
		tp, _, fn := getClassMetrics(class, predictions, actuals)
		if tp+fn > 0 {
			recall := float64(tp) / float64(tp+fn)
			recallSum += recall
			classCount++
		}
	}

	if classCount == 0 {
		return 0.0
	}

	return (recallSum / float64(classCount)) * 100
}

func calculateKappa(predictions, actuals []string) float64 {
	if len(predictions) != len(actuals) || len(predictions) == 0 {
		return 0.0
	}

	classSet := getUniqueClasses(append(predictions, actuals...))
	classes := make([]string, 0, len(classSet))
	for class := range classSet {
		classes = append(classes, class)
	}

	po := calculateAccuracy(predictions, actuals) / 100

	pe := 0.0
	total := float64(len(predictions))

	for _, class := range classes {
		actualCount := countClassOccurrences(class, actuals)
		predictedCount := countClassOccurrences(class, predictions)

		pActual := float64(actualCount) / total
		pPredicted := float64(predictedCount) / total
		pe += pActual * pPredicted
	}

	if pe == 1.0 {
		return 0.0
	}

	return (po - pe) / (1.0 - pe)
}

func calculateMCC(predictions, actuals []string) float64 {
	if len(predictions) != len(actuals) || len(predictions) == 0 {
		return 0.0
	}

	classSet := getUniqueClasses(actuals)
	var mccSum float64
	classCount := 0

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		tn := len(predictions) - tp - fp - fn

		denominator := math.Sqrt(float64((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)))
		if denominator == 0 {
			continue
		}

		mcc := float64(tp*tn-fp*fn) / denominator
		mccSum += mcc
		classCount++
	}

	if classCount == 0 {
		return 0.0
	}

	return mccSum / float64(classCount)
}

func calculateHammingLoss(predictions, actuals []string) float64 {
	if len(predictions) != len(actuals) || len(predictions) == 0 {
		return 0.0
	}

	incorrect := 0
	for i := 0; i < len(predictions); i++ {
		if predictions[i] != actuals[i] {
			incorrect++
		}
	}

	return float64(incorrect) / float64(len(predictions))
}

func calculatePerClassMetrics(predictions, actuals []string) (map[string]float64, map[string]float64, map[string]float64) {
	precision := make(map[string]float64)
	recall := make(map[string]float64)
	f1Score := make(map[string]float64)

	classSet := getUniqueClasses(append(predictions, actuals...))

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)

		if tp+fp > 0 {
			precision[class] = float64(tp) / float64(tp+fp)
		}

		if tp+fn > 0 {
			recall[class] = float64(tp) / float64(tp+fn)
		}

		if precision[class]+recall[class] > 0 {
			f1Score[class] = 2 * precision[class] * recall[class] / (precision[class] + recall[class])
		}
	}

	return precision, recall, f1Score
}

func calculateAdvancedPerClassMetrics(predictions, actuals []string) (map[string]float64, map[string]float64, map[string]float64, map[string]float64) {
	specificity := make(map[string]float64)
	npv := make(map[string]float64)
	fpr := make(map[string]float64)
	fnr := make(map[string]float64)

	classSet := getUniqueClasses(actuals)

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		tn := len(predictions) - tp - fp - fn

		if tn+fp > 0 {
			specificity[class] = float64(tn) / float64(tn+fp)
		}

		if tn+fn > 0 {
			npv[class] = float64(tn) / float64(tn+fn)
		}

		if fp+tn > 0 {
			fpr[class] = float64(fp) / float64(fp+tn)
		}

		if fn+tp > 0 {
			fnr[class] = float64(fn) / float64(fn+tp)
		}
	}

	return specificity, npv, fpr, fnr
}

func calculateJaccardPerClass(predictions, actuals []string) map[string]float64 {
	jaccard := make(map[string]float64)
	classSet := getUniqueClasses(actuals)

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		if tp+fp+fn > 0 {
			jaccard[class] = float64(tp) / float64(tp+fp+fn)
		}
	}

	return jaccard
}

func calculateConfusionMatrix(predictions, actuals []string) map[string]map[string]int {
	matrix := make(map[string]map[string]int)
	classSet := getUniqueClasses(append(predictions, actuals...))

	for actual := range classSet {
		matrix[actual] = make(map[string]int)
		for predicted := range classSet {
			matrix[actual][predicted] = 0
		}
	}

	for i := 0; i < len(predictions); i++ {
		actual := actuals[i]
		predicted := predictions[i]
		matrix[actual][predicted]++
	}

	return matrix
}

func calculateMacroAverage(values map[string]float64) float64 {
	if len(values) == 0 {
		return 0.0
	}

	sum := 0.0
	count := 0
	for _, value := range values {
		if !math.IsNaN(value) {
			sum += value
			count++
		}
	}

	if count == 0 {
		return 0.0
	}

	return (sum / float64(count)) * 100
}

func calculateMicroF1(predictions, actuals []string) float64 {
	classSet := getUniqueClasses(append(predictions, actuals...))

	totalTP := 0
	totalFP := 0
	totalFN := 0

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		totalTP += tp
		totalFP += fp
		totalFN += fn
	}

	if totalTP == 0 {
		return 0.0
	}

	precision := float64(totalTP) / float64(totalTP+totalFP)
	recall := float64(totalTP) / float64(totalTP+totalFN)

	if precision+recall == 0 {
		return 0.0
	}

	return 2 * precision * recall / (precision + recall) * 100
}

func calculateMicroPrecisionRecall(predictions, actuals []string) (float64, float64) {
	classSet := getUniqueClasses(actuals)

	totalTP := 0
	totalFP := 0
	totalFN := 0

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		totalTP += tp
		totalFP += fp
		totalFN += fn
	}

	var precision, recall float64

	if totalTP+totalFP > 0 {
		precision = float64(totalTP) / float64(totalTP+totalFP) * 100
	}

	if totalTP+totalFN > 0 {
		recall = float64(totalTP) / float64(totalTP+totalFN) * 100
	}

	return precision, recall
}

func calculateWeightedAverage(values map[string]float64, predictions, actuals []string) float64 {
	if len(values) == 0 {
		return 0.0
	}

	support := make(map[string]int)
	for _, actual := range actuals {
		support[actual]++
	}

	weightedSum := 0.0
	totalSupport := 0

	for class, value := range values {
		classSupport := support[class]
		if !math.IsNaN(value) {
			weightedSum += value * float64(classSupport)
		}
		totalSupport += classSupport
	}

	if totalSupport == 0 {
		return 0.0
	}

	return (weightedSum / float64(totalSupport)) * 100
}

func calculateMicroJaccard(predictions, actuals []string) float64 {
	classSet := getUniqueClasses(actuals)

	totalTP := 0
	totalFP := 0
	totalFN := 0

	for class := range classSet {
		tp, fp, fn := getClassMetrics(class, predictions, actuals)
		totalTP += tp
		totalFP += fp
		totalFN += fn
	}

	if totalTP+totalFP+totalFN > 0 {
		return float64(totalTP) / float64(totalTP+totalFP+totalFN) * 100
	}

	return 0.0
}

func generateClassificationReport(stats *Stats, predictions, actuals []string) string {
	var report strings.Builder

	report.WriteString("DETAILED CLASSIFICATION REPORT\n")
	report.WriteString(strings.Repeat("-", 80) + "\n")
	report.WriteString(fmt.Sprintf("%-10s %10s %10s %10s %10s %10s %10s %10s\n",
		"Class", "Precision", "Recall", "F1-Score", "Specificity", "NPV", "Jaccard", "Support"))
	report.WriteString(strings.Repeat("-", 80) + "\n")

	support := make(map[string]int)
	for _, actual := range actuals {
		support[actual]++
	}

	for class := range stats.ClassDistribution {
		precision := stats.Precision[class] * 100
		recall := stats.Recall[class] * 100
		f1 := stats.F1Score[class] * 100
		specificity := stats.Specificity[class] * 100
		npv := stats.NPV[class] * 100
		jaccard := stats.JaccardScore[class] * 100
		classSupport := support[class]

		report.WriteString(fmt.Sprintf("%-10s %9.2f%% %9.2f%% %9.2f%% %10.2f%% %9.2f%% %9.2f%% %10d\n",
			class, precision, recall, f1, specificity, npv, jaccard, classSupport))
	}

	report.WriteString(strings.Repeat("-", 80) + "\n")
	report.WriteString(fmt.Sprintf("%-10s %9.2f%% %9.2f%% %9.2f%% %10s %9s %9.2f%% %10d\n",
		"macro avg", stats.MacroPrecision, stats.MacroRecall, stats.MacroF1,
		"-", "-", stats.MacroJaccard, len(actuals)))
	report.WriteString(fmt.Sprintf("%-10s %9.2f%% %9.2f%% %9.2f%% %10s %9s %9.2f%% %10d\n",
		"micro avg", stats.MicroPrecision, stats.MicroRecall, stats.MicroF1,
		"-", "-", stats.MicroJaccard, len(actuals)))
	report.WriteString(fmt.Sprintf("%-10s %9.2f%% %9.2f%% %9.2f%% %10s %9s %9.2f%% %10d\n",
		"weighted", stats.WeightedPrecision, stats.WeightedRecall, stats.WeightedF1,
		"-", "-", stats.WeightedJaccard, len(actuals)))

	return report.String()
}

func calculateConsensusStats(results []models.Result) *ConsensusStats {
	var consensusResults []models.Result
	var repeatCount int

	for _, result := range results {
		if result.Consensus != nil {
			consensusResults = append(consensusResults, result)
			if repeatCount == 0 {
				repeatCount = result.Consensus.Total
			}
		}
	}

	if len(consensusResults) == 0 {
		return nil
	}

	stats := &ConsensusStats{
		ItemsWithConsensus:  len(consensusResults),
		RepeatCount:         repeatCount,
		DistributionVariety: make(map[int]int),
	}

	var ratioSum float64
	minRatio := 1.0
	maxRatio := 0.0

	for _, result := range consensusResults {
		ratio := result.Consensus.Ratio
		ratioSum += ratio

		if ratio < minRatio {
			minRatio = ratio
		}
		if ratio > maxRatio {
			maxRatio = ratio
		}

		uniqueAnswers := len(result.Consensus.Distribution)
		stats.DistributionVariety[uniqueAnswers]++
	}

	stats.AvgConsensusRatio = ratioSum / float64(len(consensusResults))
	stats.MinConsensusRatio = minRatio
	stats.MaxConsensusRatio = maxRatio

	return stats
}

func getUniqueClasses(items []string) map[string]bool {
	classSet := make(map[string]bool)
	for _, item := range items {
		classSet[item] = true
	}
	return classSet
}

func getClassMetrics(class string, predictions, actuals []string) (tp, fp, fn int) {
	for i := 0; i < len(predictions); i++ {
		if actuals[i] == class && predictions[i] == class {
			tp++
		} else if actuals[i] != class && predictions[i] == class {
			fp++
		} else if actuals[i] == class && predictions[i] != class {
			fn++
		}
	}
	return tp, fp, fn
}

func countClassOccurrences(class string, items []string) int {
	count := 0
	for _, item := range items {
		if item == class {
			count++
		}
	}
	return count
}

// Helper methods for writing CSV format
func (r *Reporter) writeCSVSummary(writer *csv.Writer) error {
	if err := writer.Write([]string{"Section", "Metric", "Value"}); err != nil {
		return err
	}

	records := [][]string{
		{"Summary", "Total Items", fmt.Sprintf("%d", r.stats.Total)},
		{"Summary", "Success Rate", fmt.Sprintf("%.2f%%", r.stats.SuccessRate)},
		{"Summary", "Accuracy", fmt.Sprintf("%.2f%%", r.stats.Accuracy)},
		{"Summary", "Balanced Accuracy", fmt.Sprintf("%.2f%%", r.stats.BalancedAccuracy)},
		{"Summary", "Macro F1", fmt.Sprintf("%.2f%%", r.stats.MacroF1)},
		{"Summary", "Micro F1", fmt.Sprintf("%.2f%%", r.stats.MicroF1)},
		{"Summary", "Weighted F1", fmt.Sprintf("%.2f%%", r.stats.WeightedF1)},
		{"Summary", "Cohen's Kappa", fmt.Sprintf("%.4f", r.stats.Kappa)},
		{"Summary", "MCC", fmt.Sprintf("%.4f", r.stats.MCC)},
		{"Performance", "Throughput Per Second", fmt.Sprintf("%.2f", r.stats.ThroughputPerSecond)},
		{"Performance", "Average Response Time", r.stats.AvgTime.String()},
		{"Performance", "P95 Response Time", r.stats.P95Time.String()},
	}

	return writer.WriteAll(records)
}

func (r *Reporter) writeCSVPerClassMetrics(writer *csv.Writer) error {
	if err := writer.Write([]string{"", "", ""}); err != nil {
		return err
	}
	
	// Write header for per-class metrics
	if err := writer.Write([]string{"Class", "Precision", "Recall", "F1-Score", "Specificity", "NPV", "Jaccard", "Support"}); err != nil {
		return err
	}

	for class := range r.stats.ClassDistribution {
		support := r.stats.ClassDistribution[class]
		record := []string{
			class,
			fmt.Sprintf("%.2f%%", r.stats.Precision[class]*100),
			fmt.Sprintf("%.2f%%", r.stats.Recall[class]*100),
			fmt.Sprintf("%.2f%%", r.stats.F1Score[class]*100),
			fmt.Sprintf("%.2f%%", r.stats.Specificity[class]*100),
			fmt.Sprintf("%.2f%%", r.stats.NPV[class]*100),
			fmt.Sprintf("%.2f%%", r.stats.JaccardScore[class]*100),
			fmt.Sprintf("%d", support),
		}
		if err := writer.Write(record); err != nil {
			return err
		}
	}

	return nil
}

func (r *Reporter) writeCSVHypothesisTests(writer *csv.Writer) error {
	if r.stats.HypothesisTests == nil {
		return nil
	}

	if err := writer.Write([]string{"", "", ""}); err != nil {
		return err
	}

	if err := writer.Write([]string{"Test", "Statistic", "P-Value", "Significant"}); err != nil {
		return err
	}

	if r.stats.HypothesisTests.ChiSquareTest != nil {
		test := r.stats.HypothesisTests.ChiSquareTest
		record := []string{
			"Chi-Square",
			fmt.Sprintf("%.4f", test.Statistic),
			fmt.Sprintf("%.6f", test.PValue),
			fmt.Sprintf("%t", test.Significant),
		}
		if err := writer.Write(record); err != nil {
			return err
		}
	}

	return nil
}

// Helper methods for writing Excel format
func (r *Reporter) writeExcelSummary(f *excelize.File, sheet string) error {
	headers := []string{"Metric", "Value"}
	for i, header := range headers {
		cell := fmt.Sprintf("%c1", 'A'+i)
		f.SetCellValue(sheet, cell, header)
	}

	data := [][]interface{}{
		{"Total Items", r.stats.Total},
		{"Success Rate", fmt.Sprintf("%.2f%%", r.stats.SuccessRate)},
		{"Accuracy", fmt.Sprintf("%.2f%%", r.stats.Accuracy)},
		{"Balanced Accuracy", fmt.Sprintf("%.2f%%", r.stats.BalancedAccuracy)},
		{"Macro F1", fmt.Sprintf("%.2f%%", r.stats.MacroF1)},
		{"Micro F1", fmt.Sprintf("%.2f%%", r.stats.MicroF1)},
		{"Weighted F1", fmt.Sprintf("%.2f%%", r.stats.WeightedF1)},
		{"Cohen's Kappa", r.stats.Kappa},
		{"MCC", r.stats.MCC},
		{"Throughput/sec", r.stats.ThroughputPerSecond},
	}

	for i, row := range data {
		for j, value := range row {
			cell := fmt.Sprintf("%c%d", 'A'+j, i+2)
			f.SetCellValue(sheet, cell, value)
		}
	}

	return nil
}

func (r *Reporter) writeExcelPerClass(f *excelize.File, sheet string) error {
	headers := []string{"Class", "Precision", "Recall", "F1-Score", "Specificity", "NPV", "Jaccard", "Support"}
	for i, header := range headers {
		cell := fmt.Sprintf("%c1", 'A'+i)
		f.SetCellValue(sheet, cell, header)
	}

	row := 2
	for class := range r.stats.ClassDistribution {
		support := r.stats.ClassDistribution[class]
		data := []interface{}{
			class,
			r.stats.Precision[class],
			r.stats.Recall[class],
			r.stats.F1Score[class],
			r.stats.Specificity[class],
			r.stats.NPV[class],
			r.stats.JaccardScore[class],
			support,
		}

		for j, value := range data {
			cell := fmt.Sprintf("%c%d", 'A'+j, row)
			f.SetCellValue(sheet, cell, value)
		}
		row++
	}

	return nil
}

func (r *Reporter) writeExcelHypothesis(f *excelize.File, sheet string) error {
	if r.stats.HypothesisTests == nil {
		return nil
	}

	headers := []string{"Test", "Statistic", "P-Value", "Significant", "Details"}
	for i, header := range headers {
		cell := fmt.Sprintf("%c1", 'A'+i)
		f.SetCellValue(sheet, cell, header)
	}

	row := 2
	if r.stats.HypothesisTests.ChiSquareTest != nil {
		test := r.stats.HypothesisTests.ChiSquareTest
		data := []interface{}{
			"Chi-Square",
			test.Statistic,
			test.PValue,
			test.Significant,
			fmt.Sprintf("df=%d", test.DegreesOfFreedom),
		}

		for j, value := range data {
			cell := fmt.Sprintf("%c%d", 'A'+j, row)
			f.SetCellValue(sheet, cell, value)
		}
		row++
	}

	return nil
}

func (r *Reporter) writeExcelConfusion(f *excelize.File, sheet string) error {
	if len(r.stats.ConfusionMatrix) == 0 {
		return nil
	}

	// Get sorted classes
	var classes []string
	for class := range r.stats.ConfusionMatrix {
		classes = append(classes, class)
	}
	sort.Strings(classes)

	f.SetCellValue(sheet, "A1", "Actual\\Predicted")
	for i, class := range classes {
		cell := fmt.Sprintf("%c1", 'B'+i)
		f.SetCellValue(sheet, cell, class)
	}

	for i, actualClass := range classes {
		row := i + 2
		f.SetCellValue(sheet, fmt.Sprintf("A%d", row), actualClass)
		
		for j, predictedClass := range classes {
			cell := fmt.Sprintf("%c%d", 'B'+j, row)
			count := r.stats.ConfusionMatrix[actualClass][predictedClass]
			f.SetCellValue(sheet, cell, count)
		}
	}

	return nil
}



// File: internal/client/vllm.go
// Base: vllm

package client

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"sync"
	"time"

	"github.com/Vitruves/llm-client/internal/models"
)

var (
	bufferPool = sync.Pool{
		New: func() interface{} {
			return &bytes.Buffer{}
		},
	}
)

type VLLMClient struct {
	baseURL    string
	httpClient *http.Client
	timeout    time.Duration
	config     *models.Config
	requestCh  chan requestItem
}

type requestItem struct {
	req    *http.Request
	respCh chan *http.Response
	errCh  chan error
	ctx    context.Context
}

func NewVLLMClient(cfg *models.Config) (*VLLMClient, error) {
	timeout, err := time.ParseDuration(cfg.Provider.Timeout)
	if err != nil {
		return nil, fmt.Errorf("invalid timeout: %w", err)
	}

	maxConns := cfg.Processing.Workers * 2

	transport := &http.Transport{
		MaxIdleConns:          maxConns,
		MaxIdleConnsPerHost:   maxConns,
		IdleConnTimeout:       30 * time.Second,
		DisableKeepAlives:     false,
		MaxConnsPerHost:       maxConns,
		ResponseHeaderTimeout: timeout,
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
		ForceAttemptHTTP2:     false,
	}

	bufferSize := cfg.Processing.Workers * 4

	client := &VLLMClient{
		baseURL: cfg.Provider.BaseURL,
		httpClient: &http.Client{
			Transport: transport,
			Timeout:   timeout,
		},
		timeout:   timeout,
		config:    cfg,
		requestCh: make(chan requestItem, bufferSize),
	}

	if cfg.Processing.RateLimit {
		go client.rateLimitedRequestProcessor()
	}

	return client, nil
}

func (c *VLLMClient) rateLimitedRequestProcessor() {
	ticker := time.NewTicker(10 * time.Millisecond)
	defer ticker.Stop()

	for {
		select {
		case item := <-c.requestCh:
			select {
			case <-ticker.C:
			case <-item.ctx.Done():
				item.errCh <- item.ctx.Err()
				continue
			}

			resp, err := c.httpClient.Do(item.req)
			if err != nil {
				item.errCh <- err
			} else {
				item.respCh <- resp
			}

		case <-time.After(100 * time.Millisecond):
			continue
		}
	}
}

func (c *VLLMClient) SendRequest(ctx context.Context, req models.Request) (*models.Response, error) {
	start := time.Now()

	vllmReq := map[string]interface{}{
		"messages": c.formatMessages(req.Messages),
		"stream":   false,
	}

	c.addParameters(vllmReq, req.Options)
	c.addChatCompletionFeatures(vllmReq, req)

	buf := bufferPool.Get().(*bytes.Buffer)
	buf.Reset()
	defer bufferPool.Put(buf)

	if err := json.NewEncoder(buf).Encode(vllmReq); err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/v1/chat/completions", c.baseURL)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(buf.Bytes()))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")

	var resp *http.Response
	if c.config.Processing.RateLimit {
		resp, err = c.sendRateLimited(ctx, httpReq)
	} else {
		resp, err = c.httpClient.Do(httpReq)
	}

	if err != nil {
		return nil, fmt.Errorf("request failed: %w", err)
	}
	defer resp.Body.Close()

	var vllmResp struct {
		Choices []struct {
			Message struct {
				Content          *string           `json:"content"`
				ReasoningContent *string           `json:"reasoning_content"`
				ToolCalls        []models.ToolCall `json:"tool_calls,omitempty"`
			} `json:"message"`
			FinishReason *string `json:"finish_reason"`
		} `json:"choices"`
		Usage *models.Usage `json:"usage,omitempty"`
		Error struct {
			Message string `json:"message"`
		} `json:"error"`
	}

	if err := json.NewDecoder(resp.Body).Decode(&vllmResp); err != nil {
		return nil, fmt.Errorf("failed to decode response: %w", err)
	}

	if vllmResp.Error.Message != "" {
		return &models.Response{
			Success:      false,
			Error:        vllmResp.Error.Message,
			ResponseTime: time.Since(start),
		}, nil
	}

	if len(vllmResp.Choices) == 0 {
		return &models.Response{
			Success:      false,
			Error:        "no response choices returned",
			ResponseTime: time.Since(start),
		}, nil
	}

	choice := vllmResp.Choices[0]
	var finalContent string

	if choice.Message.Content != nil {
		finalContent = *choice.Message.Content
	}

	if choice.Message.ReasoningContent != nil && *choice.Message.ReasoningContent != "" {
		reasoningContent := *choice.Message.ReasoningContent

		if finalContent == "" {
			finalContent = fmt.Sprintf("<think>\n%s\n</think>", reasoningContent)
		} else {
			finalContent = fmt.Sprintf("<think>\n%s\n</think>\n\n%s", reasoningContent, finalContent)
		}
	}

	return &models.Response{
		Content:      finalContent,
		Success:      true,
		ResponseTime: time.Since(start),
		ToolCalls:    choice.Message.ToolCalls,
		Usage:        vllmResp.Usage,
		FinishReason: choice.FinishReason,
	}, nil
}

func (c *VLLMClient) sendRateLimited(ctx context.Context, req *http.Request) (*http.Response, error) {
	respCh := make(chan *http.Response, 1)
	errCh := make(chan error, 1)

	item := requestItem{
		req:    req,
		respCh: respCh,
		errCh:  errCh,
		ctx:    ctx,
	}

	select {
	case c.requestCh <- item:
	case <-ctx.Done():
		return nil, ctx.Err()
	}

	select {
	case resp := <-respCh:
		return resp, nil
	case err := <-errCh:
		return nil, err
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}

func (c *VLLMClient) addParameters(req map[string]interface{}, params models.ModelParameters) {
	flashInferSafe := c.config.Processing.FlashInferSafe != nil && *c.config.Processing.FlashInferSafe

	// Basic sampling parameters
	if params.Temperature != nil {
		req["temperature"] = *params.Temperature
	}
	if params.MaxTokens != nil {
		req["max_tokens"] = *params.MaxTokens
	}
	if params.MinTokens != nil {
		req["min_tokens"] = *params.MinTokens
	}
	if params.TopP != nil {
		req["top_p"] = *params.TopP
	}
	if params.TopK != nil {
		req["top_k"] = *params.TopK
	}
	if params.MinP != nil {
		req["min_p"] = *params.MinP
	}
	if params.N != nil {
		req["n"] = *params.N
	}

	// Penalty parameters
	if params.RepetitionPenalty != nil {
		req["repetition_penalty"] = *params.RepetitionPenalty
	}
	if params.PresencePenalty != nil {
		req["presence_penalty"] = *params.PresencePenalty
	}
	if params.FrequencyPenalty != nil {
		req["frequency_penalty"] = *params.FrequencyPenalty
	}

	// Seed (only if FlashInfer safe mode is disabled)
	if !flashInferSafe && params.Seed != nil {
		req["seed"] = *params.Seed
	}

	if len(params.Stop) > 0 {
		req["stop"] = params.Stop
	}
	if len(params.StopTokenIds) > 0 {
		req["stop_token_ids"] = params.StopTokenIds
	}
	if len(params.BadWords) > 0 {
		req["bad_words"] = params.BadWords
	}

	if params.IncludeStopStrInOutput != nil {
		req["include_stop_str_in_output"] = *params.IncludeStopStrInOutput
	}
	if params.IgnoreEos != nil {
		req["ignore_eos"] = *params.IgnoreEos
	}

	if params.Logprobs != nil {
		req["logprobs"] = *params.Logprobs
	}
	if params.PromptLogprobs != nil {
		req["prompt_logprobs"] = *params.PromptLogprobs
	}

	if params.TruncatePromptTokens != nil {
		req["truncate_prompt_tokens"] = *params.TruncatePromptTokens
	}
	if params.SkipSpecialTokens != nil {
		req["skip_special_tokens"] = *params.SkipSpecialTokens
	}
	if params.SpacesBetweenSpecialTokens != nil {
		req["spaces_between_special_tokens"] = *params.SpacesBetweenSpecialTokens
	}
	
	if params.EnableThinking != nil {
		extraBody := make(map[string]interface{})
		if existing, ok := req["extra_body"].(map[string]interface{}); ok {
			extraBody = existing
		}
		
		chatTemplateKwargs := make(map[string]interface{})
		if existing, ok := extraBody["chat_template_kwargs"].(map[string]interface{}); ok {
			chatTemplateKwargs = existing
		}
		
		chatTemplateKwargs["enable_thinking"] = *params.EnableThinking
		extraBody["chat_template_kwargs"] = chatTemplateKwargs
		req["extra_body"] = extraBody
	}

	// vLLM Guided Generation Parameters
	if len(params.GuidedChoice) > 0 {
		req["guided_choice"] = params.GuidedChoice
	}
	if params.GuidedRegex != nil {
		req["guided_regex"] = *params.GuidedRegex
	}
	if params.GuidedJSON != nil {
		req["guided_json"] = params.GuidedJSON
	}
	if params.GuidedGrammar != nil {
		req["guided_grammar"] = *params.GuidedGrammar
	}
	if params.GuidedWhitespacePattern != nil {
		req["guided_whitespace_pattern"] = *params.GuidedWhitespacePattern
	}
	if params.GuidedDecodingBackend != nil {
		req["guided_decoding_backend"] = *params.GuidedDecodingBackend
	}

	// Additional vLLM Parameters
	if params.MaxLogprobs != nil {
		req["max_logprobs"] = *params.MaxLogprobs
	}
	if params.Echo != nil {
		req["echo"] = *params.Echo
	}
	if params.BestOf != nil {
		req["best_of"] = *params.BestOf
	}
	if params.UseBeamSearch != nil {
		req["use_beam_search"] = *params.UseBeamSearch
	}
	if params.LengthPenalty != nil {
		req["length_penalty"] = *params.LengthPenalty
	}
	if params.EarlyStopping != nil {
		req["early_stopping"] = *params.EarlyStopping
	}
}

func (c *VLLMClient) formatMessages(messages []models.Message) []interface{} {
	formatted := make([]interface{}, len(messages))
	for i, msg := range messages {
		msgMap := map[string]interface{}{
			"role": msg.Role,
		}

		if msg.IsTextOnly() {
			msgMap["content"] = msg.GetTextContent()
		} else {
			msgMap["content"] = msg.Content
		}

		if msg.Name != nil {
			msgMap["name"] = *msg.Name
		}
		if len(msg.ToolCalls) > 0 {
			msgMap["tool_calls"] = msg.ToolCalls
		}
		if msg.ToolCallId != nil {
			msgMap["tool_call_id"] = *msg.ToolCallId
		}
		if msg.FunctionCall != nil {
			msgMap["function_call"] = msg.FunctionCall
		}

		formatted[i] = msgMap
	}
	return formatted
}

func (c *VLLMClient) addChatCompletionFeatures(req map[string]interface{}, chatReq models.Request) {
	if len(chatReq.Tools) > 0 {
		req["tools"] = chatReq.Tools
	}
	if chatReq.ToolChoice != nil {
		req["tool_choice"] = chatReq.ToolChoice
	}
	if chatReq.ResponseFormat != nil {
		req["response_format"] = chatReq.ResponseFormat
	}
	if chatReq.Stream != nil {
		req["stream"] = *chatReq.Stream
	}
	if chatReq.StreamOptions != nil {
		req["stream_options"] = chatReq.StreamOptions
	}
}

func (c *VLLMClient) HealthCheck(ctx context.Context) error {
	url := fmt.Sprintf("%s/health", c.baseURL)
	req, err := http.NewRequestWithContext(ctx, "GET", url, nil)
	if err != nil {
		return err
	}

	resp, err := c.httpClient.Do(req)
	if err != nil {
		return err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("health check failed with status: %d", resp.StatusCode)
	}

	return nil
}

func (c *VLLMClient) GetServerInfo(ctx context.Context) (*models.ServerInfo, error) {
	serverInfo := &models.ServerInfo{
		ServerURL:  c.baseURL,
		ServerType: "vllm",
		Timestamp:  float64(time.Now().Unix()),
		Available:  false,
		Config:     make(map[string]interface{}),
		Models:     make(map[string]interface{}),
		Features:   make(map[string]interface{}),
	}

	if err := c.HealthCheck(ctx); err != nil {
		return serverInfo, nil // Return with Available=false
	}
	serverInfo.Available = true

	modelsURL := fmt.Sprintf("%s/v1/models", c.baseURL)
	req, err := http.NewRequestWithContext(ctx, "GET", modelsURL, nil)
	if err == nil {
		resp, err := c.httpClient.Do(req)
		if err == nil && resp.StatusCode == http.StatusOK {
			defer resp.Body.Close()
			var modelsResp struct {
				Data []struct {
					ID          string        `json:"id"`
					Object      string        `json:"object"`
					Created     int64         `json:"created"`
					OwnedBy     string        `json:"owned_by"`
					Root        string        `json:"root"`
					Parent      string        `json:"parent"`
					MaxModelLen int           `json:"max_model_len"`
					Permission  []interface{} `json:"permission"`
				} `json:"data"`
			}
			if json.NewDecoder(resp.Body).Decode(&modelsResp) == nil && len(modelsResp.Data) > 0 {
				model := modelsResp.Data[0]
				serverInfo.Models = map[string]interface{}{
					"model_name":    model.ID,
					"max_model_len": model.MaxModelLen,
					"created":       model.Created,
					"owned_by":      model.OwnedBy,
					"root":          model.Root,
					"parent":        model.Parent,
				}
			}
		}
	}

	// Try to fetch version info (some vLLM servers expose this)
	versionURL := fmt.Sprintf("%s/version", c.baseURL)
	req, err = http.NewRequestWithContext(ctx, "GET", versionURL, nil)
	if err == nil {
		resp, err := c.httpClient.Do(req)
		if err == nil && resp.StatusCode == http.StatusOK {
			defer resp.Body.Close()
			var versionResp map[string]interface{}
			if json.NewDecoder(resp.Body).Decode(&versionResp) == nil {
				serverInfo.Features["version_info"] = versionResp
			}
		}
	}

	serverInfo.Config = map[string]interface{}{
		"base_url":   c.baseURL,
		"timeout":    c.timeout.String(),
		"model_name": c.config.Model.Name,
		"provider":   c.config.Provider.Name,
		"workers":    c.config.Processing.Workers,
		"batch_size": c.config.Processing.BatchSize,
		"rate_limit": c.config.Processing.RateLimit,
	}

	// Add model parameters
	if c.config.Model.Parameters.Temperature != nil {
		serverInfo.Config["temperature"] = *c.config.Model.Parameters.Temperature
	}
	if c.config.Model.Parameters.MaxTokens != nil {
		serverInfo.Config["max_tokens"] = *c.config.Model.Parameters.MaxTokens
	}
	if c.config.Model.Parameters.TopP != nil {
		serverInfo.Config["top_p"] = *c.config.Model.Parameters.TopP
	}
	if c.config.Model.Parameters.TopK != nil {
		serverInfo.Config["top_k"] = *c.config.Model.Parameters.TopK
	}

	serverInfo.Features = map[string]interface{}{
		"supports_streaming":    true,
		"supports_chat":         true,
		"supports_completions":  true,
		"supports_embeddings":   false, // vLLM can support embeddings but depends on model
		"supports_reasoning":    true,  // vLLM supports reasoning models
		"supports_tool_calling": true,
		"supports_vision":       false, // Depends on model
		"api_version":           "openai_compatible",
	}

	return serverInfo, nil
}

func (c *VLLMClient) Close() error {
	return nil
}



// File: internal/loader/loader.go
// Base: loader

package loader

import (
	"encoding/csv"
	"encoding/json"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"strings"

	"github.com/Vitruves/llm-client/internal/models"

	"github.com/parquet-go/parquet-go"
	"github.com/xuri/excelize/v2"
)

func LoadData(filename string) ([]models.DataRow, error) {
	ext := strings.ToLower(filepath.Ext(filename))

	switch ext {
	case ".csv":
		return loadCSV(filename)
	case ".json":
		return loadJSON(filename)
	case ".xlsx", ".xls":
		return loadExcel(filename)
	case ".parquet":
		return loadParquet(filename)
	default:
		return nil, fmt.Errorf("unsupported file format: %s", ext)
	}
}

func loadCSV(filename string) ([]models.DataRow, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	reader := csv.NewReader(file)
	
	headers, err := reader.Read()
	if err != nil {
		return nil, fmt.Errorf("failed to read CSV header: %w", err)
	}

	var data []models.DataRow
	rowIndex := 0

	for {
		record, err := reader.Read()
		if err != nil {
			if err == io.EOF {
				break
			}
			continue
		}

		if len(record) != len(headers) {
			continue
		}

		row := models.DataRow{
			Index: rowIndex,
			Data:  make(map[string]interface{}),
		}

		for j, value := range record {
			row.Data[headers[j]] = value
			if headers[j] == "text" || headers[j] == "content" {
				row.Text = value
			}
		}

		if row.Text == "" && len(record) > 0 {
			row.Text = record[0]
		}

		data = append(data, row)
		rowIndex++
	}

	if len(data) == 0 {
		return nil, fmt.Errorf("CSV file must have at least one valid data row")
	}

	return data, nil
}

func loadJSON(filename string) ([]models.DataRow, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	var rawData []map[string]interface{}
	if err := json.NewDecoder(file).Decode(&rawData); err != nil {
		return nil, err
	}

	var data []models.DataRow
	for i, item := range rawData {
		row := models.DataRow{
			Index: i,
			Data:  item,
		}

		if text, ok := item["text"].(string); ok {
			row.Text = text
		} else if content, ok := item["content"].(string); ok {
			row.Text = content
		} else {
			for _, value := range item {
				if str, ok := value.(string); ok && len(str) > 10 {
					row.Text = str
					break
				}
			}
		}

		data = append(data, row)
	}

	return data, nil
}

func loadExcel(filename string) ([]models.DataRow, error) {
	f, err := excelize.OpenFile(filename)
	if err != nil {
		return nil, err
	}
	defer f.Close()

	sheets := f.GetSheetList()
	if len(sheets) == 0 {
		return nil, fmt.Errorf("no sheets found")
	}

	rows, err := f.GetRows(sheets[0])
	if err != nil {
		return nil, err
	}

	if len(rows) < 2 {
		return nil, fmt.Errorf("Excel file must have at least a header and one data row")
	}

	headers := rows[0]
	var data []models.DataRow

	for i, row := range rows[1:] {
		if len(row) == 0 {
			continue
		}

		dataRow := models.DataRow{
			Index: i,
			Data:  make(map[string]interface{}),
		}

		for j, value := range row {
			if j < len(headers) && headers[j] != "" {
				dataRow.Data[headers[j]] = value
				if headers[j] == "text" || headers[j] == "content" {
					dataRow.Text = value
				}
			}
		}

		if dataRow.Text == "" && len(row) > 0 {
			dataRow.Text = row[0]
		}

		data = append(data, dataRow)
	}

	return data, nil
}

func loadParquet(filename string) ([]models.DataRow, error) {
	file, err := os.Open(filename)
	if err != nil {
		return nil, err
	}
	defer file.Close()

	stat, err := file.Stat()
	if err != nil {
		return nil, err
	}

	pf, err := parquet.OpenFile(file, stat.Size())
	if err != nil {
		return nil, err
	}

	// Get the schema to understand the structure
	schema := pf.Schema()
	columns := schema.Columns()

	var data []models.DataRow
	rowIndex := 0

	for _, rowGroup := range pf.RowGroups() {
		rows := rowGroup.Rows()
		defer rows.Close()

		for {
			rowValues := make([]parquet.Row, 1)
			n, err := rows.ReadRows(rowValues)
			if err != nil {
				if err.Error() == "EOF" {
					break
				}
				return nil, err
			}
			if n == 0 {
				break
			}

			row := rowValues[0]

			dataRow := models.DataRow{
				Index: rowIndex,
				Data:  make(map[string]interface{}),
			}

			row.Range(func(columnIndex int, columnValues []parquet.Value) bool {
				if columnIndex < len(columns) {
					columnPath := columns[columnIndex]
					columnName := columnPath[len(columnPath)-1] // Get the leaf column name

					if len(columnValues) > 0 {
						value := columnValues[0]

						var goValue interface{}
						if value.IsNull() {
							goValue = nil
						} else {
							switch value.Kind() {
							case parquet.Boolean:
								goValue = value.Boolean()
							case parquet.Int32:
								goValue = value.Int32()
							case parquet.Int64:
								goValue = value.Int64()
							case parquet.Float:
								goValue = value.Float()
							case parquet.Double:
								goValue = value.Double()
							case parquet.ByteArray:
								goValue = string(value.ByteArray())
							case parquet.FixedLenByteArray:
								goValue = string(value.ByteArray())
							default:
								goValue = value.String()
							}
						}

						dataRow.Data[columnName] = goValue

						if (columnName == "text" || columnName == "content" || columnName == "REVIEW") && goValue != nil {
							if str, ok := goValue.(string); ok {
								dataRow.Text = str
							}
						}
					}
				}
				return true // Continue iteration
			})

			if dataRow.Text == "" {
				for _, value := range dataRow.Data {
					if str, ok := value.(string); ok && len(str) > 10 {
						dataRow.Text = str
						break
					}
				}
			}

			data = append(data, dataRow)
			rowIndex++
		}
	}

	if len(data) == 0 {
		return nil, fmt.Errorf("parquet file is empty")
	}

	return data, nil
}


