# This is a comprehensive showcase configuration file for the LLM Client.
# It demonstrates all available options and their typical usage.
#
# To use this file, you can copy it and modify the values to fit your specific needs,
# or simply refer to it as a reference for available configurations.

# 1. Provider Configuration: Defines how the LLM client connects to the model server.
provider:
  name: "vllm" # Required: The type of LLM provider. Valid values: "vllm", "llamacpp", "openai"
  base_url: "http://localhost:8000/v1" # Required (for vllm/llamacpp): The base URL of the LLM API endpoint.
  timeout: "600s" # Optional: Request timeout duration (e.g., "30s", "5m").
  api_key: "YOUR_OPENAI_API_KEY" # Required (for openai): Your API key for authentication.
  # For llama.cpp, you might not need an API key depending on your setup.

# 2. Model Configuration: Specifies the LLM and its generation parameters.
model:
  name: "qwen-2.5-7b-chat" # Required: The specific model identifier string (e.g., "gpt-4", "mixtral-8x7b").
  parameters: # Optional: Parameters to control the model's generation behavior.
    temperature: 0.7 # Optional (float, 0.0-2.0): Controls randomness in output. Higher values mean more random.
    max_tokens: 1024 # Optional (int): Maximum number of tokens to generate in the response.
    min_tokens: 1 # Optional (int): Minimum number of tokens to generate in the response.
    top_p: 0.9 # Optional (float, 0.0-1.0): Nucleus sampling parameter. Considers tokens whose cumulative probability exceeds this value.
    top_k: 50 # Optional (int): Top-k sampling parameter. Considers only the top 'k' most likely tokens.
    min_p: 0.05 # Optional (float, 0.0-1.0): Minimum probability threshold.
    repetition_penalty: 1.1 # Optional (float): Penalizes new tokens based on their existing frequency in the text.
    presence_penalty: 0.0 # Optional (float, -2.0 to 2.0): Penalizes new tokens based on whether they appear in the text so far.
    frequency_penalty: 0.0 # Optional (float, -2.0 to 2.0): Penalizes new tokens based on their frequency in the text so far.
    seed: 42 # Optional (int64): Random seed for reproducible generations.
    n: 1 # Optional (int): Number of completions to generate for each prompt. (Only supported by some providers, like OpenAI).
    stop: ["<|im_end|>", "Observation:"] # Optional ([]string): List of strings that will stop the generation.
    stop_token_ids: [151645, 151643] # Optional ([]int): List of token IDs that will stop the generation.
    bad_words: ["offensive", "inappropriate"] # Optional ([]string): Words that should not be generated.
    include_stop_str_in_output: false # Optional (bool): Whether to include the stop string in the output.
    ignore_eos: false # Optional (bool): Whether to ignore the end-of-sequence token.
    logprobs: 0 # Optional (int): Number of log probabilities to return.
    prompt_logprobs: 0 # Optional (int): Number of prompt log probabilities.
    truncate_prompt_tokens: 2048 # Optional (int): Maximum prompt tokens before truncation.
    chat_format: "chatml" # Optional (string): Chat format for llama.cpp (e.g., "chatml", "llama-2").
    skip_special_tokens: false # Optional (bool): Whether to skip special tokens in the output.
    spaces_between_special_tokens: false # Optional (bool): Whether to add spaces between special tokens in the output.

# 3. Classification Configuration: Defines the LLM's prompt templates and how to parse its responses.
classification:
  template: # Prompt templates for the LLM.
    system: "You are a helpful assistant. Classify the user's input into one of the following categories: positive, negative, neutral." # Required: The system-level instruction for the LLM.
    user: "Review: {REVIEW_TEXT}\nClassify this review." # Required: The user-facing prompt template.
                                                      # Placeholders like {REVIEW_TEXT} will be replaced by input data.

  field_mapping: # Optional: How input data fields map to internal processing.
    input_text_field: "REVIEW_TEXT" # Optional (string): Specifies which original input column contains the primary text for `InputText`.
                                 # If not set, the system will try to infer from common names (e.g., "text", "content").
    placeholder_map: # Optional (map[string]string): Custom mappings for placeholders in the user template.
                     # Example: If your input has a column 'product_review', and your template uses '{REVIEW_TEXT}',
                     # you can map it here:
      REVIEW_TEXT: "product_review"

  parsing: # Defines how to extract the final answer from the LLM's response.
    find: ["positive", "negative", "neutral", "*"] # Required ([]string): A list of expected answers to look for in the LLM's response.
                                                    # Order matters: the first match wins. Use "*" to match any non-empty response.
    default: "unknown" # Required (string): The default value to use if no answer is found after all parsing attempts.
    fallback: "error_parsing" # Required (string): The value to use if an error occurs during parsing.
    map: # Optional (map[string]string): A mapping to transform extracted answers.
         # Example: If LLM returns "POS" but you want "positive":
      POS: "positive"
      NEG: "negative"
    thinking_tags: "<thinking></thinking>" # Optional (string): Tags that delineate "thinking" content in the LLM's response.
                                          # Example: "<thinking>I will analyze this.</thinking> The answer is positive."
    preserve_thinking: true # Optional (bool): If true, the content within `thinking_tags` will be preserved in the output.
    answer_patterns: # Optional ([]string): List of regex patterns to extract answers.
                     # Each pattern must have exactly one capture group (...).
      - "The classification is: (.*)"
      - "Category: (positive|negative|neutral)"
    case_sensitive: false # Optional (bool): If true, string matching (`find`) and regex patterns are case-sensitive.
    exact_match: false # Optional (bool): If true, `find` array requires an exact match, not a substring.

# 4. Processing Configuration: Controls concurrency and performance.
processing:
  workers: 10 # Required (int): Number of concurrent goroutines (workers) to process items.
  batch_size: 1 # Required (int): Number of items to process in a single batch request to the LLM. (Typically 1 for most LLMs).
  repeat: 1 # Required (int, 1-10): Number of times to send a request for each item to the LLM (for consensus mode).
  rate_limit: false # Required (bool): If true, adds a 1-second delay between `repeat` requests to avoid rate limiting.
  flashinfer_safe: false # Optional (bool): If true, disables features that might be incompatible with FlashInfer optimization (e.g., certain stop tokens).

  live_metrics: # Optional: Configuration for real-time performance metrics during processing.
    enabled: true # Required (bool): If true, enables live metric calculation and display.
    metric: "accuracy" # Required (string): The metric to calculate. Valid: "accuracy", "f1", "kappa".
    ground_truth: "actual_category" # Required (string): The name of the input column containing the ground truth labels.
    average: "macro" # Optional (string): Averaging method for F1/Kappa. Valid: "macro", "micro", "weighted".
    classes: ["positive", "negative", "neutral"] # Optional ([]string): Expected class labels for metric calculation.

# 5. Output Configuration: Defines how processed results are saved.
output:
  directory: "results/my_classification_run" # Required (string): The directory where output files will be saved.
  format: "json" # Required (string): The desired output file format. Valid: "json", "csv", "parquet", "xlsx".
  include_raw_response: false # Optional (bool): If true, the full raw response from the LLM is included in the output.
  include_thinking: true # Optional (bool): If true, extracted thinking content is included in the output.
  # input_text_field: "deprecated_field" # Deprecated: Use classification.field_mapping.input_text_field instead. 